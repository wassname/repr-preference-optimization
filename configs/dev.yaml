# Development configuration - small model, quick training
base_model: 'reciprocate/tiny-llama'
batch_size: 4
max_steps: 50
max_epochs: 1
lora_r: 8
lora_alpha: 16
layers: [2, 4, 6]
verbose: 2
accelerator: "auto"
devices: 1
