
# DPO and others are meant to be applied to a SFT tuned base model NOT a already preference tuned Chat or Instruction model https://arxiv.org/abs/2405.14734
base_model: 'princeton-nlp/Llama-3-Base-8B-SFT'
# load_in_4bit: True
# load_in_8bit: False
# batch_size: 16
collection_layers_side: # 0-31
  - 13
  - 14
  - 15
  - 16
  - 17
  - 18
  - 19
  - 20
  - 21
  - 22
  - 23
  - 24
  - 25
  - 26
  - 27
  - 28
  - 29
  - 30
  - 31
collection_layers_hs:
- 10
- 20
- 30
