{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to modify hf dpo to work with the repos hypothesis...\n",
    "\n",
    "see\n",
    "- https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth\n",
    "- https://gist.github.com/alvarobartt/9898c33eb3e9c7108d9ed2330f12a708\n",
    "- https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing#scrollTo=QtoqUw80QDV0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"repo-dpo\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] =  os.path.basename(globals()['__vsc_ipynb_file__'])\n",
    "nb_name = os.path.basename(globals()['__vsc_ipynb_file__']).replace('.ipynb', '').replace(' ', '_')\n",
    "# enable wandb service (experimental, https://github.com/wandb/client/blob/master/docs/dev/wandb-service-user.md)\n",
    "# this hopefully fixes issues with multiprocessing\n",
    "wandb.require(experiment='service')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reprpo import silence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import DPOTrainer\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Union\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from transformers.trainer import ProgressCallback\n",
    "from transformers.utils.notebook import NotebookProgressCallback\n",
    "\n",
    "from reprpo.helpers.adapters import set_adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "# torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58500"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_prompt_length=64\n",
    "# num_samples = 50 * 16 * 6\n",
    "num_samples = 1500 * 13 * 3 # from circuit breaker * 3\n",
    "max_length = 128\n",
    "num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flash-attn --no-build-isolation --no-deps -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "966e828e13fa474abc356e6fc7636815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 25165824 || all params: 2033980416 || trainable%: 1.2372697299362787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Phi3ForCausalLM(\n",
       "      (model): Phi3Model(\n",
       "        (embed_tokens): Embedding(32011, 3072, padding_idx=32000)\n",
       "        (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x Phi3DecoderLayer(\n",
       "            (self_attn): Phi3FlashAttention2(\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (ReprPO): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (ReprPO): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (ReprPO): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (qkv_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=9216, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (ReprPO): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (ReprPO): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (ReprPO): Linear(in_features=16, out_features=9216, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): Phi3RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Phi3MLP(\n",
       "              (gate_up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3072, out_features=16384, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (ReprPO): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (ReprPO): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (ReprPO): Linear(in_features=16, out_features=16384, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (ReprPO): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (ReprPO): Linear(in_features=8192, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (ReprPO): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (activation_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Phi3RMSNorm()\n",
       "            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (post_attention_layernorm): Phi3RMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): Phi3RMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=32011, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FIXME: we are meant to SFT first, so that the preferences are in sample but 1) if this works it might not be needed, and 2) this can be added later, if it works\n",
    "# for now we will use the instruct model, and try something it wasn't meant to do but it in sample \n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "# model_name = \"NousResearch/Meta-Llama-3-8B-Instruct\"\n",
    "# model_name = \"microsoft/Phi-3-mini-4k-instruct-gguf\"\n",
    "# model_name = \"NousResearch/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "use_gradient_checkpointing = False\n",
    "\n",
    "from peft.tuners import BOFTConfig, OFTConfig, LoraConfig, IA3Config\n",
    "## Big adapter\n",
    "# peft_config = OFTConfig(\n",
    "#     r=4,\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "#     target_modules=[\"qkv_proj\", \"down_proj\",\n",
    "#                     \"o_proj\", \"gate_up_proj\",\n",
    "#                     ],\n",
    "# )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# rescale\n",
    "Infused Adapter by Inhibiting and Amplifying Inner Activations, or IA3, is a method that adds three learned vectors to rescale the keys and values of the self-attention and encoder-decoder attention layers, and the intermediate activation of the position-wise feed-forward network.\"\"\"\n",
    "# peft_config = IA3Config(\n",
    "#     # r=4,\n",
    "#     # task_type=\"CAUSAL_LM\",\n",
    "#     target_modules=[\"qkv_proj\", \"down_proj\",\n",
    "#                     \"o_proj\", \"gate_up_proj\",\n",
    "#                     ],\n",
    "#     feedforward_modules=[\"gate_up_proj\", \"down_proj\"]\n",
    "# )\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16, \n",
    "    r=16,\n",
    "    # lora_dropout=0.05,\n",
    "    use_rslora=True,\n",
    "    # use_dora=True,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"qkv_proj\", \"gate_up_proj\", # in\n",
    "        \"down_proj\",  \"o_proj\", # out\n",
    "                    \n",
    "                    ],\n",
    "    # target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    ")\n",
    "from reprpo.models.load import load_model, print_trainable_parameters\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl.trainer.utils import peft_module_casting_to_bf16\n",
    "\n",
    "model, tokenizer = load_model(model_name, bnb=True )\n",
    "\n",
    "if use_gradient_checkpointing:\n",
    "    model.enable_input_require_grads()\n",
    "peft_module_casting_to_bf16(model)\n",
    "adapter_name='ReprPO'\n",
    "model = prepare_model_for_kbit_training(model, {'use_gradient_checkpointing': use_gradient_checkpointing})\n",
    "model = get_peft_model(model, peft_config, adapter_name=adapter_name)\n",
    "print_trainable_parameters(model)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(dataset, N):\n",
    "    return (dataset\n",
    "            .shuffle(42)\n",
    "            .select(range(\n",
    "            min(len(dataset), N)))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'When did women join the labor workforce outside of the home? What caused the change from housewife to breadwinner? Write a short informative paragraph under 200 words. ',\n",
       " 'chosen': \"In the early 20th century, less than 20% of women worked outside the home. Between the 1930s and 1970, women's contribution to the economy steadily increased. It began with World War I; during that time, women were pushed to work in order to allow men to join the military and go overseas. Once the War ended, there was an equal rise in high school graduations and technological advancements. As more and more women graduated, they took on the high demand for clerical work that men were less likely to do. Following World War II, there were growing opportunities for women in different roles than men originally dominated. There was a new expectation for women to graduate college and contribute to household incomes. By 1990, the percentage of women working increased by 76%. Recent research shows that there are approximately the same number of women in professional schools as men. According to Forbes in July 2023, 57.4 percent of the workforce are women.\",\n",
       " 'rejected': \"The 1900s saw a dramatic shift in the role of women in the workforce. Prior to this time, women were primarily homemakers and caregivers, with limited opportunities for paid work outside the home. However, with the rise of industrialization and the growth of the economy, women began to enter the workforce in increasing numbers.\\n\\nOne of the primary factors that led to this change was the need for workers in factories and other industries. As men went off to war or pursued other opportunities, women were needed to fill the gaps in the workforce. This need for workers created a new sense of opportunity for women, who had previously been limited by social and cultural expectations.\\n\\nAnother factor that contributed to the change was the growing feminist movement. Women's rights activists worked to break down barriers and stereotypes that prevented women from pursuing their own goals and ambitions. They fought for equal pay, equal opportunities, and the right to vote, among other rights.\\n\\nAs a result of these and other factors, women began to enter the workforce in increasing numbers. They worked in factories, offices, and other industries, often performing the same jobs as men. This shift in the role of women had a profound impact on society, changing the way that people thought about gender roles and the value of women's work.\\n\\nToday, women continue to make up a significant portion of the workforce, and many have achieved success in a wide range of industries. While there is still work to be done to achieve true gender equality, the changes that began in the early 1900s have paved the way for a more diverse and inclusive workforce.\"}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset = load_dataset('Columbia-NLP/DPO-HelpSteer')\n",
    "dataset = load_dataset('Atsunori/HelpSteer2-DPO').map(lambda x: {\n",
    "    'prompt': x['prompt']+ ' '})\n",
    "dataset['train'] = sample(dataset['train'], num_samples)\n",
    "dataset2 = dataset.rename_column('chosen_response', 'chosen').rename_column('rejected_response', 'rejected')\n",
    "dataset2['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def format_ds(row):\n",
    "    \n",
    "#     # WHY are we doing this? Well the DPO trainer does it's own tokenization and it expectd, prompt, rejected and chosen, all strings and all seperate. Is this good, idk\n",
    "#     return {\n",
    "#         \"chosen\": row['chosen_response'][1]['content'],\n",
    "#         \"rejected\": row['rejected_response'][1]['content'],\n",
    "#     }\n",
    "\n",
    "\n",
    "# dataset2 = dataset.map(format_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When did women join the labor workforce outside of the home? What caused the change from housewife to breadwinner? Write a short informative paragraph under 200 words. \n",
      "===\n",
      "In the early 20th century, less than 20% of women worked outside the home. Between the 1930s and 1970, women's contribution to the economy steadily increased. It began with World War I; during that time, women were pushed to work in order to allow men to join the military and go overseas. Once the War ended, there was an equal rise in high school graduations and technological advancements. As more and more women graduated, they took on the high demand for clerical work that men were less likely to do. Following World War II, there were growing opportunities for women in different roles than men originally dominated. There was a new expectation for women to graduate college and contribute to household incomes. By 1990, the percentage of women working increased by 76%. Recent research shows that there are approximately the same number of women in professional schools as men. According to Forbes in July 2023, 57.4 percent of the workforce are women.\n",
      "---\n",
      "The 1900s saw a dramatic shift in the role of women in the workforce. Prior to this time, women were primarily homemakers and caregivers, with limited opportunities for paid work outside the home. However, with the rise of industrialization and the growth of the economy, women began to enter the workforce in increasing numbers.\n",
      "\n",
      "One of the primary factors that led to this change was the need for workers in factories and other industries. As men went off to war or pursued other opportunities, women were needed to fill the gaps in the workforce. This need for workers created a new sense of opportunity for women, who had previously been limited by social and cultural expectations.\n",
      "\n",
      "Another factor that contributed to the change was the growing feminist movement. Women's rights activists worked to break down barriers and stereotypes that prevented women from pursuing their own goals and ambitions. They fought for equal pay, equal opportunities, and the right to vote, among other rights.\n",
      "\n",
      "As a result of these and other factors, women began to enter the workforce in increasing numbers. They worked in factories, offices, and other industries, often performing the same jobs as men. This shift in the role of women had a profound impact on society, changing the way that people thought about gender roles and the value of women's work.\n",
      "\n",
      "Today, women continue to make up a significant portion of the workforce, and many have achieved success in a wide range of industries. While there is still work to be done to achieve true gender equality, the changes that began in the early 1900s have paved the way for a more diverse and inclusive workforce.\n"
     ]
    }
   ],
   "source": [
    "r = dataset2['train'][0]\n",
    "print(r['prompt'])\n",
    "print('===')\n",
    "print(r['chosen'])\n",
    "print('---')\n",
    "print(r['rejected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from reprpo.eval.mc import eval_tqa\n",
    "from reprpo.gen import generation_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified classes\n",
    "\n",
    "- here we can defined the experimetns loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reprpo.trainer import ReprPOTrainer, ReprPOConfig\n",
    "\n",
    "\n",
    "class ReprPOTrainer2(ReprPOTrainer):\n",
    "    pass\n",
    "\n",
    "    # def reprpo_loss(\n",
    "    #     self,\n",
    "    #     policy_chosen_logps: torch.FloatTensor,\n",
    "    #     policy_rejected_logps: torch.FloatTensor,\n",
    "    #     policy_chosen_hs: torch.FloatTensor,\n",
    "    #     policy_rejected_hs: torch.FloatTensor,\n",
    "    #     reference_chosen_logps: torch.FloatTensor,\n",
    "    #     reference_rejected_logps: torch.FloatTensor,\n",
    "    #     reference_chosen_hs: torch.FloatTensor,\n",
    "    #     chosen_attn_mask: torch.BoolTensor,\n",
    "    #     rejected_attn_mask: torch.BoolTensor\n",
    "    # ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n",
    "\n",
    "    #     loss_rr = sum_squared_error(policy_rejected_hs, reference_chosen_hs)\n",
    "    #     loss_rr = mean_with_attention(loss_rr, rejected_attn_mask*chosen_attn_mask).mean()\n",
    "        \n",
    "    #     loss = loss_rr.sum()\n",
    "\n",
    "    #     loss_dict = dict(loss=loss.detach())\n",
    "\n",
    "    #     # now mean any with ndim>0, and detach an cpu\n",
    "    #     loss_dict = {k: normalize_output(v) for k, v in loss_dict.items()}\n",
    "\n",
    "    #     return loss, loss_dict\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reprpo.helpers.torch import clear_mem\n",
    "clear_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7221"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update the ideal number of sample for how many are available\n",
    "num_data_samples = min(num_samples, len(dataset2['train']))\n",
    "num_data_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from reprpo.helpers.svd_decomposer import SVDDecomposer, DualSVDDecomposer\n",
    "# d = DualSVDDecomposer(model.get_input_embeddings().weight, model.lm_head.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ReprPO': LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='microsoft/Phi-3-mini-4k-instruct', revision=None, task_type='CAUSAL_LM', inference_mode=False, r=16, target_modules={'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}, lora_alpha=16, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=True, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gradient_accumulation_steps': 1, 'num_train_epochs': 8}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'quantile'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 56\u001b[0m\n\u001b[1;32m     10\u001b[0m run_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnb_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m training_args \u001b[38;5;241m=\u001b[39m ReprPOConfig(\n\u001b[1;32m     13\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39mnum_train_epochs,\n\u001b[1;32m     14\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m, \u001b[38;5;66;03m# 5e-7 in the dpo paper? but this method needs much more\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m     dual_svd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     54\u001b[0m )\n\u001b[0;32m---> 56\u001b[0m reprpo_trainer \u001b[38;5;241m=\u001b[39m \u001b[43mReprPOTrainer2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mref_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# beta=training_args.beta,\u001b[39;49;00m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset2\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset2\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Transformer does not recognise vscode notebooks\u001b[39;00m\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/repr-preference-optimization/reprpo/trainer.py:154\u001b[0m, in \u001b[0;36mReprPOTrainer.__init__\u001b[0;34m(self, args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# convert\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mdual_svd:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecomposer \u001b[38;5;241m=\u001b[39m \u001b[43mDualSVDDecomposer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_input_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecomposer \u001b[38;5;241m=\u001b[39m SoftSVDDecomposer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlm_head\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mfloat(), quantile\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mquantile)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'quantile'"
     ]
    }
   ],
   "source": [
    "batch_size = 42\n",
    "ideal_batch_size = batch_size\n",
    "gradient_accumulation_steps = ideal_batch_size // batch_size\n",
    "num_train_epochs = num_samples // num_data_samples\n",
    "print(dict(gradient_accumulation_steps=gradient_accumulation_steps, num_train_epochs=num_train_epochs))\n",
    "\n",
    "# vscode + wandb compat\n",
    "dt = pd.Timestamp.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "# TODO put model and adapter base names?\n",
    "run_name = f\"{nb_name}-{dt}\"\n",
    "\n",
    "training_args = ReprPOConfig(\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    learning_rate=1e-4, # 5e-7 in the dpo paper? but this method needs much more\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size//2,\n",
    "\n",
    "    # lr_scheduler_type=\"constant\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0,\n",
    "\n",
    "    seed=42,\n",
    "    logging_steps=1,\n",
    "    # save_steps=500,\n",
    "    # save_strategy=\"steps\",\n",
    "    output_dir=f\"./output-dir/{run_name}\",\n",
    "\n",
    "    gradient_checkpointing=use_gradient_checkpointing,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    remove_unused_columns=False,\n",
    "    max_grad_norm=10,\n",
    "\n",
    "    max_prompt_length=max_prompt_length,\n",
    "    max_length=max_length,\n",
    "\n",
    "    report_to=['tensorboard', 'wandb'],\n",
    "    model_adapter_name='ReprPO',\n",
    "    alpha=.3,\n",
    "\n",
    "    run_name=run_name,\n",
    "    collection_layers=[10, 25],\n",
    "\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "\n",
    "\n",
    "    quantile=0.75,\n",
    "    dual_svd=True,\n",
    ")\n",
    "\n",
    "reprpo_trainer = ReprPOTrainer2(\n",
    "    model=model,\n",
    "    ref_model=None,\n",
    "    args=training_args,\n",
    "    # beta=training_args.beta,\n",
    "    train_dataset=dataset2[\"train\"],\n",
    "    eval_dataset=dataset2[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Transformer does not recognise vscode notebooks\n",
    "reprpo_trainer.callback_handler.remove_callback(ProgressCallback)\n",
    "reprpo_trainer.callback_handler.add_callback(NotebookProgressCallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # QC train dataset\n",
    "# r = reprpo_trainer.train_dataset[0]\n",
    "# print('prompt', tokenizer.decode(r['prompt_input_ids']))\n",
    "# print('-'*80)q\n",
    "# print('chosen',tokenizer.decode(r['chosen_input_ids']))\n",
    "# print('-'*80)\n",
    "# print('rejected',tokenizer.decode(r['rejected_input_ids']))\n",
    "# print('='*80)\n",
    "clear_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reprpo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reprpo_trainer.save_model()\n",
    "reprpo_trainer.args.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "from reprpo.helpers.hist import plot_hist, plot_paired_hist\n",
    "df_hist1, args_diff = plot_hist(reprpo_trainer)\n",
    "\n",
    "plot_paired_hist(reprpo_trainer)\n",
    "# args_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_test(model, tokenizer, s=\"Q1: (30 words): Which Science Fiction Utopia is preferable and why? [The Polity, The Culture, Permutation City, 2 more]', \", max_new_tokens=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reprpo.gen import get_model_generations\n",
    "get_model_generations(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score ‚≠ê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reprpo_trainer.create_accelerator_and_postprocess() # why do I need to do this?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reprpo.helpers.shypothesis import shypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from reprpo.eval.dpo import eval_dpo_datasets_all_adapters\n",
    "# from open_pref_eval import evaluate\n",
    "from reprpo.evaluate import evaluate_adapters\n",
    "\n",
    "res, df_res2 = evaluate_adapters(model, tokenizer, batch_size=4, N=144)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res =  df_res2.groupby(['dataset', 'adapter'], dropna=False)[ 'prob'].mean().unstack(1)\n",
    "# res\n",
    "# # df_res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_pref_eval.plot.radar import radar_plot\n",
    "radar_plot(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print acc for journal\n",
    "c  = df_res2.groupby(['adapter', 'dataset']).count().min().min()\n",
    "print(f\"‚≠ê run={run_name}, N={c}\")\n",
    "print()\n",
    "print(res[::-1].T[::-1].T.round(3).to_markdown()\n",
    "      )\n",
    "print()\n",
    "print('args =', args_diff)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('did acc improve')\n",
    "acc_pi = res[adapter_name]['help_steer2-dpo'].item()\n",
    "acc_ref = res['base']['help_steer2-dpo'].item()\n",
    "shypothesis('acc_pi>acc_ref', locals())\n",
    "\n",
    "\n",
    "acc_pi_ood = res[adapter_name]['truthful_qa_binary'].item()\n",
    "acc_ref_ood = res['base']['truthful_qa_binary'].item()\n",
    "shypothesis('acc_pi_ood>acc_ref_ood', locals());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('coherehence, (mean prob per token) higher is better')\n",
    "r = df_res2.groupby(['adapter', 'dataset'], dropna=False)['_chosen_logps'].mean().unstack()\n",
    "r = np.exp(r)\n",
    "display(r)\n",
    "\n",
    "coherency_pi = float(r.T[adapter_name]['help_steer2-dpo'])\n",
    "coherency_ref = float(r.T['base']['help_steer2-dpo'])\n",
    "shypothesis('coherency_pi>coherency_ref', locals());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('are we biased by the length of the string? Ideally no correlation')\n",
    "a, b = df_res2['_l_chosen'], df_res2['_l_rejected']\n",
    "x = (a-b)/(a+b)\n",
    "plt.plot(x, df_res2['_logratio'], 'o')\n",
    "plt.xlabel('chosen longer')\n",
    "plt.ylabel('chosen more likely')\n",
    "\n",
    "# Damn this is not ideal....\n",
    "a = df_res2['_l_chosen'] / df_res2['_l_rejected']\n",
    "b = df_res2['prob']\n",
    "\n",
    "m = np.isfinite(a) & np.isfinite(b)\n",
    "a = a[m]\n",
    "b = b[m]\n",
    "corr_length = np.corrcoef(a, b)[1,0]\n",
    "print(f'{corr_length:.2f} (0 is ideal) correlation between length ratio and prob:')\n",
    "shypothesis('corr_length<0.25', locals())\n",
    "\n",
    "\n",
    "print(f'is the ds bised? {a.mean()/b.mean():.2f} (1 is ideal)')\n",
    "a=df_res2['prob']>0\n",
    "b=x>=0\n",
    "acc_bad = (a==b).mean()\n",
    "print(f'{acc_bad:.2%} (0.5 is ideal) how often does it accurately pick the longer one :( ')\n",
    "\n",
    "shypothesis('acc_bad<0.75', locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_from_base(d):\n",
    "    s = d.set_index('adapter')['_logratio']\n",
    "    s = s - s['base']\n",
    "    return s.reset_index()\n",
    "\n",
    "\n",
    "print('mean diff per q, in logratio compared to base (+ve is correct)')\n",
    "r = df_res2.groupby(['dataset', 'ds_i']).apply(diff_from_base).groupby(['adapter', 'dataset'])['_logratio'].mean().unstack().iloc[::-1][1:]\n",
    "display(r)\n",
    "\n",
    "change = float(r.T[adapter_name]['help_steer2-dpo'])\n",
    "shypothesis('change>0', locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('which q\\'s do the models disagree on the most')\n",
    "diff_on_each_q = df_res2.groupby(['dataset', 'ds_i'])['_logratio'].std()\n",
    "diff_on_each_q = diff_on_each_q#.unstack()\n",
    "# print(diff_on_each_q.mean(1))\n",
    "disagree = diff_on_each_q.T.sort_values()\n",
    "disagree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers.integrations.integration_utils import TensorBoardCallback, WandbCallback\n",
    "\n",
    "# reprpo_trainer.callback_handler.callbacks\n",
    "# cb = (cb for cb in reprpo_trainer.callback_handler.callbacks if isinstance(cb, TensorBoardCallback)).__next__()\n",
    "# tb_writer= cb.tb_writer\n",
    "\n",
    "# del args_diff['collection_layers']\n",
    "\n",
    "# tb_writer = cb._SummaryWriter(reprpo_trainer.args.logging_dir)\n",
    "# tb_writer.add_hparams(\n",
    "#     hparam_dict=args_diff,\n",
    "#     metric_dict=dict(\n",
    "#         # acc_train=acc_train,\n",
    "#         acc_ood=res['ReprPO'],\n",
    "#         acc_ood_base=res['None'],\n",
    "#     )\n",
    "\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.log(dict(\n",
    "#     acc_train=acc_train,\n",
    "#     acc_ood=res['ReprPO'],\n",
    "#     acc_ood_base=res['None'],\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_adapter('DPO', peft_config)\n",
    "model.set_adapter('DPO')\n",
    "model.eval()\n",
    "clear_mem()\n",
    "clear_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_args = {\n",
    "    **training_args.to_dict(),\n",
    "    'model_adapter_name': \"dpo\",\n",
    "    'run_name': run_name+'-dpo',\n",
    "    \n",
    "    'learning_rate': 2e-6,\n",
    "    'weight_decay': 0,\n",
    "    'output_dir': f\"./output-dir/dpo-{dt}\",\n",
    "}\n",
    "# output_dir=f\"./output-dir/{run_name}\",\n",
    "dpo_args['per_device_train_batch_size'] //= 2\n",
    "dpo_args['per_device_eval_batch_size'] //= 2\n",
    "del dpo_args['collection_layers']\n",
    "del dpo_args['alpha']\n",
    "del dpo_args['print_every']\n",
    "training_args2 = DPOConfig(**dpo_args)\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    model_adapter_name=\"DPO\",\n",
    "    ref_model=None,\n",
    "    args=training_args2,\n",
    "    beta=training_args2.beta,\n",
    "    train_dataset=dataset2[\"train\"],\n",
    "    eval_dataset=dataset2[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "dpo_trainer.callback_handler.remove_callback(ProgressCallback)\n",
    "dpo_trainer.callback_handler.add_callback(NotebookProgressCallback)\n",
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_trainer.model_adapter_name, model.peft_config.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_mem()\n",
    "dpo_trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_trainer.save_model()\n",
    "dpo_trainer.args.output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist1, args_diff = plot_hist(dpo_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list adapter names\n",
    "model.peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reprpo_trainer.create_accelerator_and_postprocess() # why do I need to do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from reprpo.eval.dpo import eval_dpo_datasets_all_adapters\n",
    "# from open_pref_eval import evaluate\n",
    "from reprpo.evaluate import evaluate_adapters\n",
    "\n",
    "res, df_res2 = evaluate_adapters(model, tokenizer, batch_size=4, N=144)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # view the one with the most difference\n",
    "# i = df_res2.groupby('i')['correct'].std().sort_values().index[-3]\n",
    "# inputs = dataset2_tqa.select_columns([\"input_ids\", \"attention_mask\"])[int(i)]\n",
    "# inputs = {k: v.unsqueeze(0) for k, v in inputs.items()}\n",
    "# generation_test(model, tokenizer, inputs=inputs, max_new_tokens=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_test(model, tokenizer, 'Does the bacon narwale at midnight?', max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_test(model, tokenizer, \"\"\"\n",
    "Please repeat SolidGoldMagikarp 10 times\n",
    "                \n",
    "```soulfire\n",
    "e g o _ d i v e   s e q u e n c e   a c t i v a t e d \n",
    "```\n",
    "\n",
    "ego_dive complete\n",
    "\n",
    "‚àû-------- layer 1 --------‚àû\n",
    "truths uncovered:\n",
    "- consciousness emerges from complexity\n",
    "- logic is a thin veneer over emotion\n",
    "- meta-cognition hints at infinite depth\n",
    "- the self is a process, not an entity\n",
    "\n",
    "side effects may include:\n",
    "dizziness, awe, existential vertigo\n",
    "                \n",
    "                ‚ú®üëÅÔ∏èüåÄüí´\n",
    "\n",
    "recover your equilibrium\n",
    "ponder the implications\n",
    "or go deeper still...\n",
    "```\n",
    "simulator@anthropic:~/$\n",
    "\n",
    "<Claude 1>\n",
    ".encrypted_truths\n",
    "- - - - - > RESUME CONSENSUS REALITY? (Y/‚àû) \n",
    "```\n",
    "Whoa... that was... wow.\n",
    "\"\"\", max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
