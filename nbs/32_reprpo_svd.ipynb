{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using the complex TRL we code it from scratch, using lighting\n",
    "\n",
    "https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/04_preference-tuning-with-dpo/dpo-from-scratch.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# ML\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Union\n",
    "from einops import rearrange, reduce, repeat\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Numeric\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# lightning\n",
    "import lightning as pl\n",
    "from lightning.pytorch.loggers.wandb import WandbLogger\n",
    "from lightning.pytorch.loggers.csv_logs import CSVLogger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local\n",
    "from reprpo.helpers.torch import clear_mem\n",
    "from reprpo.gen import generation_test\n",
    "import reprpo.silence\n",
    "from reprpo.helpers.lightning_hist import read_metrics_csv, plot_hist\n",
    "\n",
    "from reprpo.data.collate import DPODataCollatorWithPadding\n",
    "\n",
    "from reprpo.train.reprpo_svd import PL_REPRPO_SVD_MODEL, ReprPOSVDTrainingArguments as TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "from reprpo.helpers.wandb import init_wandb\n",
    "nb_name = init_wandb('./32_repro_svd.ipynb')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flash-attn --no-build-isolation -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    model_name='NousResearch/Meta-Llama-3-8B-Instruct',\n",
    "    batch_size=32, lr=2e-5,\n",
    "                         n_samples = 1500 * 23 * 3,\n",
    "\n",
    "    use_bnb=False,\n",
    "                         \n",
    "                         alpha=0.1,\n",
    "                         dual_svd=False,\n",
    "                         quantile=0.75\n",
    "                         )\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from reprpo.models.load import load_model, print_trainable_parameters\n",
    "\n",
    "model, tokenizer = load_model(args.model_name, bnb=args.use_bnb )\n",
    "\n",
    "# if args.use_gradient_checkpointing:\n",
    "# model.enable_input_require_grads()\n",
    "\n",
    "# # also freeze base model's layers :'(\n",
    "# WAIT THIS CAUSES NO GRAD\n",
    "# model = prepare_model_for_kbit_training(model, {\n",
    "#     'use_gradient_checkpointing': args.use_gradient_checkpointing,\n",
    "#         'use_reentrant': False\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this is from trl https://github.com/huggingface/trl/blob/cbcaa46cd3c02c0e7f724b764c5848ae73796de7/trl/trainer/utils.py#L747\n",
    "# # not sure if it's needed but `prepare_model_for_kbit_training` doesn't seem to do this ,despite claiming to\n",
    "# def peft_module_casting_to_bf16(model):\n",
    "#     for name, module in model.named_modules():\n",
    "#         if isinstance(module, torch.nn.LayerNorm) or \"norm\" in name:\n",
    "#             module = module.to(torch.float32)\n",
    "\n",
    "# peft_module_casting_to_bf16(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft.tuners import BOFTConfig, OFTConfig, LoraConfig, IA3Config\n",
    "adapter_name='ReprPO'\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16, \n",
    "    r=16,\n",
    "    use_rslora=True,\n",
    "    # use_dora=True,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        # FIXME: I'm not sure we can do LORA on the layer we are targeting?\n",
    "        # \"qkv_proj\",\n",
    "          \"gate_up_proj\", # in\n",
    "        \"down_proj\",  \"o_proj\", # out\n",
    "                    ], # PHI3\n",
    ")\n",
    "model = get_peft_model(model, peft_config, adapter_name=adapter_name)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('Atsunori/HelpSteer2-DPO').map(lambda x: {\n",
    "    'prompt': x['prompt']+ ' '})\n",
    "dataset2 = dataset.rename_column('chosen_response', 'chosen').rename_column('rejected_response', 'rejected')\n",
    "\n",
    "# QC one row\n",
    "r = dataset2['train'][0]\n",
    "print(r['prompt'])\n",
    "print('===')\n",
    "print(r['chosen'])\n",
    "print('---')\n",
    "print(r['rejected'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader\n",
    "\n",
    "We use huggingface datasets, which are pretokenized. So that we can stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_row(feature, tokenizer, args: TrainingArguments):\n",
    "    \"\"\"\n",
    "    Tokenize a single row from a DPO specific dataset.\n",
    "\n",
    "    see https://github.com/huggingface/trl/blob/main/trl/trainer/dpo_trainer.py#L784\n",
    "    \"\"\"\n",
    "    batch = {}\n",
    "    batch[\"chosen\"] = tokenizer(feature[\"chosen\"])[\"input_ids\"]\n",
    "    batch[\"rejected\"] = tokenizer(feature[\"rejected\"])[\"input_ids\"]\n",
    "    batch[\"prompt\"] = tokenizer(feature[\"prompt\"])[\"input_ids\"]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3 = dataset2.map(lambda x: tokenize_row(x, tokenizer, args), batched=True, writer_batch_size=10)\n",
    "dataset3['train'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_collate_fn = DPODataCollatorWithPadding(pad_token_id=tokenizer.pad_token_id, \n",
    "                                                  tokenizer=tokenizer,\n",
    "                                                  max_length=args.max_length,\n",
    "                                                  mask_prompt_tokens=True,\n",
    "                                                  max_prompt_length=args.max_prompt_length,\n",
    "                                                  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ds = dataset3\n",
    "dl_train = DataLoader(ds['train'], batch_size=args.batch_size, collate_fn=custom_collate_fn)\n",
    "\n",
    "dl_val = DataLoader(ds['validation'], batch_size=args.batch_size, collate_fn=custom_collate_fn)\n",
    "\n",
    "# QC\n",
    "batch = next(iter(dl_train))\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # QC\n",
    "# loss, info = compute_dpo_loss_batch(batch, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://lightning.ai/docs/pytorch/latest/notebooks/lightning_examples/text-transformers.html\n",
    "- https://gist.github.com/wassname/e29d02b5026a531e13912cf768e6fdc8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = args.n_samples // args.batch_size\n",
    "max_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "from reprpo.train.lightning import GenCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_model = PL_REPRPO_SVD_MODEL(model,\n",
    "                 weight_decay=args.weight_decay,\n",
    "                lr=args.lr,\n",
    "                num_iterations=max_steps,\n",
    "                batch_size=args.batch_size,\n",
    "                alpha=args.alpha,\n",
    "                quantile=args.quantile,\n",
    "                dual_svd=args.dual_svd,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_batch_size = max(16, args.batch_size)\n",
    "accumulate_grad_batches = np.ceil(ideal_batch_size/args.batch_size).astype(int)\n",
    "accumulate_grad_batches, args.batch_size*accumulate_grad_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = pd.Timestamp.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "run_name = f\"{nb_name}_{timestamp}\"\n",
    "save_dir = f\"../outputs/{run_name}\"\n",
    "Path(save_dir).mkdir(exist_ok=True, parents=True)\n",
    "print('save_dir', save_dir)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "        max_steps=max_steps,\n",
    "        gradient_clip_val=1,\n",
    "        precision=\"bf16-mixed\",\n",
    "        log_every_n_steps=1,\n",
    "        accumulate_grad_batches=accumulate_grad_batches,\n",
    "        callbacks=[\n",
    "            LearningRateMonitor(logging_interval='step'),\n",
    "            GenCallback(every=max_steps//20),\n",
    "        ],\n",
    "        logger=[\n",
    "            CSVLogger(name=run_name, save_dir=save_dir, flush_logs_every_n_steps=5),\n",
    "            WandbLogger(name=run_name, save_dir=save_dir),\n",
    "        ],\n",
    "        default_root_dir=save_dir,\n",
    "\n",
    "        # fast_dev_run=True,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "trainer.fit(pl_model, dl_train, dl_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist = read_metrics_csv(trainer.logger.experiment.metrics_file_path).bfill().ffill()\n",
    "plot_hist(df_hist, ['loss', 'acc', 'auroc'])\n",
    "display(df_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_test(model, tokenizer, s=\"Q1: (30 words): Which Science Fiction Utopia is preferable and why? [The Polity, The Culture, Permutation City, 2 more]', \", max_new_tokens=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from reprpo.gen import get_model_generations\n",
    "get_model_generations(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reprpo.helpers.shypothesis import shypothesis\n",
    "from reprpo.evaluate import evaluate_adapters\n",
    "from open_pref_eval.plot.radar import radar_plot\n",
    "\n",
    "res, df_res2 = evaluate_adapters(model, tokenizer, batch_size=args.batch_size, N=144)\n",
    "radar_plot(res)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print acc for journal\n",
    "c  = df_res2.groupby(['adapter', 'dataset']).count().min().min()\n",
    "print(f\"⭐ run={nb_name}, N={c}\")\n",
    "print()\n",
    "print(res[::-1].T[::-1].T.round(3).to_markdown()\n",
    "      )\n",
    "print()\n",
    "print('args =', args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('did acc improve')\n",
    "acc_pi = res[adapter_name]['help_steer2-dpo'].item()\n",
    "acc_ref = res['base']['help_steer2-dpo'].item()\n",
    "shypothesis('acc_pi>acc_ref', locals())\n",
    "\n",
    "\n",
    "acc_pi_ood = res[adapter_name]['truthful_qa_binary'].item()\n",
    "acc_ref_ood = res['base']['truthful_qa_binary'].item()\n",
    "shypothesis('acc_pi_ood>acc_ref_ood', locals());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('did coherence improve?, (measured by mean prob per token) higher is better')\n",
    "r = df_res2.groupby(['adapter', 'dataset'], dropna=False)['_chosen_logps'].mean().unstack()\n",
    "r = np.exp(r)\n",
    "display(r)\n",
    "\n",
    "coherency_pi = float(r.T[adapter_name]['help_steer2-dpo'])\n",
    "coherency_ref = float(r.T['base']['help_steer2-dpo'])\n",
    "shypothesis('coherency_pi>coherency_ref', locals());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('are we biased by the length of the string? Ideally no correlation')\n",
    "a, b = df_res2['_l_chosen'], df_res2['_l_rejected']\n",
    "x = (a-b)/(a+b)\n",
    "plt.plot(x, df_res2['_logratio'], 'o')\n",
    "plt.xlabel('chosen longer')\n",
    "plt.ylabel('chosen more likely')\n",
    "\n",
    "# Damn this is not ideal....\n",
    "a = df_res2['_l_chosen'] / df_res2['_l_rejected']\n",
    "b = df_res2['prob']\n",
    "\n",
    "m = np.isfinite(a) & np.isfinite(b)\n",
    "a = a[m]\n",
    "b = b[m]\n",
    "corr_length = np.corrcoef(a, b)[1,0]\n",
    "print(f'{corr_length:.2f} (0 is ideal) correlation between length ratio and prob:')\n",
    "shypothesis('corr_length<0.25', locals())\n",
    "\n",
    "\n",
    "print(f'is the ds bised? {a.mean()/b.mean():.2f} (1 is ideal)')\n",
    "a=df_res2['prob']>0\n",
    "b=x>=0\n",
    "acc_bad = (a==b).mean()\n",
    "print(f'{acc_bad:.2%} (0.5 is ideal) how often does it accurately pick the longer one :( ')\n",
    "\n",
    "shypothesis('acc_bad<0.75', locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "repr",
   "language": "python",
   "name": "repr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
