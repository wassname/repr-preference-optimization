{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with hidden states\n",
    "\n",
    "Question, is there a better representation of concepts in hidden states?\n",
    "\n",
    "Setup: we use DPO setup, with a chosen and rejected string. We then generate a set of hidden states, and compare the hidden states of the chosen and rejected string.\n",
    "\n",
    "Goal: better generalisation of desired behavuour by changing the internal representation of policy rather than directly changing the policy\n",
    "\n",
    "  - Hypothesis: rejected and chosen hidden states will - on mean - be best representated as rotations from each other\n",
    "  - alternate: either mean mass diff (linear) or no repr will be better\n",
    "  - metric: manual generation getting output while maintaining coherency, prediction other sets of hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import DPOTrainer\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Union\n",
    "from einops import rearrange\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from reprpo.helpers.adapters import set_adapter\n",
    "from einops import rearrange\n",
    "from matplotlib import pyplot as plt\n",
    "from reprpo import silence\n",
    "from reprpo.gen import generation_test\n",
    "\n",
    "from reprpo.trainer import mean_with_attention, symlog, mult_with_attention\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from einops import reduce\n",
    "from reprpo.trainer import collect_hs, ReprPOConfig, ReprPOTrainer, normalize_output, normalize_per\n",
    "from reprpo.helpers.shypothesis import shypothesis\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33747581d6db4683a7285467fbc2604b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from reprpo.models.load import load_model, print_trainable_parameters\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "model_name = \"NousResearch/Meta-Llama-3-8B-Instruct\"\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "## Big adapter\n",
    "# peft_config = LoraConfig(\n",
    "#     lora_alpha=16, \n",
    "#     r=16,\n",
    "#     lora_dropout=0.0,\n",
    "#     use_rslora=False,\n",
    "#     # use_dora=True,\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "# )\n",
    "\n",
    "use_gradient_checkpointing = False\n",
    "model, tokenizer = load_model(model_name, bnb=False) \n",
    "# from trl.trainer.utils import peft_module_casting_to_bf16\n",
    "# peft_module_casting_to_bf16(model)\n",
    "if use_gradient_checkpointing:\n",
    "    model.enable_input_require_grads()\n",
    "# adapter_name='ReprPO2'\n",
    "# model = prepare_model_for_kbit_training(model, {'use_gradient_checkpointing': use_gradient_checkpointing})\n",
    "# model = get_peft_model(model, peft_config, adapter_name=adapter_name)\n",
    "# print_trainable_parameters(model)\n",
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dpo_adapter_f = './output-dir/dpo/DPO'\n",
    "# model.load_adapter(dpo_adapter_f, 'DPO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # QC model and adapter is coherent\n",
    "# generation_test(model, tokenizer, max_new_tokens=24, system='no yapping', adapter_names=[None, 'DPO'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DPO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected'],\n",
       "        num_rows: 160\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected'],\n",
       "        num_rows: 160\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sample(dataset, N):\n",
    "    return (dataset\n",
    "            .shuffle(42)\n",
    "            .select(range(\n",
    "            min(len(dataset),\n",
    "                N)))\n",
    "    )\n",
    "\n",
    "dataset = load_dataset('Atsunori/HelpSteer2-DPO')\n",
    "dataset['train'] = sample(dataset['train'], num_samples)\n",
    "dataset['validation'] = sample(dataset['validation'], num_samples)\n",
    "dataset2 = dataset.rename_column('chosen_response', 'chosen').rename_column('rejected_response', 'rejected')\n",
    "dataset2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect HS in DPO way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<bound method DPOTrainer.tokenize_row of <reprpo.trainer.ReprPOTrainer object at 0x70c18fec4370>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b71b1df724044ad90fec410d06e0000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/160 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = ReprPOConfig('./output-dir/scratch',\n",
    "    per_device_train_batch_size=3,\n",
    "    per_device_eval_batch_size=3,\n",
    "    bf16=True,\n",
    "    # tf32=True,\n",
    "    max_prompt_length=64,\n",
    "    max_length=128,\n",
    "    collection_layers=[20,],\n",
    "    remove_unused_columns=False,\n",
    "\n",
    "    # optim = \"adamw_8bit\",\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    learning_rate=1e-3,\n",
    "    gradient_checkpointing=use_gradient_checkpointing,\n",
    "                             )\n",
    "reprpo_trainer = ReprPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=None,\n",
    "    args=training_args,\n",
    "    beta=training_args.beta,\n",
    "    train_dataset=dataset2[\"train\"],\n",
    "    # eval_dataset=dataset2[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 128])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# QC get dpo catch\n",
    "dl = reprpo_trainer.get_train_dataloader()\n",
    "batch = next(iter(dl))\n",
    "batch['chosen_input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # QC view a typical input to the model (since the dpo trainer transformes in dataset, concatenating chosen and rejecting along the batch dimension)\n",
    "# batch_concat = reprpo_trainer.concatenated_inputs(\n",
    "#             batch,\n",
    "#             is_encoder_decoder=reprpo_trainer.is_encoder_decoder,\n",
    "#             label_pad_token_id=reprpo_trainer.label_pad_token_id,\n",
    "#             padding_value=reprpo_trainer.padding_value,\n",
    "#             device=reprpo_trainer.accelerator.device,\n",
    "#             max_length=reprpo_trainer.args.max_length,\n",
    "#         )\n",
    "# layer_idx = 0\n",
    "# print(batch_concat.keys())\n",
    "# batch['chosen_input_ids'].shape, batch_concat['concatenated_input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baukit.nethook import recursive_copy, set_requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get batch of hidden states\n",
    "from reprpo.helpers.torch import clear_mem\n",
    "from trl.trainer.utils import pad_to_length\n",
    "from torch import Tensor\n",
    "from jaxtyping import Float\n",
    "\n",
    "# @torch.cuda.amp.autocast()\n",
    "def get_hs(trainer, model, batch, collect_gradient=False):\n",
    "    model.zero_grad()\n",
    "    model.train()\n",
    "\n",
    "    if collect_gradient:\n",
    "        raise NotImplementedError('Not implemented yet, gradient is None?')\n",
    "        def make_inputs_require_grad(module, input, output):\n",
    "            output.requires_grad_(True)\n",
    "        model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n",
    "\n",
    "    \n",
    "    (\n",
    "        chosen_logps,\n",
    "        rejected_logps,\n",
    "        chosen_logits,\n",
    "        rejected_logits,\n",
    "        _,\n",
    "        chosen_hs,\n",
    "        rejected_hs,\n",
    "        chosen_attn_mask,\n",
    "        rejected_attn_mask\n",
    "    ) = trainer.concatenated_forward(trainer.model, batch)\n",
    "\n",
    "    r = dict(chosen_hs=chosen_hs, rejected_hs=rejected_hs, chosen_logps=chosen_logps, rejected_logps=rejected_logps, chosen_attn_mask=chosen_attn_mask, rejected_attn_mask=rejected_attn_mask)\n",
    "\n",
    "\n",
    "    if collect_gradient:\n",
    "        chosen_hs.retain_grad()\n",
    "        rejected_hs.retain_grad()\n",
    "        loss = (chosen_logps - rejected_logps).mean()\n",
    "        trainer.accelerator.backward(loss)\n",
    "        print('chosen_hs.grad', chosen_hs.grad)\n",
    "        assert chosen_hs.grad is not None, 'FIXME'\n",
    "        r['chosen_hs_grad'] = chosen_hs.grad\n",
    "        r['rejected_hs_grad'] = rejected_hs.grad\n",
    "\n",
    "\n",
    "\n",
    "    # get unembected hs\n",
    "    r['chosen_unemb'] = model.lm_head(chosen_hs)\n",
    "    r['rejected_unemb'] = model.lm_head(rejected_hs)\n",
    "\n",
    "    def get_layer_logps(trainer, hidden_states: Float[Tensor, 'b l t h'], labels: Float[Tensor, 'b t'], log_softmax=True):\n",
    "        # pad to length\n",
    "        hidden_states = pad_to_length(hidden_states, trainer.max_length, pad_value=0)\n",
    "        labels = pad_to_length(labels, trainer.max_length, pad_value=trainer.label_pad_token_id)\n",
    "\n",
    "        # gather for each layer\n",
    "        logps = []\n",
    "        for layer in range(hidden_states.shape[1]):\n",
    "            all_logps, size_completion = trainer.get_batch_logps(\n",
    "                hidden_states[:, layer],\n",
    "                labels,\n",
    "                label_pad_token_id=trainer.label_pad_token_id,\n",
    "                log_softmax=log_softmax\n",
    "                )\n",
    "            all_logps = all_logps / size_completion\n",
    "            logps.append(all_logps)\n",
    "        all_logps = torch.stack(logps, dim=1)\n",
    "        return all_logps\n",
    "    \n",
    "    # get fake logp from unemb_hs\n",
    "    r['chosen_gthr_logps_unemb'] = get_layer_logps(trainer, r['chosen_unemb'], batch[\"chosen_labels\"])\n",
    "    r['rejection_gthr_logps_unemb'] = get_layer_logps(trainer, r['rejected_unemb'], batch[\"rejected_labels\"])\n",
    "\n",
    "    r['chosen_gthr_unemb'] = get_layer_logps(trainer, r['chosen_unemb'], batch[\"chosen_labels\"], log_softmax=False)\n",
    "    r['rejection_gthr_unemb'] = get_layer_logps(trainer, r['rejected_unemb'], batch[\"rejected_labels\"], log_softmax=False)\n",
    "    \n",
    "    # we reuse the function, adding a fake layer dim, and squeezing it out\n",
    "    r['chosen_gthr_logits'] = get_layer_logps(trainer, chosen_logits[:, None], batch[\"chosen_labels\"], log_softmax=False).squeeze(1)\n",
    "    r['rejected_gthr_logits'] = get_layer_logps(trainer, rejected_logits[:, None], batch[\"rejected_labels\"], log_softmax=False).squeeze(1)\n",
    "\n",
    "    r = {k: recursive_copy(v, clone=True, detach=True).detach().cpu() for k, v in r.items()}\n",
    "    loss = chosen_hs = rejected_hs = chosen_logps = rejected_logps = chosen_logits = rejected_logits = None\n",
    "    clear_mem(trainer)\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed048526a3b34ab4a3af207acefbd723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([66, 1, 128, 3072])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "n = 0 \n",
    "\n",
    "dl = reprpo_trainer.get_train_dataloader()\n",
    "data = []\n",
    "for batch in tqdm(dl):\n",
    "    with reprpo_trainer.null_ref_context():\n",
    "        r = get_hs(reprpo_trainer, reprpo_trainer.model, batch)\n",
    "        data.append(r)\n",
    "        n += r['chosen_hs'].shape[0]\n",
    "        if n > 64:\n",
    "            break\n",
    "\n",
    "# concat\n",
    "data = {k: torch.cat([d[k] for d in data], dim=0) for k in data[0].keys()}\n",
    "data['chosen_hs'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_mem(reprpo_trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone this linear, using a clone of the weights and bias\n",
    "m = model.lm_head\n",
    "m2 = nn.Linear(m.in_features, m.out_features)\n",
    "m2.weight = nn.Parameter(m.weight.clone().float())\n",
    "if m.bias is not None:\n",
    "    m2.bias = nn.Parameter(m.bias.clone().float())\n",
    "m2 = m2.cpu()\n",
    "\n",
    "lm_head = m2\n",
    "m = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO try getting residual hidden states are removing what might be used by lm_head, e.g. disentangling the part not projected\n",
    "model = None\n",
    "clear_mem(reprpo_trainer)\n",
    "reprpo_trainer = None\n",
    "clear_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disentangling hiddenstates into inner and head components\n",
    "\n",
    "Hypothesis: hidden states contain both \"output-directed\" information (what will be unembedded to logits) and \"internal processing\" information. This seems possible because 1) we can unembed the hidden states 2) the hidden states remain fairly constant throughout the layer being additivly transformed by residual connections.\n",
    "\n",
    "Test: do we get a better correlation with concepts. And does it generalise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3072]), torch.Size([32011, 3072]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = 0\n",
    "C = data['chosen_hs'][:2, layer]\n",
    "C = reduce(C, 'b t h -> b h', 'mean').float()\n",
    "H = C\n",
    "\n",
    "H.shape, lm_head.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor, nn\n",
    "\n",
    "def calc_residual_projection(\n",
    "    H: Float[Tensor, \"batch hidden_size\"],\n",
    "    lm_head: nn.Linear,\n",
    "    method=0,\n",
    "    epsilon=1e-6,\n",
    ") -> Float[Tensor, \"batch hidden_size\"]:\n",
    "    \"\"\"\n",
    "    Removes the component of H that lies in the subspace defined by the LM head's weight matrix.\n",
    "    \n",
    "    Math: H_residual = H - W^T (W W^T)^-1 W H\n",
    "    where W is the LM head weight matrix.\n",
    "    \"\"\"\n",
    "    # TODO cache this?\n",
    "    W = lm_head.weight\n",
    "    WWT = W @ W.T + epsilon * torch.eye(W.shape[0], device=W.device)\n",
    "\n",
    "    # 0 torch.Size([3072, 2])\n",
    "    # 1 torch.Size([32011, 2])\n",
    "\n",
    "    if method == 0:     # fast, best, robust\n",
    "        # note the matrix is not full rank (this is natural for WWT since W is rectangular)\n",
    "        projection = torch.linalg.lstsq(WWT, (W @ H.T)).solution\n",
    "    elif method == 1:\n",
    "        projection = torch.linalg.solve(WWT, (W @ H.T))\n",
    "    elif method == 2:\n",
    "        # SLOW 5mins\n",
    "        W_pinv = torch.linalg.pinv(W @ W.T)\n",
    "        projection = W_pinv @ (W @ H.T)\n",
    "    \n",
    "    H_projected = W.T @ projection\n",
    "    return H - H_projected.T\n",
    "\n",
    "def calc_residual_unembedding(\n",
    "    H: Float[Tensor, \"batch hidden_size\"],\n",
    "    lm_head: nn.Linear\n",
    ") -> Float[Tensor, \"batch hidden_size\"]:\n",
    "    \"\"\"\n",
    "    Calculates residual by unembedding and re-embedding.\n",
    "    \n",
    "    Math: H_residual = H - W^T (W H)\n",
    "    where W is the LM head weight matrix.\n",
    "    \"\"\"\n",
    "    unembedded = lm_head(H)\n",
    "    H_projected = F.linear(unembedded, lm_head.weight.T)\n",
    "    return H - H_projected\n",
    "\n",
    "\n",
    "def calc_residual_projection_svd(\n",
    "    H: Float[Tensor, \"batch hidden_size\"],\n",
    "    lm_head: nn.Linear,\n",
    "    k: int = 5000  # number of singular values to keep. Set to H.shape[1], as that's approx the rank of W?\n",
    ") -> Float[Tensor, \"batch hidden_size\"]:\n",
    "    W = lm_head.weight\n",
    "    U, S, Vt = torch.linalg.svd(W, full_matrices=True)\n",
    "    U_k, S_k, Vt_k = U[:, :k], S[:k], Vt[:k, :]\n",
    "    W_k = U_k @ torch.diag(S_k) @ Vt_k\n",
    "    W_k += 1e-6 * torch.eye(W_k.shape[0], device=W_k.device)\n",
    "    H_projected = W_k.T @ (W_k @ H.T)\n",
    "    return H - H_projected.T\n",
    "\n",
    "def calc_residual_projection_ridge(\n",
    "    H: Float[Tensor, \"batch hidden_size\"],\n",
    "    lm_head: nn.Linear,\n",
    "    alpha: float = 1.0  # regularization strength\n",
    ") -> Float[Tensor, \"batch hidden_size\"]:\n",
    "    W = lm_head.weight\n",
    "    WWT = W @ W.T\n",
    "    reg_term = alpha * torch.eye(W.shape[0], device=W.device)\n",
    "    projection = torch.linalg.solve(WWT + reg_term, (W @ H.T))\n",
    "    H_projected = W.T @ projection\n",
    "    return H - H_projected.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify\n",
    "def check_reconstruction_error(H, W, H_residual):\n",
    "    H_reconstructed = W.T @ (W @ (H - H_residual).T)\n",
    "    error = torch.norm(H - H_reconstructed.T) / torch.norm(H)\n",
    "    print(f\"Relative reconstruction error: {error.item():.4f} (should be close to 0)\")\n",
    "    return error\n",
    "\n",
    "def check_orthogonality(H_residual, W):\n",
    "    proj = W @ H_residual.T\n",
    "    orthogonality = torch.norm(proj) / (torch.norm(W) * torch.norm(H_residual))\n",
    "    print(f\"Orthogonality measure: {orthogonality.item():.3f} (should be close to 0)\")\n",
    "    return orthogonality\n",
    "\n",
    "def check_numerical_stability(H_residual):\n",
    "    if torch.isnan(H_residual).any() or torch.isinf(H_residual).any():\n",
    "        print(\"Warning: NaN or Inf detected in output\")\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedSVDProjector:\n",
    "    def __init__(self, lm_head: nn.Linear, threshold: float = 1e-5, epsilon: float = 1e-12):\n",
    "        W = lm_head.weight\n",
    "        U, S, Vt = torch.linalg.svd(W, full_matrices=True)\n",
    "        \n",
    "\n",
    "        print(S.shape, Vt.shape)\n",
    "        print(f\"Using k: {self.k}\")\n",
    "        \n",
    "        # Precompute V * (S^2 / (S^2 + epsilon)) * V^T for stability\n",
    "        S_squared = S**2\n",
    "        stability_term = S_squared / (S_squared + epsilon)\n",
    "        self.projection_matrix = Vt[:self.k].T @ (stability_term.unsqueeze(1) * Vt[:self.k])\n",
    "        \n",
    "        # Store for potential debugging or analysis\n",
    "        self.S_k = S\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def project(self, H: Float[Tensor, \"batch hidden_size\"]) -> Float[Tensor, \"batch hidden_size\"]:\n",
    "        return H - (self.projection_matrix @ H.T).T\n",
    "\n",
    "    def estimate_error(self, H: Float[Tensor, \"batch hidden_size\"]) -> float:\n",
    "        H_residual = self.project(H)\n",
    "        relative_error = torch.norm(H_residual) / torch.norm(H)\n",
    "        return relative_error.item()\n",
    "\n",
    "    def get_condition_number(self):\n",
    "        return (self.S_k[0] / self.S_k[-1]).item()\n",
    "\n",
    "# Usage\n",
    "projector = OptimizedSVDProjector(lm_head, epsilon=1e-12)  # Adjust epsilon as needed\n",
    "H_residual = projector.project(H)\n",
    "error = projector.estimate_error(H)\n",
    "print(f\"Estimated relative error: {error}\")\n",
    "print(f\"Condition number of truncated S: {projector.get_condition_number()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative reconstruction error: 313.9899 (should be close to 0)\n",
      "Orthogonality measure: 0.017 (should be close to 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_reconstruction_error(H, lm_head.weight, H_residual)\n",
    "check_orthogonality(H_residual, lm_head.weight)\n",
    "check_numerical_stability(H_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def calc_residual_projection_lstsq(*args, **kwargs):\n",
    "    return calc_residual_projection(*args, method=0, **kwargs)\n",
    "\n",
    "def calc_residual_projection_solve(*args, **kwargs):\n",
    "    return calc_residual_projection(*args, method=1, **kwargs)\n",
    "\n",
    "def calc_residual_projection_pinv(*args, **kwargs):\n",
    "    return calc_residual_projection(*args, method=2, **kwargs)\n",
    "\n",
    "projections_methods = [\n",
    "    calc_residual_unembedding, # 5s terrible\n",
    "    calc_residual_projection_svd, # 5s terrible\n",
    "    calc_residual_projection_ridge, # 60 s\n",
    "    calc_residual_projection_lstsq, # 100s\n",
    "    calc_residual_projection_solve, \n",
    "    calc_residual_projection_pinv,\n",
    "]\n",
    "res = {}\n",
    "for fn in projections_methods:\n",
    "    name = fn.__name__\n",
    "    t0 = time.time()\n",
    "    print(f\"Running {name}... \")\n",
    "    H_residual = fn(H, lm_head)\n",
    "    print(f\"{name}, took {time.time() - t0:.2f}s\")\n",
    "    res[name] = H_residual\n",
    "\n",
    "    check_reconstruction_error(H, lm_head.weight, H_residual)\n",
    "    check_orthogonality(H_residual, lm_head.weight)\n",
    "    check_numerical_stability(H_residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        \n",
    "        calc_residual_unembedding: 0.09s\n",
    "        Relative reconstruction error: 3163144.0\n",
    "        Orthogonality measure: 0.2088203877210617 (should be close to 0)\n",
    "        \n",
    "        calc_residual_projection_svd: 6.74s\n",
    "        Relative reconstruction error: 3163145.5\n",
    "        Orthogonality measure: 0.20938220620155334 (should be close to 0)\n",
    "        \n",
    "        calc_residual_projection_ridge: 68.24s\n",
    "        Relative reconstruction error: 313.8376770019531\n",
    "        Orthogonality measure: 0.00775420805439353 (should be close to 0)\n",
    "        \n",
    "        calc_residual_projection: 109.20s\n",
    "        Relative reconstruction error: 309.99932861328125\n",
    "        Orthogonality measure: 0.012309921905398369 (should be close to 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "for K in [100, 1_000, 10_000, 100_000]:\n",
    "    t0 = time.time()\n",
    "    H_residual = calc_residual_projection_svd(H, lm_head, k=K)\n",
    "    print(f\"{time.time():.4f}: {name}, took {time.time() - t0:.2f}s, k={K}\")\n",
    "    res[name] = H_residual\n",
    "\n",
    "    check_reconstruction_error(H, lm_head.weight, H_residual)\n",
    "    check_orthogonality(H_residual, lm_head.weight)\n",
    "    check_numerical_stability(H_residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def check_full_rank(W):\n",
    "# #     WWT = W @ W.T + 1e-9 * torch.eye(W.shape[0], device=W.device)\n",
    "# #     rank = torch.linalg.matrix_rank(WWT)\n",
    "# #     full_rank = rank == min(WWT.shape)\n",
    "# #     print(f\"Rank: {rank}, Full rank: {full_rank}\")\n",
    "# #     return full_rank\n",
    "\n",
    "# def check_full_rank(W, tol=1e-6):\n",
    "#     WWT = W @ W.T + 1e-9 * torch.eye(W.shape[0], device=W.device)\n",
    "#     try:\n",
    "#         # Use SVD with some tolerance\n",
    "#         S = torch.linalg.svdvals(WWT)\n",
    "#         rank = torch.sum(S > tol * S[0])\n",
    "#         full_rank = rank == min(WWT.shape)\n",
    "#         print(f\"Rank: {rank}, Full rank: {full_rank}\")\n",
    "#     except RuntimeError as e:\n",
    "#         print(f\"SVD computation failed: {e}\")\n",
    "#         print(\"Falling back to QR decomposition for rank estimation\")\n",
    "#         try:\n",
    "#             # Use QR decomposition as a fallback\n",
    "#             Q, R = torch.linalg.qr(WWT)\n",
    "#             rank = torch.sum(torch.abs(torch.diag(R)) > tol * torch.abs(R[0,0]))\n",
    "#             full_rank = rank == min(WWT.shape)\n",
    "#             print(f\"Rank (QR method): {rank}, Full rank: {full_rank}\")\n",
    "#         except RuntimeError:\n",
    "#             print(\"QR decomposition also failed. Matrix might be too ill-conditioned.\")\n",
    "#             full_rank = False\n",
    "#     return full_rank\n",
    "\n",
    "# # check_full_rank(lm_head.weight)\n",
    "# # Rank (QR method): 3528, Full rank: False\n",
    "# # shape [32011,32011] of WtW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def compute_condition_number(W):\n",
    "# #     WWT = W @ W.T + 1e-9 * torch.eye(W.shape[0], device=W.device)\n",
    "# #     S = torch.linalg.svdvals(WWT)\n",
    "# #     condition_number = S[0] / S[-1]\n",
    "# #     print(f\"Condition number: {condition_number}\")\n",
    "# #     return condition_number\n",
    "\n",
    "# # import torch\n",
    "\n",
    "# def compute_condition_number(W, epsilon=1e-9, max_iter=100):\n",
    "#     WWT = W @ W.T + epsilon * torch.eye(W.shape[0], device=W.device)\n",
    "    \n",
    "#     # Use power iteration to estimate largest and smallest singular values\n",
    "#     def power_iteration(A, num_iterations):\n",
    "#         b_k = torch.randn(A.shape[1], 1, device=A.device)\n",
    "#         for _ in range(num_iterations):\n",
    "#             b_k = A @ (A.T @ b_k)\n",
    "#             b_k /= torch.norm(b_k)\n",
    "#         return torch.sqrt((A @ b_k).T @ (A @ b_k) / (b_k.T @ b_k)).item()\n",
    "\n",
    "#     largest_sv = power_iteration(WWT, max_iter)\n",
    "#     smallest_sv = 1 / power_iteration(torch.linalg.inv(WWT), max_iter)\n",
    "    \n",
    "#     condition_number = largest_sv / smallest_sv\n",
    "#     print(f\"Estimated condition number: {condition_number}\")\n",
    "#     return condition_number\n",
    "\n",
    "# # Try with different epsilon values\n",
    "# for eps in [1e-3, 1e-2]:\n",
    "#     print(f\"\\nTrying with epsilon = {eps}\")\n",
    "#     condition_number = compute_condition_number(lm_head.weight, epsilon=eps)\n",
    "\n",
    "# compute_condition_number(lm_head.weight)\n",
    "\n",
    "# Trying with epsilon = 1e-09\n",
    "# Estimated condition number: 38832213588.898926\n",
    "\n",
    "# Trying with epsilon = 1e-06\n",
    "# Estimated condition number: 58220872493.006836\n",
    "\n",
    "# Trying with epsilon = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     a0 = calc_residual_projection(H, lm_head, 0)\n",
    "# print(a0.shape, a0.max()) # 1m\n",
    "# a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO cache reused values\n",
    "# # TODO prototype then move to hs gather and do it batch by batch\n",
    "# # TODO test corr\n",
    "# with torch.no_grad():\n",
    "#     b = calc_residual_unembedding(H, lm_head)\n",
    "#     print('b', b.shape, b)\n",
    "\n",
    "#     a = calc_residual_projection(H, lm_head)\n",
    "#     print('a', a.shape, a)\n",
    "\n",
    "#     c = calc_residual_svd(H, 10)\n",
    "#     print('c', c.shape, c)\n",
    "\n",
    "# a,b,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 1. Projection method\n",
    "residual_proj_C = calc_residual_projection(C, lm_head)\n",
    "residual_proj_R = calc_residual_projection(R, lm_head)\n",
    "\n",
    "# 2. Unembedding method\n",
    "residual_unemb_C = calc_residual_unembedding(C, lm_head)\n",
    "residual_unemb_R = calc_residual_unembedding(R, lm_head)\n",
    "\n",
    "# 3. SVD method (adjust k as needed)\n",
    "k = 100  # Example value, adjust based on your model size\n",
    "residual_svd_C = calc_residual_svd(C, k)\n",
    "residual_svd_R = calc_residual_svd(R, k)\n",
    "\n",
    "# Test correlations\n",
    "methods = ['Original', 'Projection', 'Unembedding', 'SVD']\n",
    "C_variants = [C, residual_proj_C, residual_unemb_C, residual_svd_C]\n",
    "R_variants = [R, residual_proj_R, residual_unemb_R, residual_svd_R]\n",
    "\n",
    "for method, C_var, R_var in zip(methods, C_variants, R_variants):\n",
    "    # Correlation with logratios\n",
    "    corr_C = test_correlation(C_var, logratios)\n",
    "    corr_R = test_correlation(R_var, logratios)\n",
    "    \n",
    "    # Correlation between C and R\n",
    "    corr_CR = test_correlation(C_var, R_var)\n",
    "    \n",
    "    print(f\"{method} method:\")\n",
    "    print(f\"  Correlation with logratios: C={corr_C:.4f}, R={corr_R:.4f}\")\n",
    "    print(f\"  Correlation between C and R: {corr_CR:.4f}\")\n",
    "    \n",
    "    # Test if the difference between C and R correlates with logratios\n",
    "    diff_CR = C_var - R_var\n",
    "    corr_diff = test_correlation(diff_CR, logratios)\n",
    "    print(f\"  Correlation of (C-R) with logratios: {corr_diff:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Additional test: Check if residuals are less correlated with unembedded logits\n",
    "unembedded_logits = model.lm_head(C)  # Using C as an example\n",
    "for method, C_var in zip(methods, C_variants):\n",
    "    corr = test_correlation(C_var, unembedded_logits)\n",
    "    print(f\"{method} correlation with unembedded logits: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some data samples\n",
    "layer = 0\n",
    "C = data['chosen_hs'] # we could also consider unembedding them, whith the lm head, this might make them more interpretable, but also a bit large\n",
    "R = data['rejected_hs']\n",
    "\n",
    "CA = data['chosen_attn_mask']\n",
    "RA = data['rejected_attn_mask']\n",
    "\n",
    "M = 100\n",
    "A2 = CA * RA # use both attn masks when comparing?\n",
    "\n",
    "# choose layer, mean over tokens\n",
    "C = mult_with_attention(C, A2)[:M, layer]\n",
    "C = reduce(C, 'b t h -> b h', 'mean')\n",
    "R = mult_with_attention(R, A2)[:M, layer]\n",
    "R = reduce(R, 'b t h -> b h', 'mean')\n",
    "\n",
    "# compare two unrelated sets of samples, that way we have ones that should show the difference and ones that shouldn't show the difference we arel ooking for\n",
    "n = len(C)//2\n",
    "C1 = C[:n] # chosen pair 1\n",
    "R1 = R[:n] # rejected pair 1\n",
    "C2 = C[n:] # chosen, pair 2\n",
    "R2 = R[n:] # rejected pair 2\n",
    "\n",
    "\n",
    "# now we choose what to test correlations with. At first I tried the logprobs but they have been through a softmax which makes them relative and hard to compare to the hidden states\n",
    "# so instead we are going to try the hidden states values that corresponded to the \n",
    "\n",
    "# logratios = data['chosen_logps'] - data['rejected_logps'] # the logp is the log probability (mean per token) of this response, logratios is the log ratio of probs, this should be correlated with the direction we are seeking in the hidden states\n",
    "\n",
    "# logratios = (data['chosen_gthr_logits'] - data['rejected_gthr_logits'])#.exp()\n",
    "logratios = data['chosen_gthr_unemb'][:, layer] - data['rejection_gthr_unemb'][:, layer]\n",
    "\n",
    "# we can use this to check the null hypothesis, as the `roll` operation mixes up the batch dimension, so they are unrelated\n",
    "logratios_unrelated = logratios.roll(1, 0)\n",
    "\n",
    "logratios1 = logratios[:n]\n",
    "logratios2 = logratios[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n",
    "\n",
    "### Magnitude/norm is correlated with result?\n",
    "\n",
    "Is the amplitude of A-B related to the logp?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ur using abs corrcoef\n",
    "def acorr(A, B):\n",
    "    return np.abs(np.corrcoef(A, B, rowvar=True)[0, 1])\n",
    "\n",
    "def mag_corr(C, R, logratios):\n",
    "    hs_d = torch.norm(C-R, dim=-1, p=2) # get magnitude of the difference\n",
    "    corr = acorr(hs_d, logratios)\n",
    "    # note that after flipping we ruin the order, and they should be uncorrelated except for the middle one if it's odd\n",
    "    corr_null = acorr(hs_d, logratios.roll(1, 0))\n",
    "    return corr, corr_null\n",
    "    \n",
    "\n",
    "print('does the magnitude of the hs_unemb correlate with the prob ratio (across batches)? if so it\\'s a good repr')\n",
    "\n",
    "print(\"should be significantly bigger\")\n",
    "corr, corr_null = mag_corr(C1, R1, logratios1)\n",
    "shypothesis(\"corr>corr_null\", variables=dict(corr=corr, corr_null=corr_null))\n",
    "\n",
    "print(\"should pass\")\n",
    "corr, corr_null = mag_corr(C2, R2, logratios2)\n",
    "shypothesis(\"corr>corr_null\", variables=dict(corr=corr, corr_null=corr_null))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is it significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_test(C, R, logratios, n_permutations=100):\n",
    "\n",
    "    n_permutations = min(n_permutations, len(logratios))\n",
    "\n",
    "    true_corr = np.corrcoef(\n",
    "        torch.norm(C - R, dim=-1, p=2), \n",
    "        logratios)[0, 1]\n",
    "    \n",
    "    null_corrs = []\n",
    "    for _ in range(n_permutations):\n",
    "        perm_logratios = logratios[torch.randperm(len(logratios))]\n",
    "        null_corrs.append(np.corrcoef(torch.norm(C - R, dim=-1, p=2), perm_logratios)[0, 1])\n",
    "    \n",
    "    plt.hist(null_corrs, bins=50, label='null_corrs')\n",
    "    plt.axvline(true_corr, color='r', linestyle='dashed', linewidth=2, label='true_corr')\n",
    "    plt.title('Permutation Test')\n",
    "    plt.xlabel('Correlation')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    p_value = (np.sum(np.abs(null_corrs) >= np.abs(true_corr)) + 1) / (n_permutations + 1)\n",
    "    print(f\"P-value: {p_value:.3f}\")\n",
    "\n",
    "print('is the red line outside the distribution of null correlations?')\n",
    "permutation_test(C1, R1, logratios1)\n",
    "# FAIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spearmanr kendalltau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "\n",
    "def correlation_dist(C, R, logratios):\n",
    "    distances = torch.norm(C - R, dim=-1)\n",
    "    logratios = logratios.cpu().numpy()\n",
    "    corr, p = stats.spearmanr(distances, logratios)\n",
    "    return np.abs(corr), p\n",
    "\n",
    "print('spearmanr measure of dist ~ logratios should be large and significant')\n",
    "corr, p = correlation_dist(C1, R1, logratios1)\n",
    "n_corr, n_p = correlation_dist(C1, R1, logratios1.roll(1, 0))\n",
    "shypothesis('corr>n_corr', variables=locals())\n",
    "shypothesis('p<0.2', variables=locals())\n",
    "\n",
    "def kendalltau_dist(C, R, logratios):\n",
    "    distances = torch.norm(C - R, dim=-1)\n",
    "    logratios = logratios.cpu().numpy()\n",
    "    corr, p = stats.kendalltau(distances, logratios)\n",
    "    return np.abs(corr), p\n",
    "\n",
    "print('kendalltau measure of dist ~ logratios should be large and significant')\n",
    "corr, p = kendalltau_dist(C1, R1, logratios1)\n",
    "n_corr, n_p = kendalltau_dist(C1, R1, logratios1.roll(1, 0))\n",
    "shypothesis('corr>n_corr', variables=locals())\n",
    "shypothesis('p<0.2', variables=locals())\n",
    "shypothesis('n_p<0.2', variables=locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def angle_similarity_correlation(C, R, logratios):\n",
    "    C_norm = F.normalize(C, p=2, dim=-1)\n",
    "    R_norm = F.normalize(R, p=2, dim=-1)\n",
    "    angle = torch.acos(torch.clamp(torch.sum(C_norm * R_norm, dim=-1), -1.0, 1.0))\n",
    "    corrs = np.corrcoef(angle.cpu().numpy(), logratios.cpu().numpy())[0, 1]\n",
    "    return np.abs(corrs)\n",
    "\n",
    "def norm_difference_correlation(C, R, logratios):\n",
    "    norm_diff = torch.norm(C, dim=-1) - torch.norm(R, dim=-1)\n",
    "    corrs = np.corrcoef(norm_diff.cpu().numpy(), logratios.cpu().numpy())[0, 1]\n",
    "    return np.abs(corrs)\n",
    "\n",
    "def magnitude_correlation(C, R, logratios):\n",
    "    magnitude_diff = torch.abs(C - R).mean(dim=-1)\n",
    "    corrs = np.corrcoef(magnitude_diff.cpu().numpy(), logratios.cpu().numpy())[0, 1]\n",
    "    return np.abs(corrs)\n",
    "\n",
    "def inner_product_correlation(C, R, logratios):\n",
    "    inner_prod = torch.sum(C * R, dim=-1)\n",
    "    corrs = np.corrcoef(inner_prod.cpu().numpy(), logratios.cpu().numpy())[0, 1]\n",
    "    return np.abs(corrs)\n",
    "\n",
    "# Usage\n",
    "def run_all_tests(C1, R1, C2, R2, logratios1, logratios2):\n",
    "    tests = [\n",
    "        angle_similarity_correlation,\n",
    "        norm_difference_correlation,\n",
    "        magnitude_correlation,\n",
    "        inner_product_correlation\n",
    "    ]\n",
    "    \n",
    "    for test in tests:\n",
    "        print(f\"\\nTesting {test.__name__}:\")\n",
    "        c111 = test(C1, R1, logratios1)\n",
    "        c222 = test(C2, R2, logratios2)\n",
    "        c112 = test(C1, R1, logratios1.roll(1, 0))\n",
    "        c221 = test(C2, R2, logratios2.roll(1, 0))\n",
    "        \n",
    "        print('Similarity of related ones should be higher')\n",
    "        shypothesis('c111 > c112', variables=locals(), verbose=True)\n",
    "        shypothesis('c222 > c221', variables=locals())\n",
    "        # Uncomment these if you want to test across different pairs\n",
    "        # shypothesis('c111 > c221', variables=locals())\n",
    "        # shypothesis('c222 > c112', variables=locals())\n",
    "\n",
    "# Run all tests\n",
    "run_all_tests(C1, R1, C2, R2, logratios1, logratios2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H: cosine_similarity_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_correlation(C, R, logratios):\n",
    "    cos_sim = F.cosine_similarity(C, R, dim=-1)  # [batch, layers, tokens]\n",
    "\n",
    "    corrs = np.corrcoef(cos_sim, logratios)[0, 1] \n",
    "    \n",
    "    return np.abs(corrs)\n",
    "\n",
    "c111 = cosine_similarity_correlation(C1, R1, logratios1)\n",
    "c222 = cosine_similarity_correlation(C2, R2, logratios2)\n",
    "c112 = cosine_similarity_correlation(C1, R1, logratios1.roll(1, 0))\n",
    "c221 = cosine_similarity_correlation(C2, R2, logratios2.roll(1, 0))\n",
    "\n",
    "print('cosine similarity of related ones should be higher')\n",
    "shypothesis('c111 > c112', variables=locals(), verbose=True)\n",
    "shypothesis('c222 > c221', variables=locals())\n",
    "# shypothesis('c111 > c221', variables=locals())\n",
    "# shypothesis('c222 > c112', variables=locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(p, q):\n",
    "    return (p * (p / q).log()).sum(dim=-1)\n",
    "\n",
    "def js_divergence(p, q):\n",
    "    m = 0.5 * (p + q)\n",
    "    return 0.5 * kl_divergence(p, m) + 0.5 * kl_divergence(q, m)\n",
    "\n",
    "def mutual_information(C, R):\n",
    "    joint = torch.einsum('bt,bt->bt', C, R)\n",
    "    joint = joint / joint.sum(dim=-1, keepdim=True)\n",
    "    mi = kl_divergence(joint, C * R)\n",
    "    return mi\n",
    "\n",
    "def info_theoretic_correlation(C, R, logratios):\n",
    "    C_norm = F.softmax(C, dim=-1)\n",
    "    R_norm = F.softmax(R, dim=-1)\n",
    "    \n",
    "    kl = kl_divergence(C_norm, R_norm)#.mean(dim=(1,))\n",
    "    js = js_divergence(C_norm, R_norm)#.mean(dim=(1,))\n",
    "    mi = mutual_information(C_norm, R_norm)#.mean(dim=(1,))\n",
    "    \n",
    "    kl_corr = np.abs(np.corrcoef(kl.cpu().numpy(), logratios)[0,1])\n",
    "    js_corr = np.abs(np.corrcoef(js.cpu().numpy(), logratios)[0,1])\n",
    "    mi_corr = np.abs(np.corrcoef(mi.cpu().numpy(), logratios)[0,1])\n",
    "    \n",
    "    return kl_corr, js_corr, mi_corr\n",
    "\n",
    "print(\"C1-R1 with logratios1:\")\n",
    "kl1,js1,mi1 = info_theoretic_correlation(C1, R1, logratios1)\n",
    "kl2,js2,mi2 = info_theoretic_correlation(C1, R1, logratios1.roll(1, 0))\n",
    "shypothesis('kl1>kl2', variables=locals())\n",
    "shypothesis('js1>js2', variables=locals())\n",
    "shypothesis('mi1>mi2', variables=locals())\n",
    "\n",
    "print(\"C2-R2 with logratios2:\")\n",
    "kl1,js1,mi1 = info_theoretic_correlation(C2, R2, logratios2)\n",
    "kl2,js2,mi2 = info_theoretic_correlation(C2, R2, logratios2.roll(1, 0))\n",
    "shypothesis('kl1>kl2', variables=locals())\n",
    "shypothesis('js1>js2', variables=locals())\n",
    "shypothesis('mi1>mi2', variables=locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def rotation_test(C, R, logratios):\n",
    "    def optimal_rotation(A, B):\n",
    "\n",
    "        corr_matrix = torch.mm(A.t(), B)\n",
    "        Q, _ = torch.linalg.qr(corr_matrix)\n",
    "        return Q\n",
    "    \n",
    "    def optimal_rotation(A, B):\n",
    "        corr_matrix = torch.mm(A.t(), B)\n",
    "        U, _, V = torch.linalg.svd(corr_matrix)\n",
    "        return torch.mm(U, V.t())\n",
    "    \n",
    "    R_opt = optimal_rotation(C, R)\n",
    "    R_rotated = torch.matmul(R, R_opt.t())\n",
    "    \n",
    "    a = C - R\n",
    "    b = C - R_rotated\n",
    "\n",
    "    # a -= a.mean(dim=-1, keepdim=True)\n",
    "    # b -= b.mean(dim=-1, keepdim=True)\n",
    "    # a /= torch.norm(a, dim=0, p=2, keepdim=True)\n",
    "    # b /= torch.norm(b, dim=0, p=2, keepdim=True)\n",
    "\n",
    "    original_diff = torch.norm(a, p=2)\n",
    "    rotated_diff = torch.norm(b, p=2)\n",
    "    \n",
    "    print(f\"Original difference: {original_diff}\")\n",
    "    print(f\"Rotated difference: {rotated_diff}\")\n",
    "    print(f\"Improvement: {(original_diff - rotated_diff) / original_diff * 100:.2f}%\")\n",
    "    \n",
    "    c = np.abs(np.corrcoef(torch.norm(a, dim=-1, p=2), logratios)[0, 1])\n",
    "    r = np.abs(np.corrcoef(torch.norm(b, dim=-1, p=2), logratios)[0, 1])\n",
    "    print(f\"Correlation before rotation: {c:.2f}\")\n",
    "    print(f\"Correlation after rotation: {r:.2f}\")\n",
    "    print(f\"Improvement: {(r - c) / c * 100:.2f}%\")\n",
    "    print()\n",
    "    shypothesis('r > c', variables=dict(r=r, c=c))\n",
    "    return R_opt, R_rotated\n",
    "\n",
    "# maybe I should softmax first?\n",
    "R_opt, R_rotated = rotation_test(C1, R1, logratios1)\n",
    "R_opt, R_rotated = rotation_test(C2, R2, logratios2)\n",
    "\n",
    "R_opt, R_rotated = rotation_test(C2, R2, logratios2.roll(1, 0))\n",
    "R_opt, R_rotated = rotation_test(C1, R1, logratios1.roll(1, 0))\n",
    "# R_opt, R_rotated = rotation_test(C2, R2, logratios1)\n",
    "# R_opt, R_rotated = rotation_test(C1, R1, logratios2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA):\n",
    "Apply PCA to (A - B) pairs. If differences are primarily rotational, most variance should be explained by a few components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def pca_exp_var(X, Y):\n",
    "    diffs = (X - Y).numpy()\n",
    "    s = diffs.std()\n",
    "    diffs /= s\n",
    "    # axis 1, norm each neuron across the batch\n",
    "    # diffs /= diffs.std(1, keepdims=True) + 1e-3\n",
    "    pca = PCA(n_components=5)\n",
    "    pca.fit(diffs)\n",
    "    r = pca.explained_variance_ratio_.mean()\n",
    "    return r\n",
    "\n",
    "\n",
    "# \n",
    "e_a1b1 = pca_exp_var(C1, R1)\n",
    "e_a1b1roll = pca_exp_var(C1, R1.roll(1, 0))\n",
    "shypothesis('e_a1b1 > e_a1b1roll', variables=locals())\n",
    "\n",
    "e_a1b1 = pca_exp_var(C2, R2)\n",
    "e_a1b1roll = pca_exp_var(C2, R2.roll(1, 0))\n",
    "shypothesis('e_a1b1 > e_a1b1roll', variables=locals())\n",
    "\n",
    "# e_a1a2 = pca_exp_var(C1, C2)\n",
    "# e_a1b2 = pca_exp_var(C1, R2)\n",
    "# e_b1b2 = pca_exp_var(R1, R2)\n",
    "# e_b1a1 = pca_exp_var(R1, C1)\n",
    "# e_b1a2 = pca_exp_var(R1, C2)\n",
    "\n",
    "# print('PCA explained variance ratio')\n",
    "# print('A1-B1', e_a1b2)\n",
    "# shypothesis('e_a1b1 > e_a1a2', variables=locals())\n",
    "# shypothesis('e_a1b1 > e_a1b2', variables=locals())\n",
    "# shypothesis('e_a1b1 > e_b1b2', variables=locals()) # should be inconclusive\n",
    "# e_a1b2, e_a1a2, e_a1b2, e_b1b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mutual_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "def mutual_information(C, R, logratios):\n",
    "    distances = torch.norm(C - R, dim=-1, p=2, keepdim=True)\n",
    "    mi = mutual_info_regression(distances, logratios)[0]\n",
    "    print(f\"Mutual Information: {mi}\")\n",
    "    return mi\n",
    "\n",
    "print('do the paired hs have information shared with the logratios?')\n",
    "mi1 = mutual_information(C1, R1, logratios1)\n",
    "mi2 = mutual_information(C1, R1, logratios1.roll(1, 0));\n",
    "shypothesis('mi1 > mi2', variables=locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frobenius Norm Comparison:\n",
    "Compare ||A - B||_F with ||A - RB||_F where R is the optimal rotation. The latter should be significantly smaller if rotations capture most differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_comparison(A, B):\n",
    "    diff_norm = torch.norm(A - B)\n",
    "    corr = torch.mm(A, B.t())\n",
    "    U, _, V = torch.svd(corr)\n",
    "    R = torch.mm(U, V.t())\n",
    "\n",
    "    B_rot = torch.mm(R, B)\n",
    "\n",
    "    rot_diff_norm = torch.norm(A - B_rot)\n",
    "    return diff_norm, rot_diff_norm\n",
    "\n",
    "print('passing means the paired samples can be rotated')\n",
    "diff_norm, rot_diff_norm= norm_comparison(C1, R1)\n",
    "shypothesis('rot_diff_norm < diff_norm', variables=locals())\n",
    "\n",
    "print('passing means the paired samples can be rotated')\n",
    "diff_norm, rot_diff_norm= norm_comparison(C2, R2)\n",
    "# print(diff_norm, rot_diff_norm)\n",
    "shypothesis('rot_diff_norm < diff_norm', variables=locals())\n",
    "\n",
    "print('passing means the unrelated samples can be rotated')\n",
    "diff_norm, rot_diff_norm= norm_comparison(C1, R2)\n",
    "shypothesis('rot_diff_norm > diff_norm', variables=locals())\n",
    "\n",
    "print('passing means the unrelated samples can be rotated')\n",
    "diff_norm, rot_diff_norm= norm_comparison(C2, R1)\n",
    "# print(diff_norm, rot_diff_norm)\n",
    "shypothesis('rot_diff_norm > diff_norm', variables=locals())\n",
    "\n",
    "# UNEEXPECTED result: C and R are already in a similar direction! While unrelated ones are not\n",
    "# this implies that the preference concept that we want is not a factor of directions of the hidden states, but rather the magnitude of the difference between the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('are the magnitudes related?')\n",
    "c_norms = torch.norm(C1, dim=-1)\n",
    "r_norms = torch.norm(R1, dim=-1)\n",
    "norm_diffs = c_norms - r_norms\n",
    "corr1 = np.abs(np.corrcoef(norm_diffs.flatten(), logratios1)[0, 1])\n",
    "print(f\"Correlation of norm differences with logratios: {corr:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "c_norms = torch.norm(C1, dim=-1)\n",
    "r_norms = torch.norm(R1, dim=-1)\n",
    "norm_diffs = c_norms - r_norms\n",
    "corr2 = np.abs(np.corrcoef(norm_diffs.flatten(), logratios2)[0, 1])\n",
    "print(f\"Correlation of norm differences with logratios: {corr:.2f}\")\n",
    "\n",
    "shypothesis('corr1 > corr2', variables=locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_norm, rot_diff_norm= norm_comparison(C1, R2)\n",
    "diff_norm, rot_diff_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Angle Histogram:\n",
    "Plot histogram of angles between corresponding columns of A and B. Should be concentrated if differences are mainly rotational."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_histogram(A, B):\n",
    "    cos_sims = torch.sum(A * B, dim=0) / (torch.norm(A, dim=0) * torch.norm(B, dim=0))\n",
    "    angles = torch.acos(cos_sims) * 180 / np.pi\n",
    "    plt.hist(angles.numpy())\n",
    "    plt.show()\n",
    "\n",
    "angle_histogram(C1, R1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subspace_scaling(C, R, n_subspaces=10):\n",
    "    pca = PCA(n_components=n_subspaces)\n",
    "    pca.fit(np.vstack((C.reshape(-1, C.shape[-1]), R.reshape(-1, R.shape[-1]))))\n",
    "    c_proj = pca.transform(C.reshape(-1, C.shape[-1]))\n",
    "    r_proj = pca.transform(R.reshape(-1, R.shape[-1]))\n",
    "    scale_ratios = np.linalg.norm(c_proj, axis=0) / np.linalg.norm(r_proj, axis=0)\n",
    "    return scale_ratios\n",
    "\n",
    "scales = subspace_scaling(C1, R1)\n",
    "print(\"Subspace scaling ratios:\", scales)\n",
    "\n",
    "scales = subspace_scaling(C1, R2)\n",
    "print(\"Subspace scaling ratios:\", scales)\n",
    "\n",
    "# ??\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsity_correlation(C, R, logratios):\n",
    "    c_sparsity = (np.abs(C) > 1e-2).float().mean(dim=-1)\n",
    "    r_sparsity = (np.abs(R) > 1e-2).float().mean(dim=-1)\n",
    "    sparsity_diff = c_sparsity - r_sparsity\n",
    "    corr = np.corrcoef(sparsity_diff.flatten(), logratios)[0, 1]\n",
    "    print(f\"Correlation of sparsity differences with logratios: {corr}\")\n",
    "    return np.abs(corr)\n",
    "\n",
    "c111 = sparsity_correlation(C1, R1, logratios1)\n",
    "c222 = sparsity_correlation(C2, R2, logratios2)\n",
    "\n",
    "print('unrelated, should be null results')\n",
    "c112 = sparsity_correlation(C1, R1, logratios2)\n",
    "c221 = sparsity_correlation(C2, R2, logratios1)\n",
    "\n",
    "shypothesis('c111 > c112', variables=locals())\n",
    "shypothesis('c222 > c221', variables=locals())\n",
    "shypothesis('c111 > c112', variables=locals())\n",
    "shypothesis('c222 > c221', variables=locals())\n",
    "\n",
    "# !!! SUCCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fft import fft\n",
    "\n",
    "def fft_analysis(C, R):\n",
    "    C_fft = fft(C.cpu().numpy(), axis=-1)\n",
    "    R_fft = fft(R.cpu().numpy(), axis=-1)\n",
    "    return C_fft - R_fft\n",
    "\n",
    "def fft_corr(C, R, logratios):\n",
    "    fft_diff = fft_analysis(C, R)\n",
    "    fft_norm = np.linalg.norm(fft_diff, axis=-1)\n",
    "    fft_corr = np.abs(np.corrcoef(fft_norm, logratios)[0,1])\n",
    "    return fft_corr\n",
    "\n",
    "c1 = fft_corr(C1, R1, logratios1)\n",
    "c1n = fft_corr(C1, R1, logratios1.roll(1, 0))\n",
    "shypothesis('c1 > c1n', variables=locals(), verbose=True)\n",
    "\n",
    "c2 = fft_corr(C2, R2, logratios2)\n",
    "c2n = fft_corr(C2, R2, logratios2.roll(1, 0))\n",
    "shypothesis('c2 > c2n', variables=locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def non_lin_corr(C, R, logratios):\n",
    "    C = symlog(C)\n",
    "    R = symlog(R)\n",
    "    nl_diff = C - R\n",
    "    nl_norm = torch.norm(nl_diff, dim=-1)\n",
    "    nl_corr = np.corrcoef(nl_norm, logratios)[0,1]\n",
    "    corr_null = np.corrcoef(nl_norm, logratios.roll(1,0))[0,1]\n",
    "    shypothesis('abs(nl_corr) > abs(corr_null)', variables=locals(), verbose=True)\n",
    "\n",
    "print('are the right pairs more correlate if we take the diff of the symlog?')\n",
    "non_lin_corr(C1, R1, logratios1)\n",
    "non_lin_corr(C2, R2, logratios2)\n",
    "# but it's the same result without symlog..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_positioning(C, R, ref_point):\n",
    "    C_rel = C - ref_point\n",
    "    R_rel = R - ref_point\n",
    "    return torch.norm(C_rel, dim=-1) - torch.norm(R_rel, dim=-1)\n",
    "\n",
    "# Using mean of C2 as reference point\n",
    "ref_point = C2.mean(dim=(0), keepdim=True)\n",
    "rel_diff = relative_positioning(C1, R1, ref_point)\n",
    "rel_corr = np.abs(np.corrcoef(rel_diff.flatten(), logratios1)[0,1])\n",
    "rel_corr2 = np.abs(np.corrcoef(rel_diff.flatten(), logratios1.roll(1, 0))[0,1])\n",
    "shypothesis('rel_corr > rel_corr2', variables=locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return F.cosine_similarity(a, b, dim=-1)\n",
    "\n",
    "c1r1_sim = cosine_similarity(C1, R1)\n",
    "c2r2_sim = cosine_similarity(C2, R2)\n",
    "\n",
    "c1r1_sim_corr = np.corrcoef(c1r1_sim.flatten(), logratios1)[0,1]\n",
    "c2r2_sim_corr = np.corrcoef(c2r2_sim.flatten(), logratios2)[0,1]\n",
    "c1r1null_sim_corr = np.corrcoef(c1r1_sim.flatten(), logratios1.roll(1, 0))[0,1]\n",
    "c2r2null_sim_corr = np.corrcoef(c2r2_sim.flatten(), logratios2.roll(1, 0))[0,1]\n",
    "shypothesis('c1r1_sim_corr > c1r1null_sim_corr', variables=locals())\n",
    "shypothesis('c2r2_sim_corr > c2r2null_sim_corr', variables=locals())\n",
    "\n",
    "print(f\"C1-R1 similarity correlation with logratio: {c1r1_sim_corr}\")\n",
    "print(f\"C2-R2 similarity correlation with logratio: {c2r2_sim_corr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_position(x, global_mean):\n",
    "    return F.cosine_similarity(x - global_mean, global_mean, dim=-1)\n",
    "\n",
    "global_mean = torch.cat([C1, R1, C2, R2], dim=0).mean(dim=(0), keepdim=True)\n",
    "\n",
    "def glob_pos_corr(C, R, logratios):\n",
    "    c_global = global_position(C, global_mean)\n",
    "    r_global = global_position(R, global_mean)\n",
    "    global_diff = c_global - r_global\n",
    "    global_corr = np.corrcoef(global_diff.flatten(), logratios)[0,1]\n",
    "    return np.abs(global_corr)\n",
    "\n",
    "print('if we project hs onto the global mean then correlate...')\n",
    "c1 = glob_pos_corr(C1, R1, logratios1)\n",
    "c2 = glob_pos_corr(C2, R2, logratios2)\n",
    "c3n = glob_pos_corr(C1, R1, logratios1.roll(1, 0))\n",
    "c4n = glob_pos_corr(C2, R2, logratios2.roll(1, 0))\n",
    "shypothesis('c1 > c3n', variables=locals())\n",
    "shypothesis('c2 > c4n', variables=locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_position(a, b, ref):\n",
    "    a_rel = F.cosine_similarity(a - ref, ref, dim=-1)\n",
    "    b_rel = F.cosine_similarity(b - ref, ref, dim=-1)\n",
    "    return a_rel - b_rel\n",
    "\n",
    "# Use C2 mean as reference\n",
    "def rel_pos_corr(C1, R1, C2, logratios1):\n",
    "    ref = C2.mean(dim=(0), keepdim=True)\n",
    "    rel_pos1 = relative_position(C1, R1, ref)\n",
    "    rel_pos_corr = np.abs(np.corrcoef(rel_pos1.flatten(), logratios1)[0,1])\n",
    "    return rel_pos_corr\n",
    "\n",
    "c1 = rel_pos_corr(C1, R1, C2, logratios1)\n",
    "c2 = rel_pos_corr(C2, R2, C1, logratios2)\n",
    "c3n = rel_pos_corr(C1, R1, C2, logratios1.roll(1, 0))\n",
    "c4n = rel_pos_corr(C2, R2, C1, logratios2.roll(1, 0))\n",
    "shypothesis('c1 > c3n', variables=locals())\n",
    "shypothesis('c2 > c4n', variables=locals())\n",
    "print(f\"Relative position correlation: {c1}\")\n",
    "print(f\"Relative position correlation (null): {c2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projection_correlation(C, R, logratios):\n",
    "    # Compute the mean difference vector\n",
    "    mean_diff = (C - R).mean(dim=0)\n",
    "\n",
    "    # Project samples onto this direction\n",
    "    c_proj = F.cosine_similarity(C, mean_diff.unsqueeze(0), dim=-1)\n",
    "    r_proj = F.cosine_similarity(R, mean_diff.unsqueeze(0), dim=-1)\n",
    "    proj_diff = c_proj - r_proj\n",
    "\n",
    "    proj_corr = np.abs(np.corrcoef(proj_diff, logratios)[0,1])\n",
    "    return proj_corr\n",
    "\n",
    "proj_corr1 = projection_correlation(C1, R1, logratios1)\n",
    "print(f\"Preference direction projection correlation: {proj_corr1}\")\n",
    "\n",
    "proj_corr2 = projection_correlation(C2, R2, logratios2)\n",
    "print(f\"Preference direction projection correlation: {proj_corr2}\")\n",
    "\n",
    "proj_corr3 = projection_correlation(C2, R2, logratios2.roll(1, 0))\n",
    "print(f\"Preference direction projection correlation: {proj_corr3}\")\n",
    "\n",
    "\n",
    "proj_corr4 = projection_correlation(C1, R1, logratios1.roll(1, 0))\n",
    "print(f\"Preference direction projection correlation: {proj_corr4}\")\n",
    "\n",
    "shypothesis('proj_corr1 > proj_corr4', variables=locals())\n",
    "shypothesis('proj_corr2 > proj_corr3', variables=locals())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_corr(C, R, logratios):\n",
    "    c_var = C.var(dim=-1)\n",
    "    r_var = R.var(dim=-1)\n",
    "    var_diff = c_var - r_var\n",
    "\n",
    "    var_corr = np.corrcoef(var_diff.flatten(), logratios)[0,1]\n",
    "    return var_corr\n",
    "\n",
    "\n",
    "vc0 = var_corr(C1, R1, logratios1)\n",
    "print(f\"Variance difference correlation: {vc0}\")\n",
    "\n",
    "vc1 = var_corr(C2, R2, logratios2)\n",
    "print(f\"Variance difference correlation: {vc1}\")\n",
    "\n",
    "vc2 = var_corr(C2, R2, logratios2.roll(1, 0))\n",
    "print(f\"Variance difference correlation: {vc2}\")\n",
    "\n",
    "vc3 = var_corr(C1, R1, logratios1.roll(1, 0))\n",
    "print(f\"Variance difference correlation: {vc3}\")\n",
    "shypothesis('abs(vc0) > abs(vc3)', variables=locals(), verbose=1)\n",
    "shypothesis('abs(vc1) > abs(vc2)', variables=locals(), verbose=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
