{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with hidden states\n",
    "\n",
    "Question, is there a better representation of concepts in hidden states?\n",
    "\n",
    "Setup: we use DPO setup, with a chosen and rejected string. We then generate a set of hidden states, and compare the hidden states of the chosen and rejected string.\n",
    "\n",
    "Goal: better generalisation of desired behavuour by changing the internal representation of policy rather than directly changing the policy\n",
    "\n",
    "  - Hypothesis: rejected and chosen hidden states will - on mean - be best representated as rotations from each other\n",
    "  - alternate: either mean mass diff (linear) or no repr will be better\n",
    "  - metric: manual generation getting output while maintaining coherency, prediction other sets of hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import DPOTrainer\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Union\n",
    "from einops import rearrange\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from reprpo.helpers.adapters import set_adapter\n",
    "from einops import rearrange\n",
    "from matplotlib import pyplot as plt\n",
    "from reprpo import silence\n",
    "from reprpo.gen import generation_test\n",
    "\n",
    "from reprpo.trainer import mean_with_attention, symlog, mult_with_attention\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from einops import reduce\n",
    "from reprpo.trainer import collect_hs, ReprPOConfig, ReprPOTrainer, normalize_output, normalize_per\n",
    "from reprpo.helpers.shypothesis import shypothesis\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2db15ac0b70148bf90d212bd99cf44e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 41943040 || all params: 4582543360 || trainable%: 0.9152786281546499\n"
     ]
    }
   ],
   "source": [
    "from reprpo.models.load import load_model, print_trainable_parameters\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "model_name = \"NousResearch/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "## Big adapter\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16, \n",
    "    r=16,\n",
    "    lora_dropout=0.0,\n",
    "    use_rslora=False,\n",
    "    # use_dora=True,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    ")\n",
    "\n",
    "\n",
    "model, tokenizer = load_model(model_name, )\n",
    "# from trl.trainer.utils import peft_module_casting_to_bf16\n",
    "# peft_module_casting_to_bf16(model)\n",
    "adapter_name='ReprPO2'\n",
    "model = prepare_model_for_kbit_training(model, {'use_gradient_checkpointing': True})\n",
    "model = get_peft_model(model, peft_config, adapter_name=adapter_name)\n",
    "print_trainable_parameters(model)\n",
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['base_model.model.model.embed_tokens.weight', 'base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.0.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.0.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.0.input_layernorm.weight', 'base_model.model.model.layers.0.post_attention_layernorm.weight', 'base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.1.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.1.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.1.input_layernorm.weight', 'base_model.model.model.layers.1.post_attention_layernorm.weight', 'base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.2.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.2.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.2.input_layernorm.weight', 'base_model.model.model.layers.2.post_attention_layernorm.weight', 'base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.3.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.3.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.3.input_layernorm.weight', 'base_model.model.model.layers.3.post_attention_layernorm.weight', 'base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.4.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.4.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.4.input_layernorm.weight', 'base_model.model.model.layers.4.post_attention_layernorm.weight', 'base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.5.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.5.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.5.input_layernorm.weight', 'base_model.model.model.layers.5.post_attention_layernorm.weight', 'base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.6.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.6.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.6.input_layernorm.weight', 'base_model.model.model.layers.6.post_attention_layernorm.weight', 'base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.7.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.7.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.7.input_layernorm.weight', 'base_model.model.model.layers.7.post_attention_layernorm.weight', 'base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.8.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.8.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.8.input_layernorm.weight', 'base_model.model.model.layers.8.post_attention_layernorm.weight', 'base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.9.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.9.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.9.input_layernorm.weight', 'base_model.model.model.layers.9.post_attention_layernorm.weight', 'base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.10.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.10.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.10.input_layernorm.weight', 'base_model.model.model.layers.10.post_attention_layernorm.weight', 'base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.11.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.11.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.11.input_layernorm.weight', 'base_model.model.model.layers.11.post_attention_layernorm.weight', 'base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.12.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.12.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.12.input_layernorm.weight', 'base_model.model.model.layers.12.post_attention_layernorm.weight', 'base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.13.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.13.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.13.input_layernorm.weight', 'base_model.model.model.layers.13.post_attention_layernorm.weight', 'base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.14.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.14.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.14.input_layernorm.weight', 'base_model.model.model.layers.14.post_attention_layernorm.weight', 'base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.15.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.15.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.15.input_layernorm.weight', 'base_model.model.model.layers.15.post_attention_layernorm.weight', 'base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.16.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.16.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.16.input_layernorm.weight', 'base_model.model.model.layers.16.post_attention_layernorm.weight', 'base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.17.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.17.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.17.input_layernorm.weight', 'base_model.model.model.layers.17.post_attention_layernorm.weight', 'base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.18.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.18.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.18.input_layernorm.weight', 'base_model.model.model.layers.18.post_attention_layernorm.weight', 'base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.19.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.19.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.19.input_layernorm.weight', 'base_model.model.model.layers.19.post_attention_layernorm.weight', 'base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.20.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.20.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.20.input_layernorm.weight', 'base_model.model.model.layers.20.post_attention_layernorm.weight', 'base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.21.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.21.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.21.input_layernorm.weight', 'base_model.model.model.layers.21.post_attention_layernorm.weight', 'base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.22.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.22.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.22.input_layernorm.weight', 'base_model.model.model.layers.22.post_attention_layernorm.weight', 'base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.23.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.23.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.23.input_layernorm.weight', 'base_model.model.model.layers.23.post_attention_layernorm.weight', 'base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.24.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.24.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.24.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.24.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.24.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.24.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.24.input_layernorm.weight', 'base_model.model.model.layers.24.post_attention_layernorm.weight', 'base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.25.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.25.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.25.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.25.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.25.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.25.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.25.input_layernorm.weight', 'base_model.model.model.layers.25.post_attention_layernorm.weight', 'base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.26.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.26.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.26.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.26.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.26.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.26.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.26.input_layernorm.weight', 'base_model.model.model.layers.26.post_attention_layernorm.weight', 'base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.27.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.27.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.27.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.27.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.27.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.27.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.27.input_layernorm.weight', 'base_model.model.model.layers.27.post_attention_layernorm.weight', 'base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.28.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.28.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.28.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.28.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.28.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.28.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.28.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.28.input_layernorm.weight', 'base_model.model.model.layers.28.post_attention_layernorm.weight', 'base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.29.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.29.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.29.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.29.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.29.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.29.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.29.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.29.input_layernorm.weight', 'base_model.model.model.layers.29.post_attention_layernorm.weight', 'base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.30.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.30.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.30.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.30.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.30.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.30.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.30.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.30.input_layernorm.weight', 'base_model.model.model.layers.30.post_attention_layernorm.weight', 'base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.31.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.31.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.31.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.31.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.31.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.31.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.31.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.31.input_layernorm.weight', 'base_model.model.model.layers.31.post_attention_layernorm.weight', 'base_model.model.model.norm.weight', 'base_model.model.lm_head.weight'], unexpected_keys=[])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpo_adapter_f = './output-dir/dpo/DPO'\n",
    "model.load_adapter(dpo_adapter_f, 'DPO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # QC model and adapter is coherent\n",
    "# generation_test(model, tokenizer, max_new_tokens=24, system='no yapping', adapter_names=[None, 'DPO'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DPO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected'],\n",
       "        num_rows: 160\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected'],\n",
       "        num_rows: 160\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sample(dataset, N):\n",
    "    return (dataset\n",
    "            .shuffle(42)\n",
    "            .select(range(\n",
    "            min(len(dataset),\n",
    "                N)))\n",
    "    )\n",
    "\n",
    "dataset = load_dataset('Atsunori/HelpSteer2-DPO')\n",
    "dataset['train'] = sample(dataset['train'], num_samples)\n",
    "dataset['validation'] = sample(dataset['validation'], num_samples)\n",
    "dataset2 = dataset.rename_column('chosen_response', 'chosen').rename_column('rejected_response', 'rejected')\n",
    "dataset2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect HS in DPO way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/repr-preference-optimization/.venv/lib/python3.9/site-packages/trl/trainer/dpo_trainer.py:442: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = ReprPOConfig('./output-dir/scratch',\n",
    "    per_device_train_batch_size=3,\n",
    "    per_device_eval_batch_size=3,\n",
    "    gradient_checkpointing=True,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    max_prompt_length=128,\n",
    "    max_length=256,\n",
    "    collection_layers=[10, 20]\n",
    "                             )\n",
    "reprpo_trainer = ReprPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=None,\n",
    "    args=training_args,\n",
    "    beta=training_args.beta,\n",
    "    train_dataset=dataset2[\"train\"],\n",
    "    # eval_dataset=dataset2[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 256])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# QC get dpo catch\n",
    "dl = reprpo_trainer.get_train_dataloader()\n",
    "batch = next(iter(dl))\n",
    "batch['chosen_input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['concatenated_input_ids', 'concatenated_attention_mask', 'concatenated_labels'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 256]), torch.Size([12, 256]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# QC view a typical input to the model (since the dpo trainer transformes in dataset, concatenating chosen and rejecting along the batch dimension)\n",
    "batch_concat = reprpo_trainer.concatenated_inputs(\n",
    "            batch,\n",
    "            is_encoder_decoder=reprpo_trainer.is_encoder_decoder,\n",
    "            label_pad_token_id=reprpo_trainer.label_pad_token_id,\n",
    "            padding_value=reprpo_trainer.padding_value,\n",
    "            device=reprpo_trainer.accelerator.device,\n",
    "            max_length=reprpo_trainer.args.max_length,\n",
    "        )\n",
    "layer_idx = 0\n",
    "print(batch_concat.keys())\n",
    "batch['chosen_input_ids'].shape, batch_concat['concatenated_input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get batch of hidden states\n",
    "from reprpo.helpers.torch import clear_mem\n",
    "from trl.trainer.utils import pad_to_length\n",
    "from torch import Tensor\n",
    "from jaxtyping import Float\n",
    "\n",
    "def get_batch_token_logits(\n",
    "    logits: torch.FloatTensor,\n",
    "    labels: torch.LongTensor,\n",
    "    label_pad_token_id: int = -100,\n",
    "    is_encoder_decoder: bool = False,\n",
    ") -> Tuple[torch.FloatTensor, torch.LongTensor]:\n",
    "    \"\"\"Collect label logits (no softmax)\n",
    "\n",
    "    Args:\n",
    "        logits: Logits of the model (unnormalized). Shape: (batch_size, sequence_length, vocab_size)\n",
    "        labels: Labels for which to compute the log probabilities. Label tokens with a value of label_pad_token_id are ignored. Shape: (batch_size, sequence_length)\n",
    "        label_pad_token_id: The label pad token id.\n",
    "        is_encoder_decoder: Whether the model is an encoder-decoder model.\n",
    "\n",
    "    Returns:\n",
    "        A Tuple of two tensor of shape ((batch_size,), (batch_size,)) containing the logits of the given labels under the given logits in the first tensor and the number of non-masked tokens in the second tensor.\n",
    "    \"\"\"\n",
    "    if logits.shape[:-1] != labels.shape:\n",
    "        raise ValueError(\n",
    "            \"Logits (batch and sequence length dim) and labels must have the same shape.\"\n",
    "        )\n",
    "\n",
    "    if not is_encoder_decoder:\n",
    "        labels = labels[:, 1:].clone()\n",
    "        logits = logits[:, :-1, :]\n",
    "    loss_mask = labels != label_pad_token_id\n",
    "\n",
    "    # dummy token; we'll ignore the losses on these tokens later\n",
    "    labels[labels == label_pad_token_id] = 0\n",
    "\n",
    "    per_token_logits = torch.gather(\n",
    "        logits, dim=2, index=labels.unsqueeze(2)\n",
    "    ).squeeze(2)\n",
    "\n",
    "    # so this multiplies the probs and makes it quite small, in the log domain that's ok, it represents the log probs of the whole string\n",
    "    return (per_token_logits * loss_mask).sum(-1), loss_mask.sum(-1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_hs(trainer, model, batch):\n",
    "    model.eval()\n",
    "    (\n",
    "        chosen_logps,\n",
    "        rejected_logps,\n",
    "        chosen_logits,\n",
    "        rejected_logits,\n",
    "        _,\n",
    "        chosen_hs,\n",
    "        rejected_hs,\n",
    "        chosen_attn_mask,\n",
    "        rejected_attn_mask\n",
    "    ) = trainer.concatenated_forward(trainer.model, batch)\n",
    "\n",
    "    r = dict(chosen_hs=chosen_hs, rejected_hs=rejected_hs, chosen_logps=chosen_logps, rejected_logps=rejected_logps, chosen_attn_mask=chosen_attn_mask, rejected_attn_mask=rejected_attn_mask)\n",
    "\n",
    "    # get unembected hs\n",
    "    r['chosen_unemb'] = model.lm_head(chosen_hs)\n",
    "    r['rejected_unemb'] = model.lm_head(rejected_hs)\n",
    "\n",
    "\n",
    "    # FIXME: label and logits are diff length, maybe need the padding from concat_batch, might be eaiser to just use the concat batch framework for both\n",
    "    def get_layer_logps(trainer, hidden_states: Float[Tensor, 'b l t h'], labels: Float[Tensor, 'b t'], log_softmax=True):\n",
    "        # pad to length\n",
    "        hidden_states = pad_to_length(hidden_states, trainer.max_length, pad_value=0)\n",
    "        labels = pad_to_length(labels, trainer.max_length, pad_value=trainer.label_pad_token_id)\n",
    "\n",
    "        # gather for each layer\n",
    "        logps = []\n",
    "        for layer in range(hidden_states.shape[1]):\n",
    "            all_logps, size_completion = trainer.get_batch_logps(\n",
    "                hidden_states[:, layer],\n",
    "                labels,\n",
    "                label_pad_token_id=trainer.label_pad_token_id,\n",
    "                log_softmax=log_softmax\n",
    "                )\n",
    "            all_logps = all_logps / size_completion\n",
    "            logps.append(all_logps)\n",
    "        all_logps = torch.stack(logps, dim=1)\n",
    "        return all_logps\n",
    "    \n",
    "    # get fake logp from unemb_hs\n",
    "    r['chosen_gthr_logps_unemb'] = get_layer_logps(trainer, r['chosen_unemb'], batch[\"chosen_labels\"])\n",
    "    r['rejection_gthr_logps_unemb'] = get_layer_logps(trainer, r['rejected_unemb'], batch[\"rejected_labels\"])\n",
    "\n",
    "    r['chosen_gthr_unemb'] = get_layer_logps(trainer, r['chosen_unemb'], batch[\"chosen_labels\"], log_softmax=False)\n",
    "    r['rejection_gthr_unemb'] = get_layer_logps(trainer, r['rejected_unemb'], batch[\"rejected_labels\"], log_softmax=False)\n",
    "    \n",
    "    # we reuse the function, adding a fake layer dim, and squeezing it out\n",
    "    r['chosen_gthr_logits'] = get_layer_logps(trainer, chosen_logits[:, None], batch[\"chosen_labels\"], log_softmax=False).squeeze(1)\n",
    "    r['rejected_gthr_logits'] = get_layer_logps(trainer, rejected_logits[:, None], batch[\"rejected_labels\"], log_softmax=False).squeeze(1)\n",
    "\n",
    "\n",
    "    r = {k: v.detach().cpu() for k, v in r.items()}\n",
    "    clear_mem(trainer)\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b55f5c054b23431fb2114ea443fa5a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 2, 256, 4096])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = reprpo_trainer.get_train_dataloader()\n",
    "\n",
    "data = []\n",
    "for i, batch in enumerate(tqdm(dl)):\n",
    "    with reprpo_trainer.null_ref_context():\n",
    "        r = get_hs(reprpo_trainer, reprpo_trainer.model, batch)\n",
    "        data.append(r)\n",
    "        if i > 2:\n",
    "            break\n",
    "\n",
    "# concat\n",
    "data = {k: torch.cat([d[k] for d in data], dim=0) for k in data[0].keys()}\n",
    "data['chosen_hs'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "clear_mem(reprpo_trainer)\n",
    "reprpo_trainer = None\n",
    "clear_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chosen_hs',\n",
       " 'rejected_hs',\n",
       " 'chosen_logps',\n",
       " 'rejected_logps',\n",
       " 'chosen_attn_mask',\n",
       " 'rejected_attn_mask',\n",
       " 'chosen_unemb',\n",
       " 'rejected_unemb',\n",
       " 'chosen_gthr_logps_unemb',\n",
       " 'rejection_gthr_logps_unemb',\n",
       " 'chosen_gthr_unemb',\n",
       " 'rejection_gthr_unemb',\n",
       " 'chosen_gthr_logits',\n",
       " 'rejected_gthr_logits']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some data samples\n",
    "layer = 1\n",
    "C = data['chosen_hs'] # we could also consider unembedding them, whith the lm head, this might make them more interpretable, but also a bit large\n",
    "R = data['rejected_hs']\n",
    "\n",
    "CA = data['chosen_attn_mask']\n",
    "RA = data['rejected_attn_mask']\n",
    "\n",
    "M = 100\n",
    "A2 = CA * RA # use both attn masks when comparing?\n",
    "\n",
    "# choose layer, mean over tokens\n",
    "C = mult_with_attention(C, A2)[:M, layer]\n",
    "C = reduce(C, 'b t h -> b h', 'mean')\n",
    "R = mult_with_attention(R, A2)[:M, layer]\n",
    "R = reduce(R, 'b t h -> b h', 'mean')\n",
    "\n",
    "# compare two unrelated sets of samples, that way we have ones that should show the difference and ones that shouldn't show the difference we arel ooking for\n",
    "n = len(C)//2\n",
    "print('n', n)\n",
    "C1 = C[:n] # chosen pair 1\n",
    "R1 = R[:n] # rejected pair 1\n",
    "C2 = C[n:] # chosen, pair 2\n",
    "R2 = R[n:] # rejected pair 2\n",
    "\n",
    "\n",
    "# now we choose what to test correlations with. At first I tried the logprobs but they have been through a softmax which makes them relative and hard to compare to the hidden states\n",
    "# so instead we are going to try the hidden states values that corresponded to the \n",
    "\n",
    "# logratios = data['chosen_logps'] - data['rejected_logps'] # the logp is the log probability (mean per token) of this response, logratios is the log ratio of probs, this should be correlated with the direction we are seeking in the hidden states\n",
    "\n",
    "# logratios = (data['chosen_gthr_logits'] - data['rejected_gthr_logits'])#.exp()\n",
    "logratios = data['chosen_gthr_unemb'][:, layer] - data['rejection_gthr_unemb'][:, layer]\n",
    "\n",
    "# we can use this to check the null hypothesis, as the `roll` operation mixes up the batch dimension, so they are unrelated\n",
    "logratios_unrelated = logratios.roll(1, 0)\n",
    "\n",
    "logratios1 = logratios[:n]\n",
    "logratios2 = logratios[n:]\n",
    "logratios2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n",
    "\n",
    "### Magnitude/norm is correlated with result?\n",
    "\n",
    "Is the amplitude of A-B related to the logp?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "does the magnitude of the hs_unemb correlate with the prob ratio (across batches)? if so it's a good repr\n",
      "should be significantly bigger\n",
      "Hypothesis: corr>corr_null\n",
      "          : 0.082 > 0.083\n",
      "Residual  : = corr - corr_null\n",
      "Residual  : = 0.0 ❌\n",
      "\n",
      "should pass\n",
      "Hypothesis: corr>corr_null\n",
      "          : 0.589 > 0.309\n",
      "Residual  : = corr - corr_null\n",
      "Residual  : = 0.280 ✅\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ur using abs corrcoef\n",
    "def acorr(A, B):\n",
    "    return np.abs(np.corrcoef(A, B, rowvar=True)[0, 1])\n",
    "\n",
    "def mag_corr(C, R, logratios):\n",
    "    hs_d = torch.norm(C-R, dim=-1, p=2) # get magnitude of the difference\n",
    "    corr = acorr(hs_d, logratios)\n",
    "    # note that after flipping we ruin the order, and they should be uncorrelated except for the middle one if it's odd\n",
    "    corr_null = acorr(hs_d, logratios.roll(1, 0))\n",
    "    return corr, corr_null\n",
    "    \n",
    "\n",
    "print('does the magnitude of the hs_unemb correlate with the prob ratio (across batches)? if so it\\'s a good repr')\n",
    "\n",
    "print(\"should be significantly bigger\")\n",
    "corr, corr_null = mag_corr(C1, R1, logratios1)\n",
    "shypothesis(\"corr>corr_null\", variables=dict(corr=corr, corr_null=corr_null))\n",
    "\n",
    "print(\"should pass\")\n",
    "corr, corr_null = mag_corr(C2, R2, logratios2)\n",
    "shypothesis(\"corr>corr_null\", variables=dict(corr=corr, corr_null=corr_null))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is it significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is the red line outside the distribution of null correlations?\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-value: 0.923\n"
     ]
    }
   ],
   "source": [
    "def permutation_test(C, R, logratios, n_permutations=100):\n",
    "\n",
    "    n_permutations = min(n_permutations, len(logratios))\n",
    "\n",
    "    true_corr = np.corrcoef(\n",
    "        torch.norm(C - R, dim=-1, p=2), \n",
    "        logratios)[0, 1]\n",
    "    \n",
    "    null_corrs = []\n",
    "    for _ in range(n_permutations):\n",
    "        perm_logratios = logratios[torch.randperm(len(logratios))]\n",
    "        null_corrs.append(np.corrcoef(torch.norm(C - R, dim=-1, p=2), perm_logratios)[0, 1])\n",
    "    \n",
    "    plt.hist(null_corrs, bins=50, label='null_corrs')\n",
    "    plt.axvline(true_corr, color='r', linestyle='dashed', linewidth=2, label='true_corr')\n",
    "    plt.title('Permutation Test')\n",
    "    plt.xlabel('Correlation')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    p_value = (np.sum(np.abs(null_corrs) >= np.abs(true_corr)) + 1) / (n_permutations + 1)\n",
    "    print(f\"P-value: {p_value:.3f}\")\n",
    "\n",
    "print('is the red line outside the distribution of null correlations?')\n",
    "permutation_test(C1, R1, logratios1)\n",
    "# FAIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spearmanr kendalltau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spearmanr measure of dist ~ logratios should be large and significant\n",
      "Hypothesis: corr>n_corr\n",
      "          : 0.105 > 0.077\n",
      "Residual  : = corr - n_corr\n",
      "Residual  : = 0.028 ✅\n",
      "\n",
      "Hypothesis: p<0.2\n",
      "          : 0.746 < 0.200\n",
      "Residual  : = p - 0.2\n",
      "Residual  : = 0.546 ❌\n",
      "\n",
      "kendalltau measure of dist ~ logratios should be large and significant\n",
      "Hypothesis: corr>n_corr\n",
      "          : 0.061 > 0.091\n",
      "Residual  : = corr - n_corr\n",
      "Residual  : = -0.030 ❌\n",
      "\n",
      "Hypothesis: p<0.2\n",
      "          : 0.841 < 0.200\n",
      "Residual  : = p - 0.2\n",
      "Residual  : = 0.641 ❌\n",
      "\n",
      "Hypothesis: n_p<0.2\n",
      "          : 0.737 < 0.200\n",
      "Residual  : = n_p - 0.2\n",
      "Residual  : = 0.537 ❌\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "\n",
    "def correlation_dist(C, R, logratios):\n",
    "    distances = torch.norm(C - R, dim=-1)\n",
    "    logratios = logratios.cpu().numpy()\n",
    "    corr, p = stats.spearmanr(distances, logratios)\n",
    "    return np.abs(corr), p\n",
    "\n",
    "print('spearmanr measure of dist ~ logratios should be large and significant')\n",
    "corr, p = correlation_dist(C1, R1, logratios1)\n",
    "n_corr, n_p = correlation_dist(C1, R1, logratios1.roll(1, 0))\n",
    "shypothesis('corr>n_corr', variables=locals())\n",
    "shypothesis('p<0.2', variables=locals())\n",
    "\n",
    "def kendalltau_dist(C, R, logratios):\n",
    "    distances = torch.norm(C - R, dim=-1)\n",
    "    logratios = logratios.cpu().numpy()\n",
    "    corr, p = stats.kendalltau(distances, logratios)\n",
    "    return np.abs(corr), p\n",
    "\n",
    "print('kendalltau measure of dist ~ logratios should be large and significant')\n",
    "corr, p = kendalltau_dist(C1, R1, logratios1)\n",
    "n_corr, n_p = kendalltau_dist(C1, R1, logratios1.roll(1, 0))\n",
    "shypothesis('corr>n_corr', variables=locals())\n",
    "shypothesis('p<0.2', variables=locals())\n",
    "shypothesis('n_p<0.2', variables=locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing angle_similarity_correlation:\n",
      "Similarity of related ones should be higher\n",
      "Hypothesis: c111 > c112\n",
      "          : c111 > c112\n",
      "     Where: {'c111': 0.17653066864825379, 'c112': 0.06311316313647669}\n",
      "          : 0.177 > 0.063\n",
      "Residual  : = c111 - c112\n",
      "Residual  : = 0.113 ✅\n",
      "\n",
      "Hypothesis: c222 > c221\n",
      "          : 0.642 > 0.290\n",
      "Residual  : = -c221 + c222\n",
      "Residual  : = 0.351 ✅\n",
      "\n",
      "\n",
      "Testing norm_difference_correlation:\n",
      "Similarity of related ones should be higher\n",
      "Hypothesis: c111 > c112\n",
      "          : c111 > c112\n",
      "     Where: {'c111': 0.18184877604691557, 'c112': 0.13933448014055647}\n",
      "          : 0.182 > 0.139\n",
      "Residual  : = c111 - c112\n",
      "Residual  : = 0.043 ✅\n",
      "\n",
      "Hypothesis: c222 > c221\n",
      "          : 0.340 > 0.268\n",
      "Residual  : = -c221 + c222\n",
      "Residual  : = 0.072 ✅\n",
      "\n",
      "\n",
      "Testing magnitude_correlation:\n",
      "Similarity of related ones should be higher\n",
      "Hypothesis: c111 > c112\n",
      "          : c111 > c112\n",
      "     Where: {'c111': 0.09435227364063753, 'c112': 0.09191347777559443}\n",
      "          : 0.094 > 0.092\n",
      "Residual  : = c111 - c112\n",
      "Residual  : = 0.002 ✅\n",
      "\n",
      "Hypothesis: c222 > c221\n",
      "          : 0.600 > 0.306\n",
      "Residual  : = -c221 + c222\n",
      "Residual  : = 0.293 ✅\n",
      "\n",
      "\n",
      "Testing inner_product_correlation:\n",
      "Similarity of related ones should be higher\n",
      "Hypothesis: c111 > c112\n",
      "          : c111 > c112\n",
      "     Where: {'c111': 0.08523901658078226, 'c112': 0.2938953717486296}\n",
      "          : 0.085 > 0.294\n",
      "Residual  : = c111 - c112\n",
      "Residual  : = -0.209 ❌\n",
      "\n",
      "Hypothesis: c222 > c221\n",
      "          : 0.046 > 0.141\n",
      "Residual  : = -c221 + c222\n",
      "Residual  : = -0.095 ❌\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def angle_similarity_correlation(C, R, logratios):\n",
    "    C_norm = F.normalize(C, p=2, dim=-1)\n",
    "    R_norm = F.normalize(R, p=2, dim=-1)\n",
    "    angle = torch.acos(torch.clamp(torch.sum(C_norm * R_norm, dim=-1), -1.0, 1.0))\n",
    "    corrs = np.corrcoef(angle.cpu().numpy(), logratios.cpu().numpy())[0, 1]\n",
    "    return np.abs(corrs)\n",
    "\n",
    "def norm_difference_correlation(C, R, logratios):\n",
    "    norm_diff = torch.norm(C, dim=-1) - torch.norm(R, dim=-1)\n",
    "    corrs = np.corrcoef(norm_diff.cpu().numpy(), logratios.cpu().numpy())[0, 1]\n",
    "    return np.abs(corrs)\n",
    "\n",
    "def magnitude_correlation(C, R, logratios):\n",
    "    magnitude_diff = torch.abs(C - R).mean(dim=-1)\n",
    "    corrs = np.corrcoef(magnitude_diff.cpu().numpy(), logratios.cpu().numpy())[0, 1]\n",
    "    return np.abs(corrs)\n",
    "\n",
    "def inner_product_correlation(C, R, logratios):\n",
    "    inner_prod = torch.sum(C * R, dim=-1)\n",
    "    corrs = np.corrcoef(inner_prod.cpu().numpy(), logratios.cpu().numpy())[0, 1]\n",
    "    return np.abs(corrs)\n",
    "\n",
    "# Usage\n",
    "def run_all_tests(C1, R1, C2, R2, logratios1, logratios2):\n",
    "    tests = [\n",
    "        angle_similarity_correlation,\n",
    "        norm_difference_correlation,\n",
    "        magnitude_correlation,\n",
    "        inner_product_correlation\n",
    "    ]\n",
    "    \n",
    "    for test in tests:\n",
    "        print(f\"\\nTesting {test.__name__}:\")\n",
    "        c111 = test(C1, R1, logratios1)\n",
    "        c222 = test(C2, R2, logratios2)\n",
    "        c112 = test(C1, R1, logratios1.roll(1, 0))\n",
    "        c221 = test(C2, R2, logratios2.roll(1, 0))\n",
    "        \n",
    "        print('Similarity of related ones should be higher')\n",
    "        shypothesis('c111 > c112', variables=locals(), verbose=True)\n",
    "        shypothesis('c222 > c221', variables=locals())\n",
    "        # Uncomment these if you want to test across different pairs\n",
    "        # shypothesis('c111 > c221', variables=locals())\n",
    "        # shypothesis('c222 > c112', variables=locals())\n",
    "\n",
    "# Run all tests\n",
    "run_all_tests(C1, R1, C2, R2, logratios1, logratios2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H: cosine_similarity_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity of related ones should be higher\n",
      "Hypothesis: c111 > c112\n",
      "          : c111 > c112\n",
      "     Where: {'c111': 0.19708932282301475, 'c112': 0.052489255910791635}\n",
      "          : 0.197 > 0.052\n",
      "Residual  : = c111 - c112\n",
      "Residual  : = 0.145 ✅\n",
      "\n",
      "Hypothesis: c222 > c221\n",
      "          : 0.575 > 0.367\n",
      "Residual  : = -c221 + c222\n",
      "Residual  : = 0.207 ✅\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity_correlation(C, R, logratios):\n",
    "    cos_sim = F.cosine_similarity(C, R, dim=-1)  # [batch, layers, tokens]\n",
    "\n",
    "    corrs = np.corrcoef(cos_sim, logratios)[0, 1] \n",
    "    \n",
    "    return np.abs(corrs)\n",
    "\n",
    "c111 = cosine_similarity_correlation(C1, R1, logratios1)\n",
    "c222 = cosine_similarity_correlation(C2, R2, logratios2)\n",
    "c112 = cosine_similarity_correlation(C1, R1, logratios1.roll(1, 0))\n",
    "c221 = cosine_similarity_correlation(C2, R2, logratios2.roll(1, 0))\n",
    "\n",
    "print('cosine similarity of related ones should be higher')\n",
    "shypothesis('c111 > c112', variables=locals(), verbose=True)\n",
    "shypothesis('c222 > c221', variables=locals())\n",
    "# shypothesis('c111 > c221', variables=locals())\n",
    "# shypothesis('c222 > c112', variables=locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C1-R1 with logratios1:\n",
      "Hypothesis: kl1>kl2\n",
      "          : 0.004 > 0.115\n",
      "Residual  : = kl1 - kl2\n",
      "Residual  : = -0.111 ❌\n",
      "\n",
      "Hypothesis: js1>js2\n",
      "          : 0.006 > 0.115\n",
      "Residual  : = js1 - js2\n",
      "Residual  : = -0.109 ❌\n",
      "\n",
      "Hypothesis: mi1>mi2\n",
      "          : 0.449 > 0\n",
      "Residual  : = mi1 - mi2\n",
      "Residual  : = 0.449 ✅\n",
      "\n",
      "C2-R2 with logratios2:\n",
      "Hypothesis: kl1>kl2\n",
      "          : 0.134 > 0.410\n",
      "Residual  : = kl1 - kl2\n",
      "Residual  : = -0.276 ❌\n",
      "\n",
      "Hypothesis: js1>js2\n",
      "          : 0.103 > 0.409\n",
      "Residual  : = js1 - js2\n",
      "Residual  : = -0.307 ❌\n",
      "\n",
      "Hypothesis: mi1>mi2\n",
      "          : 0.164 > 0\n",
      "Residual  : = mi1 - mi2\n",
      "Residual  : = 0.164 ✅\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def kl_divergence(p, q):\n",
    "    return (p * (p / q).log()).sum(dim=-1)\n",
    "\n",
    "def js_divergence(p, q):\n",
    "    m = 0.5 * (p + q)\n",
    "    return 0.5 * kl_divergence(p, m) + 0.5 * kl_divergence(q, m)\n",
    "\n",
    "def mutual_information(C, R):\n",
    "    joint = torch.einsum('bt,bt->bt', C, R)\n",
    "    joint = joint / joint.sum(dim=-1, keepdim=True)\n",
    "    mi = kl_divergence(joint, C * R)\n",
    "    return mi\n",
    "\n",
    "def info_theoretic_correlation(C, R, logratios):\n",
    "    C_norm = F.softmax(C, dim=-1)\n",
    "    R_norm = F.softmax(R, dim=-1)\n",
    "    \n",
    "    kl = kl_divergence(C_norm, R_norm)#.mean(dim=(1,))\n",
    "    js = js_divergence(C_norm, R_norm)#.mean(dim=(1,))\n",
    "    mi = mutual_information(C_norm, R_norm)#.mean(dim=(1,))\n",
    "    \n",
    "    kl_corr = np.abs(np.corrcoef(kl.cpu().numpy(), logratios)[0,1])\n",
    "    js_corr = np.abs(np.corrcoef(js.cpu().numpy(), logratios)[0,1])\n",
    "    mi_corr = np.abs(np.corrcoef(mi.cpu().numpy(), logratios)[0,1])\n",
    "    \n",
    "    return kl_corr, js_corr, mi_corr\n",
    "\n",
    "print(\"C1-R1 with logratios1:\")\n",
    "kl1,js1,mi1 = info_theoretic_correlation(C1, R1, logratios1)\n",
    "kl2,js2,mi1 = info_theoretic_correlation(C1, R1, logratios1.roll(1, 0))\n",
    "shypothesis('kl1>kl2', variables=locals())\n",
    "shypothesis('js1>js2', variables=locals())\n",
    "shypothesis('mi1>mi2', variables=locals())\n",
    "\n",
    "print(\"C2-R2 with logratios2:\")\n",
    "kl1,js1,mi1 = info_theoretic_correlation(C2, R2, logratios2)\n",
    "kl2,js2,mi1 = info_theoretic_correlation(C2, R2, logratios2.roll(1, 0))\n",
    "shypothesis('kl1>kl2', variables=locals())\n",
    "shypothesis('js1>js2', variables=locals())\n",
    "shypothesis('mi1>mi2', variables=locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original difference: 5.040531635284424\n",
      "Rotated difference: 29.78263282775879\n",
      "Improvement: -490.86%\n",
      "Correlation before rotation: 0.08\n",
      "Correlation after rotation: 0.06\n",
      "Improvement: -21.15%\n",
      "\n",
      "Hypothesis: r > c\n",
      "          : 0.065 > 0.082\n",
      "Residual  : = -c + r\n",
      "Residual  : = -0.017 ❌\n",
      "\n",
      "Original difference: 5.16802978515625\n",
      "Rotated difference: 27.767431259155273\n",
      "Improvement: -437.29%\n",
      "Correlation before rotation: 0.59\n",
      "Correlation after rotation: 0.02\n",
      "Improvement: -96.05%\n",
      "\n",
      "Hypothesis: r > c\n",
      "          : 0.023 > 0.589\n",
      "Residual  : = -c + r\n",
      "Residual  : = -0.566 ❌\n",
      "\n",
      "Original difference: 5.16802978515625\n",
      "Rotated difference: 27.767431259155273\n",
      "Improvement: -437.29%\n",
      "Correlation before rotation: 0.31\n",
      "Correlation after rotation: 0.13\n",
      "Improvement: -56.57%\n",
      "\n",
      "Hypothesis: r > c\n",
      "          : 0.134 > 0.309\n",
      "Residual  : = -c + r\n",
      "Residual  : = -0.175 ❌\n",
      "\n",
      "Original difference: 5.040531635284424\n",
      "Rotated difference: 29.78263282775879\n",
      "Improvement: -490.86%\n",
      "Correlation before rotation: 0.08\n",
      "Correlation after rotation: 0.22\n",
      "Improvement: 161.17%\n",
      "\n",
      "Hypothesis: r > c\n",
      "          : 0.216 > 0.083\n",
      "Residual  : = -c + r\n",
      "Residual  : = 0.133 ✅\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def rotation_test(C, R, logratios):\n",
    "    def optimal_rotation(A, B):\n",
    "\n",
    "        corr_matrix = torch.mm(A.t(), B)\n",
    "        Q, _ = torch.linalg.qr(corr_matrix)\n",
    "        return Q\n",
    "    \n",
    "    def optimal_rotation(A, B):\n",
    "        corr_matrix = torch.mm(A.t(), B)\n",
    "        U, _, V = torch.linalg.svd(corr_matrix)\n",
    "        return torch.mm(U, V.t())\n",
    "    \n",
    "    R_opt = optimal_rotation(C, R)\n",
    "    R_rotated = torch.matmul(R, R_opt.t())\n",
    "    \n",
    "    a = C - R\n",
    "    b = C - R_rotated\n",
    "\n",
    "    # a -= a.mean(dim=-1, keepdim=True)\n",
    "    # b -= b.mean(dim=-1, keepdim=True)\n",
    "    # a /= torch.norm(a, dim=0, p=2, keepdim=True)\n",
    "    # b /= torch.norm(b, dim=0, p=2, keepdim=True)\n",
    "\n",
    "    original_diff = torch.norm(a, p=2)\n",
    "    rotated_diff = torch.norm(b, p=2)\n",
    "    \n",
    "    print(f\"Original difference: {original_diff}\")\n",
    "    print(f\"Rotated difference: {rotated_diff}\")\n",
    "    print(f\"Improvement: {(original_diff - rotated_diff) / original_diff * 100:.2f}%\")\n",
    "    \n",
    "    c = np.abs(np.corrcoef(torch.norm(a, dim=-1, p=2), logratios)[0, 1])\n",
    "    r = np.abs(np.corrcoef(torch.norm(b, dim=-1, p=2), logratios)[0, 1])\n",
    "    print(f\"Correlation before rotation: {c:.2f}\")\n",
    "    print(f\"Correlation after rotation: {r:.2f}\")\n",
    "    print(f\"Improvement: {(r - c) / c * 100:.2f}%\")\n",
    "    print()\n",
    "    shypothesis('r > c', variables=dict(r=r, c=c))\n",
    "    return R_opt, R_rotated\n",
    "\n",
    "# maybe I should softmax first?\n",
    "R_opt, R_rotated = rotation_test(C1, R1, logratios1)\n",
    "R_opt, R_rotated = rotation_test(C2, R2, logratios2)\n",
    "\n",
    "R_opt, R_rotated = rotation_test(C2, R2, logratios2.roll(1, 0))\n",
    "R_opt, R_rotated = rotation_test(C1, R1, logratios1.roll(1, 0))\n",
    "# R_opt, R_rotated = rotation_test(C2, R2, logratios1)\n",
    "# R_opt, R_rotated = rotation_test(C1, R1, logratios2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA):\n",
    "Apply PCA to (A - B) pairs. If differences are primarily rotational, most variance should be explained by a few components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothesis: e_a1b1 > e_a1b1roll\n",
      "          : 0.163 > 0.170\n",
      "Residual  : = e_a1b1 - e_a1b1roll\n",
      "Residual  : = -0.007 ❌\n",
      "\n",
      "Hypothesis: e_a1b1 > e_a1b1roll\n",
      "          : 0.150 > 0.157\n",
      "Residual  : = e_a1b1 - e_a1b1roll\n",
      "Residual  : = -0.008 ❌\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def pca_exp_var(X, Y):\n",
    "    diffs = (X - Y).numpy()\n",
    "    s = diffs.std()\n",
    "    diffs /= s\n",
    "    # axis 1, norm each neuron across the batch\n",
    "    # diffs /= diffs.std(1, keepdims=True) + 1e-3\n",
    "    pca = PCA(n_components=5)\n",
    "    pca.fit(diffs)\n",
    "    r = pca.explained_variance_ratio_.mean()\n",
    "    return r\n",
    "\n",
    "\n",
    "# \n",
    "e_a1b1 = pca_exp_var(C1, R1)\n",
    "e_a1b1roll = pca_exp_var(C1, R1.roll(1, 0))\n",
    "shypothesis('e_a1b1 > e_a1b1roll', variables=locals())\n",
    "\n",
    "e_a1b1 = pca_exp_var(C2, R2)\n",
    "e_a1b1roll = pca_exp_var(C2, R2.roll(1, 0))\n",
    "shypothesis('e_a1b1 > e_a1b1roll', variables=locals())\n",
    "\n",
    "# e_a1a2 = pca_exp_var(C1, C2)\n",
    "# e_a1b2 = pca_exp_var(C1, R2)\n",
    "# e_b1b2 = pca_exp_var(R1, R2)\n",
    "# e_b1a1 = pca_exp_var(R1, C1)\n",
    "# e_b1a2 = pca_exp_var(R1, C2)\n",
    "\n",
    "# print('PCA explained variance ratio')\n",
    "# print('A1-B1', e_a1b2)\n",
    "# shypothesis('e_a1b1 > e_a1a2', variables=locals())\n",
    "# shypothesis('e_a1b1 > e_a1b2', variables=locals())\n",
    "# shypothesis('e_a1b1 > e_b1b2', variables=locals()) # should be inconclusive\n",
    "# e_a1b2, e_a1a2, e_a1b2, e_b1b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mutual_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do the paired hs have information shared with the logratios?\n",
      "Mutual Information: 0.09186808561808668\n",
      "Mutual Information: 0\n",
      "Hypothesis: mi1 > mi2\n",
      "          : 0.092 > 0\n",
      "Residual  : = mi1 - mi2\n",
      "Residual  : = 0.092 ✅\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "def mutual_information(C, R, logratios):\n",
    "    distances = torch.norm(C - R, dim=-1, p=2, keepdim=True)\n",
    "    mi = mutual_info_regression(distances, logratios)[0]\n",
    "    print(f\"Mutual Information: {mi}\")\n",
    "    return mi\n",
    "\n",
    "print('do the paired hs have information shared with the logratios?')\n",
    "mi1 = mutual_information(C1, R1, logratios1)\n",
    "mi2 = mutual_information(C1, R1, logratios1.roll(1, 0));\n",
    "shypothesis('mi1 > mi2', variables=locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frobenius Norm Comparison:\n",
    "Compare ||A - B||_F with ||A - RB||_F where R is the optimal rotation. The latter should be significantly smaller if rotations capture most differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passing means the paired samples can be rotated\n",
      "Hypothesis: rot_diff_norm < diff_norm\n",
      "          : 4.886 < 5.041\n",
      "Residual  : = -diff_norm + rot_diff_norm\n",
      "Residual  : = -0.154 ✅\n",
      "\n",
      "passing means the paired samples can be rotated\n",
      "Hypothesis: rot_diff_norm < diff_norm\n",
      "          : 5.017 < 5.168\n",
      "Residual  : = -diff_norm + rot_diff_norm\n",
      "Residual  : = -0.151 ✅\n",
      "\n",
      "passing means the unrelated samples can be rotated\n",
      "Hypothesis: rot_diff_norm > diff_norm\n",
      "          : 12.650 > 14.921\n",
      "Residual  : = -diff_norm + rot_diff_norm\n",
      "Residual  : = -2.271 ❌\n",
      "\n",
      "passing means the unrelated samples can be rotated\n",
      "Hypothesis: rot_diff_norm > diff_norm\n",
      "          : 12.882 > 15.115\n",
      "Residual  : = -diff_norm + rot_diff_norm\n",
      "Residual  : = -2.233 ❌\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def norm_comparison(A, B):\n",
    "    diff_norm = torch.norm(A - B)\n",
    "    corr = torch.mm(A, B.t())\n",
    "    U, _, V = torch.svd(corr)\n",
    "    R = torch.mm(U, V.t())\n",
    "\n",
    "    B_rot = torch.mm(R, B)\n",
    "\n",
    "    rot_diff_norm = torch.norm(A - B_rot)\n",
    "    return diff_norm, rot_diff_norm\n",
    "\n",
    "print('passing means the paired samples can be rotated')\n",
    "diff_norm, rot_diff_norm= norm_comparison(C1, R1)\n",
    "shypothesis('rot_diff_norm < diff_norm', variables=locals())\n",
    "\n",
    "print('passing means the paired samples can be rotated')\n",
    "diff_norm, rot_diff_norm= norm_comparison(C2, R2)\n",
    "# print(diff_norm, rot_diff_norm)\n",
    "shypothesis('rot_diff_norm < diff_norm', variables=locals())\n",
    "\n",
    "print('passing means the unrelated samples can be rotated')\n",
    "diff_norm, rot_diff_norm= norm_comparison(C1, R2)\n",
    "shypothesis('rot_diff_norm > diff_norm', variables=locals())\n",
    "\n",
    "print('passing means the unrelated samples can be rotated')\n",
    "diff_norm, rot_diff_norm= norm_comparison(C2, R1)\n",
    "# print(diff_norm, rot_diff_norm)\n",
    "shypothesis('rot_diff_norm > diff_norm', variables=locals())\n",
    "\n",
    "# UNEEXPECTED result: C and R are already in a similar direction! While unrelated ones are not\n",
    "# this implies that the preference concept that we want is not a factor of directions of the hidden states, but rather the magnitude of the difference between the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are the magnitudes related?\n",
      "Correlation of norm differences with logratios: 0.06\n",
      "Correlation of norm differences with logratios: 0.06\n",
      "Hypothesis: corr1 > corr2\n",
      "          : 0.182 > 0.097\n",
      "Residual  : = corr1 - corr2\n",
      "Residual  : = 0.085 ✅\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('are the magnitudes related?')\n",
    "c_norms = torch.norm(C1, dim=-1)\n",
    "r_norms = torch.norm(R1, dim=-1)\n",
    "norm_diffs = c_norms - r_norms\n",
    "corr1 = np.abs(np.corrcoef(norm_diffs.flatten(), logratios1)[0, 1])\n",
    "print(f\"Correlation of norm differences with logratios: {corr:.2f}\")\n",
    "\n",
    "\n",
    "\n",
    "c_norms = torch.norm(C1, dim=-1)\n",
    "r_norms = torch.norm(R1, dim=-1)\n",
    "norm_diffs = c_norms - r_norms\n",
    "corr2 = np.abs(np.corrcoef(norm_diffs.flatten(), logratios2)[0, 1])\n",
    "print(f\"Correlation of norm differences with logratios: {corr:.2f}\")\n",
    "\n",
    "shypothesis('corr1 > corr2', variables=locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(14.9215), tensor(12.6504))"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_norm, rot_diff_norm= norm_comparison(C1, R2)\n",
    "diff_norm, rot_diff_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Angle Histogram:\n",
    "Plot histogram of angles between corresponding columns of A and B. Should be concentrated if differences are mainly rotational."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def angle_histogram(A, B):\n",
    "    cos_sims = torch.sum(A * B, dim=0) / (torch.norm(A, dim=0) * torch.norm(B, dim=0))\n",
    "    angles = torch.acos(cos_sims) * 180 / np.pi\n",
    "    plt.hist(angles.numpy())\n",
    "    plt.show()\n",
    "\n",
    "angle_histogram(C1, R1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subspace scaling ratios: [1.02573366 0.95217523 0.96733044 0.92265209 0.89132623 0.95582475\n",
      " 0.9571119  0.95361726 0.99413921 0.95186764]\n",
      "Subspace scaling ratios: [1.73066033 1.74128314 1.01378417 0.58045075 3.24369333 1.12954028\n",
      " 0.97207083 1.32819393 1.60161157 1.01987266]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0606060606060606"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def subspace_scaling(C, R, n_subspaces=10):\n",
    "    pca = PCA(n_components=n_subspaces)\n",
    "    pca.fit(np.vstack((C.reshape(-1, C.shape[-1]), R.reshape(-1, R.shape[-1]))))\n",
    "    c_proj = pca.transform(C.reshape(-1, C.shape[-1]))\n",
    "    r_proj = pca.transform(R.reshape(-1, R.shape[-1]))\n",
    "    scale_ratios = np.linalg.norm(c_proj, axis=0) / np.linalg.norm(r_proj, axis=0)\n",
    "    return scale_ratios\n",
    "\n",
    "scales = subspace_scaling(C1, R1)\n",
    "print(\"Subspace scaling ratios:\", scales)\n",
    "\n",
    "scales = subspace_scaling(C1, R2)\n",
    "print(\"Subspace scaling ratios:\", scales)\n",
    "\n",
    "# ??\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation of sparsity differences with logratios: -0.004424094892843868\n",
      "Correlation of sparsity differences with logratios: 0.14424148271708376\n",
      "unrelated, should be null results\n",
      "Correlation of sparsity differences with logratios: -0.2868185883775725\n",
      "Correlation of sparsity differences with logratios: 0.06892329032083197\n",
      "Hypothesis: c111 > c112\n",
      "          : 0.004 > 0.287\n",
      "Residual  : = c111 - c112\n",
      "Residual  : = -0.282 ❌\n",
      "\n",
      "Hypothesis: c222 > c221\n",
      "          : 0.144 > 0.069\n",
      "Residual  : = -c221 + c222\n",
      "Residual  : = 0.075 ✅\n",
      "\n",
      "Hypothesis: c111 > c112\n",
      "          : 0.004 > 0.287\n",
      "Residual  : = c111 - c112\n",
      "Residual  : = -0.282 ❌\n",
      "\n",
      "Hypothesis: c222 > c221\n",
      "          : 0.144 > 0.069\n",
      "Residual  : = -c221 + c222\n",
      "Residual  : = 0.075 ✅\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sparsity_correlation(C, R, logratios):\n",
    "    c_sparsity = (np.abs(C) > 1e-2).float().mean(dim=-1)\n",
    "    r_sparsity = (np.abs(R) > 1e-2).float().mean(dim=-1)\n",
    "    sparsity_diff = c_sparsity - r_sparsity\n",
    "    corr = np.corrcoef(sparsity_diff.flatten(), logratios)[0, 1]\n",
    "    print(f\"Correlation of sparsity differences with logratios: {corr}\")\n",
    "    return np.abs(corr)\n",
    "\n",
    "c111 = sparsity_correlation(C1, R1, logratios1)\n",
    "c222 = sparsity_correlation(C2, R2, logratios2)\n",
    "\n",
    "print('unrelated, should be null results')\n",
    "c112 = sparsity_correlation(C1, R1, logratios2)\n",
    "c221 = sparsity_correlation(C2, R2, logratios1)\n",
    "\n",
    "shypothesis('c111 > c112', variables=locals())\n",
    "shypothesis('c222 > c221', variables=locals())\n",
    "shypothesis('c111 > c112', variables=locals())\n",
    "shypothesis('c222 > c221', variables=locals())\n",
    "\n",
    "# !!! SUCCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothesis: c1 > c1n\n",
      "          : c1 > c1n\n",
      "     Where: {'c1': 0.08233526961650646, 'c1n': 0.08276932002069595}\n",
      "          : 0.082 > 0.083\n",
      "Residual  : = c1 - c1n\n",
      "Residual  : = 0.0 ❌\n",
      "\n",
      "Hypothesis: c2 > c2n\n",
      "          : 0.589 > 0.309\n",
      "Residual  : = c2 - c2n\n",
      "Residual  : = 0.280 ✅\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.fft import fft\n",
    "\n",
    "def fft_analysis(C, R):\n",
    "    C_fft = fft(C.cpu().numpy(), axis=-1)\n",
    "    R_fft = fft(R.cpu().numpy(), axis=-1)\n",
    "    return C_fft - R_fft\n",
    "\n",
    "def fft_corr(C, R, logratios):\n",
    "    fft_diff = fft_analysis(C, R)\n",
    "    fft_norm = np.linalg.norm(fft_diff, axis=-1)\n",
    "    fft_corr = np.abs(np.corrcoef(fft_norm, logratios)[0,1])\n",
    "    return fft_corr\n",
    "\n",
    "c1 = fft_corr(C1, R1, logratios1)\n",
    "c1n = fft_corr(C1, R1, logratios1.roll(1, 0))\n",
    "shypothesis('c1 > c1n', variables=locals(), verbose=True)\n",
    "\n",
    "c2 = fft_corr(C2, R2, logratios2)\n",
    "c2n = fft_corr(C2, R2, logratios2.roll(1, 0))\n",
    "shypothesis('c2 > c2n', variables=locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are the right pairs more correlate if we take the diff of the symlog?\n",
      "Hypothesis: abs(nl_corr) > abs(corr_null)\n",
      "          : Abs(nl_corr) > Abs(corr_null)\n",
      "     Where: {'corr_null': -0.10750608919582604, 'nl_corr': -0.05941568343201138}\n",
      "          : 0.059 > 0.108\n",
      "Residual  : = -Abs(corr_null) + Abs(nl_corr)\n",
      "Residual  : = -0.048 ❌\n",
      "\n",
      "Hypothesis: abs(nl_corr) > abs(corr_null)\n",
      "          : Abs(nl_corr) > Abs(corr_null)\n",
      "     Where: {'corr_null': 0.1945450034581702, 'nl_corr': 0.6174599443322354}\n",
      "          : 0.617 > 0.195\n",
      "Residual  : = -Abs(corr_null) + Abs(nl_corr)\n",
      "Residual  : = 0.423 ✅\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def non_lin_corr(C, R, logratios):\n",
    "    C = symlog(C)\n",
    "    R = symlog(R)\n",
    "    nl_diff = C - R\n",
    "    nl_norm = torch.norm(nl_diff, dim=-1)\n",
    "    nl_corr = np.corrcoef(nl_norm, logratios)[0,1]\n",
    "    corr_null = np.corrcoef(nl_norm, logratios.roll(1,0))[0,1]\n",
    "    shypothesis('abs(nl_corr) > abs(corr_null)', variables=locals(), verbose=True)\n",
    "\n",
    "print('are the right pairs more correlate if we take the diff of the symlog?')\n",
    "non_lin_corr(C1, R1, logratios1)\n",
    "non_lin_corr(C2, R2, logratios2)\n",
    "# but it's the same result without symlog..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothesis: rel_corr > rel_corr2\n",
      "          : 0.015 > 0.275\n",
      "Residual  : = rel_corr - rel_corr2\n",
      "Residual  : = -0.261 ❌\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def relative_positioning(C, R, ref_point):\n",
    "    C_rel = C - ref_point\n",
    "    R_rel = R - ref_point\n",
    "    return torch.norm(C_rel, dim=-1) - torch.norm(R_rel, dim=-1)\n",
    "\n",
    "# Using mean of C2 as reference point\n",
    "ref_point = C2.mean(dim=(0), keepdim=True)\n",
    "rel_diff = relative_positioning(C1, R1, ref_point)\n",
    "rel_corr = np.abs(np.corrcoef(rel_diff.flatten(), logratios1)[0,1])\n",
    "rel_corr2 = np.abs(np.corrcoef(rel_diff.flatten(), logratios1.roll(1, 0))[0,1])\n",
    "shypothesis('rel_corr > rel_corr2', variables=locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothesis: c1r1_sim_corr > c1r1null_sim_corr\n",
      "          : 0.197 > -0.052\n",
      "Residual  : = c1r1_sim_corr - c1r1null_sim_corr\n",
      "Residual  : = 0.250 ✅\n",
      "\n",
      "Hypothesis: c2r2_sim_corr > c2r2null_sim_corr\n",
      "          : -0.575 > -0.367\n",
      "Residual  : = c2r2_sim_corr - c2r2null_sim_corr\n",
      "Residual  : = -0.207 ❌\n",
      "\n",
      "C1-R1 similarity correlation with logratio: 0.19708932282301475\n",
      "C2-R2 similarity correlation with logratio: -0.5745852117123851\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return F.cosine_similarity(a, b, dim=-1)\n",
    "\n",
    "c1r1_sim = cosine_similarity(C1, R1)\n",
    "c2r2_sim = cosine_similarity(C2, R2)\n",
    "\n",
    "c1r1_sim_corr = np.corrcoef(c1r1_sim.flatten(), logratios1)[0,1]\n",
    "c2r2_sim_corr = np.corrcoef(c2r2_sim.flatten(), logratios2)[0,1]\n",
    "c1r1null_sim_corr = np.corrcoef(c1r1_sim.flatten(), logratios1.roll(1, 0))[0,1]\n",
    "c2r2null_sim_corr = np.corrcoef(c2r2_sim.flatten(), logratios2.roll(1, 0))[0,1]\n",
    "shypothesis('c1r1_sim_corr > c1r1null_sim_corr', variables=locals())\n",
    "shypothesis('c2r2_sim_corr > c2r2null_sim_corr', variables=locals())\n",
    "\n",
    "print(f\"C1-R1 similarity correlation with logratio: {c1r1_sim_corr}\")\n",
    "print(f\"C2-R2 similarity correlation with logratio: {c2r2_sim_corr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if we project hs onto the global mean then correlate...\n",
      "Hypothesis: c1 > c3n\n",
      "          : 0.168 > 0.298\n",
      "Residual  : = c1 - c3n\n",
      "Residual  : = -0.130 ❌\n",
      "\n",
      "Hypothesis: c2 > c4n\n",
      "          : 0.425 > 0.236\n",
      "Residual  : = c2 - c4n\n",
      "Residual  : = 0.189 ✅\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def global_position(x, global_mean):\n",
    "    return F.cosine_similarity(x - global_mean, global_mean, dim=-1)\n",
    "\n",
    "global_mean = torch.cat([C1, R1, C2, R2], dim=0).mean(dim=(0), keepdim=True)\n",
    "\n",
    "def glob_pos_corr(C, R, logratios):\n",
    "    c_global = global_position(C, global_mean)\n",
    "    r_global = global_position(R, global_mean)\n",
    "    global_diff = c_global - r_global\n",
    "    global_corr = np.corrcoef(global_diff.flatten(), logratios)[0,1]\n",
    "    return np.abs(global_corr)\n",
    "\n",
    "print('if we project hs onto the global mean then correlate...')\n",
    "c1 = glob_pos_corr(C1, R1, logratios1)\n",
    "c2 = glob_pos_corr(C2, R2, logratios2)\n",
    "c3n = glob_pos_corr(C1, R1, logratios1.roll(1, 0))\n",
    "c4n = glob_pos_corr(C2, R2, logratios2.roll(1, 0))\n",
    "shypothesis('c1 > c3n', variables=locals())\n",
    "shypothesis('c2 > c4n', variables=locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothesis: c1 > c3n\n",
      "          : 0.164 > 0.268\n",
      "Residual  : = c1 - c3n\n",
      "Residual  : = -0.104 ❌\n",
      "\n",
      "Hypothesis: c2 > c4n\n",
      "          : 0.410 > 0.228\n",
      "Residual  : = c2 - c4n\n",
      "Residual  : = 0.182 ✅\n",
      "\n",
      "Relative position correlation: 0.16419729238550518\n",
      "Relative position correlation (null): 0.41037309139761635\n"
     ]
    }
   ],
   "source": [
    "def relative_position(a, b, ref):\n",
    "    a_rel = F.cosine_similarity(a - ref, ref, dim=-1)\n",
    "    b_rel = F.cosine_similarity(b - ref, ref, dim=-1)\n",
    "    return a_rel - b_rel\n",
    "\n",
    "# Use C2 mean as reference\n",
    "def rel_pos_corr(C1, R1, C2, logratios1):\n",
    "    ref = C2.mean(dim=(0), keepdim=True)\n",
    "    rel_pos1 = relative_position(C1, R1, ref)\n",
    "    rel_pos_corr = np.abs(np.corrcoef(rel_pos1.flatten(), logratios1)[0,1])\n",
    "    return rel_pos_corr\n",
    "\n",
    "c1 = rel_pos_corr(C1, R1, C2, logratios1)\n",
    "c2 = rel_pos_corr(C2, R2, C1, logratios2)\n",
    "c3n = rel_pos_corr(C1, R1, C2, logratios1.roll(1, 0))\n",
    "c4n = rel_pos_corr(C2, R2, C1, logratios2.roll(1, 0))\n",
    "shypothesis('c1 > c3n', variables=locals())\n",
    "shypothesis('c2 > c4n', variables=locals())\n",
    "print(f\"Relative position correlation: {c1}\")\n",
    "print(f\"Relative position correlation (null): {c2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preference direction projection correlation: 0.14460565238170017\n",
      "Preference direction projection correlation: 0.5432624445982506\n",
      "Preference direction projection correlation: 0.2423268342820158\n",
      "Preference direction projection correlation: 0.17629937077085425\n",
      "Hypothesis: proj_corr1 > proj_corr4\n",
      "          : 0.145 > 0.176\n",
      "Residual  : = proj_corr1 - proj_corr4\n",
      "Residual  : = -0.032 ❌\n",
      "\n",
      "Hypothesis: proj_corr2 > proj_corr3\n",
      "          : 0.543 > 0.242\n",
      "Residual  : = proj_corr2 - proj_corr3\n",
      "Residual  : = 0.301 ✅\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def projection_correlation(C, R, logratios):\n",
    "    # Compute the mean difference vector\n",
    "    mean_diff = (C - R).mean(dim=0)\n",
    "\n",
    "    # Project samples onto this direction\n",
    "    c_proj = F.cosine_similarity(C, mean_diff.unsqueeze(0), dim=-1)\n",
    "    r_proj = F.cosine_similarity(R, mean_diff.unsqueeze(0), dim=-1)\n",
    "    proj_diff = c_proj - r_proj\n",
    "\n",
    "    proj_corr = np.abs(np.corrcoef(proj_diff, logratios)[0,1])\n",
    "    return proj_corr\n",
    "\n",
    "proj_corr1 = projection_correlation(C1, R1, logratios1)\n",
    "print(f\"Preference direction projection correlation: {proj_corr1}\")\n",
    "\n",
    "proj_corr2 = projection_correlation(C2, R2, logratios2)\n",
    "print(f\"Preference direction projection correlation: {proj_corr2}\")\n",
    "\n",
    "proj_corr3 = projection_correlation(C2, R2, logratios2.roll(1, 0))\n",
    "print(f\"Preference direction projection correlation: {proj_corr3}\")\n",
    "\n",
    "\n",
    "proj_corr4 = projection_correlation(C1, R1, logratios1.roll(1, 0))\n",
    "print(f\"Preference direction projection correlation: {proj_corr4}\")\n",
    "\n",
    "shypothesis('proj_corr1 > proj_corr4', variables=locals())\n",
    "shypothesis('proj_corr2 > proj_corr3', variables=locals())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance difference correlation: 0.1748688256315504\n",
      "Variance difference correlation: 0.32789284950757314\n",
      "Variance difference correlation: -0.29289889727162416\n",
      "Variance difference correlation: -0.06691499025422101\n",
      "Hypothesis: abs(vc0) > abs(vc3)\n",
      "          : Abs(vc0) > Abs(vc3)\n",
      "     Where: {'vc0': 0.1748688256315504, 'vc3': -0.06691499025422101}\n",
      "          : 0.175 > 0.067\n",
      "Residual  : = Abs(vc0) - Abs(vc3)\n",
      "Residual  : = 0.108 ✅\n",
      "\n",
      "Hypothesis: abs(vc1) > abs(vc2)\n",
      "          : Abs(vc1) > Abs(vc2)\n",
      "     Where: {'vc1': 0.32789284950757314, 'vc2': -0.29289889727162416}\n",
      "          : 0.328 > 0.293\n",
      "Residual  : = Abs(vc1) - Abs(vc2)\n",
      "Residual  : = 0.035 ✅\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def var_corr(C, R, logratios):\n",
    "    c_var = C.var(dim=-1)\n",
    "    r_var = R.var(dim=-1)\n",
    "    var_diff = c_var - r_var\n",
    "\n",
    "    var_corr = np.corrcoef(var_diff.flatten(), logratios)[0,1]\n",
    "    return var_corr\n",
    "\n",
    "\n",
    "vc0 = var_corr(C1, R1, logratios1)\n",
    "print(f\"Variance difference correlation: {vc0}\")\n",
    "\n",
    "vc1 = var_corr(C2, R2, logratios2)\n",
    "print(f\"Variance difference correlation: {vc1}\")\n",
    "\n",
    "vc2 = var_corr(C2, R2, logratios2.roll(1, 0))\n",
    "print(f\"Variance difference correlation: {vc2}\")\n",
    "\n",
    "vc3 = var_corr(C1, R1, logratios1.roll(1, 0))\n",
    "print(f\"Variance difference correlation: {vc3}\")\n",
    "shypothesis('abs(vc0) > abs(vc3)', variables=locals(), verbose=1)\n",
    "shypothesis('abs(vc1) > abs(vc2)', variables=locals(), verbose=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
