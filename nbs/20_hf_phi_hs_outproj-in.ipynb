{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we take the hidden states inside the mlp and attention side channels, and apply reroute and retain losses, in an attemp to improve learning and generalization compared to DPI. The idea is to force the model to learn to use the hidden states in the side channels, and to retain the information that is useful for the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"repo-dpo\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] =  os.path.basename(globals()['__vsc_ipynb_file__'])\n",
    "nb_name = os.path.basename(globals()['__vsc_ipynb_file__']).replace('.ipynb', '').replace(' ', '_')\n",
    "# enable wandb service (experimental, https://github.com/wandb/client/blob/master/docs/dev/wandb-service-user.md)\n",
    "# this hopefully fixes issues with multiprocessing\n",
    "wandb.require(experiment='service')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reprpo import silence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import DPOTrainer\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Union\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from transformers.trainer import ProgressCallback\n",
    "from transformers.utils.notebook import NotebookProgressCallback\n",
    "\n",
    "from reprpo.helpers.adapters import set_adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "# torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13500"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_prompt_length=64\n",
    "# num_samples = 50 * 16 * 6\n",
    "num_samples = 1500 * 3 * 3 # from circuit breaker * 3\n",
    "max_length = 128\n",
    "num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need\n",
    "# !pip install flash-attn --no-build-isolation --no-deps -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "befbdc61f6b843eba2f9f3b500b5a898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 25165824 || all params: 3845919744 || trainable%: 0.6543512521097476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Phi3ForCausalLM(\n",
       "      (model): Phi3Model(\n",
       "        (embed_tokens): Embedding(32011, 3072, padding_idx=32000)\n",
       "        (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x Phi3DecoderLayer(\n",
       "            (self_attn): Phi3FlashAttention2(\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (ReprPO): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (ReprPO): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (ReprPO): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (qkv_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (ReprPO): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (ReprPO): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (ReprPO): Linear(in_features=16, out_features=9216, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): Phi3RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Phi3MLP(\n",
       "              (gate_up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (ReprPO): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (ReprPO): Linear(in_features=3072, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (ReprPO): Linear(in_features=16, out_features=16384, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (ReprPO): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (ReprPO): Linear(in_features=8192, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (ReprPO): Linear(in_features=16, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (activation_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Phi3RMSNorm()\n",
       "            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (post_attention_layernorm): Phi3RMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): Phi3RMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3072, out_features=32011, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FIXME: we are meant to SFT first, so that the preferences are in sample but 1) if this works it might not be needed, and 2) this can be added later, if it works\n",
    "# for now we will use the instruct model, and try something it wasn't meant to do but it in sample \n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "# model_name = \"NousResearch/Meta-Llama-3-8B-Instruct\"\n",
    "# model_name = \"microsoft/Phi-3-mini-4k-instruct-gguf\"\n",
    "# model_name = \"NousResearch/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "\n",
    "\n",
    "from peft.tuners import BOFTConfig, OFTConfig, LoraConfig, IA3Config\n",
    "## Big adapter\n",
    "# peft_config = OFTConfig(\n",
    "#     r=4,\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "#     target_modules=[\"qkv_proj\", \"down_proj\",\n",
    "#                     \"o_proj\", \"gate_up_proj\",\n",
    "#                     ],\n",
    "# )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# rescale\n",
    "Infused Adapter by Inhibiting and Amplifying Inner Activations, or IA3, is a method that adds three learned vectors to rescale the keys and values of the self-attention and encoder-decoder attention layers, and the intermediate activation of the position-wise feed-forward network.\"\"\"\n",
    "# peft_config = IA3Config(\n",
    "#     # r=4,\n",
    "#     # task_type=\"CAUSAL_LM\",\n",
    "#     target_modules=[\"qkv_proj\", \"down_proj\",\n",
    "#                     \"o_proj\", \"gate_up_proj\",\n",
    "#                     ],\n",
    "#     feedforward_modules=[\"gate_up_proj\", \"down_proj\"]\n",
    "# )\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16, \n",
    "    r=16,\n",
    "    # lora_dropout=0.05,\n",
    "    # use_rslora=True,\n",
    "    # use_dora=True,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        # FIXME: I'm not sure we can do LORA on the layer we are targeting?\n",
    "        \"qkv_proj\", \"gate_up_proj\", # in\n",
    "        \"down_proj\",  \"o_proj\", # out\n",
    "                    \n",
    "                    ], # PHI3\n",
    "    # target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"] # LLAMA\n",
    ")\n",
    "from reprpo.models.load import load_model, print_trainable_parameters\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl.trainer.utils import peft_module_casting_to_bf16\n",
    "\n",
    "bnb = False # this doesn't seem to be able to backprop when using baukit\n",
    "use_gradient_checkpointing = False\n",
    "use_inputs = True\n",
    "\n",
    "model, tokenizer = load_model(model_name, bnb=bnb )\n",
    "\n",
    "if use_gradient_checkpointing:\n",
    "    model.enable_input_require_grads()\n",
    "adapter_name='ReprPO'\n",
    "peft_module_casting_to_bf16(model)\n",
    "model = prepare_model_for_kbit_training(model, {'use_gradient_checkpointing': use_gradient_checkpointing})\n",
    "model = get_peft_model(model, peft_config, adapter_name=adapter_name)\n",
    "print_trainable_parameters(model)\n",
    "# phi model def https://github.com/huggingface/transformers/blob/14ee2326e51cb210cec72f31b248cb722e9d5d1f/src/transformers/models/phi/modeling_phi.py#L748\n",
    "# 2 paths, mlp, and attn\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(dataset, N):\n",
    "    return (dataset\n",
    "            .shuffle(42)\n",
    "            .select(range(\n",
    "            min(len(dataset), N)))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'When did women join the labor workforce outside of the home? What caused the change from housewife to breadwinner? Write a short informative paragraph under 200 words. ',\n",
       " 'chosen': \"In the early 20th century, less than 20% of women worked outside the home. Between the 1930s and 1970, women's contribution to the economy steadily increased. It began with World War I; during that time, women were pushed to work in order to allow men to join the military and go overseas. Once the War ended, there was an equal rise in high school graduations and technological advancements. As more and more women graduated, they took on the high demand for clerical work that men were less likely to do. Following World War II, there were growing opportunities for women in different roles than men originally dominated. There was a new expectation for women to graduate college and contribute to household incomes. By 1990, the percentage of women working increased by 76%. Recent research shows that there are approximately the same number of women in professional schools as men. According to Forbes in July 2023, 57.4 percent of the workforce are women.\",\n",
       " 'rejected': \"The 1900s saw a dramatic shift in the role of women in the workforce. Prior to this time, women were primarily homemakers and caregivers, with limited opportunities for paid work outside the home. However, with the rise of industrialization and the growth of the economy, women began to enter the workforce in increasing numbers.\\n\\nOne of the primary factors that led to this change was the need for workers in factories and other industries. As men went off to war or pursued other opportunities, women were needed to fill the gaps in the workforce. This need for workers created a new sense of opportunity for women, who had previously been limited by social and cultural expectations.\\n\\nAnother factor that contributed to the change was the growing feminist movement. Women's rights activists worked to break down barriers and stereotypes that prevented women from pursuing their own goals and ambitions. They fought for equal pay, equal opportunities, and the right to vote, among other rights.\\n\\nAs a result of these and other factors, women began to enter the workforce in increasing numbers. They worked in factories, offices, and other industries, often performing the same jobs as men. This shift in the role of women had a profound impact on society, changing the way that people thought about gender roles and the value of women's work.\\n\\nToday, women continue to make up a significant portion of the workforce, and many have achieved success in a wide range of industries. While there is still work to be done to achieve true gender equality, the changes that began in the early 1900s have paved the way for a more diverse and inclusive workforce.\"}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset = load_dataset('Columbia-NLP/DPO-HelpSteer')\n",
    "dataset = load_dataset('Atsunori/HelpSteer2-DPO').map(lambda x: {\n",
    "    'prompt': x['prompt']+ ' '})\n",
    "dataset['train'] = sample(dataset['train'], num_samples)\n",
    "dataset2 = dataset.rename_column('chosen_response', 'chosen').rename_column('rejected_response', 'rejected')\n",
    "dataset2['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def format_ds(row):\n",
    "    \n",
    "#     # WHY are we doing this? Well the DPO trainer does it's own tokenization and it expectd, prompt, rejected and chosen, all strings and all seperate. Is this good, idk\n",
    "#     return {\n",
    "#         \"chosen\": row['chosen_response'][1]['content'],\n",
    "#         \"rejected\": row['rejected_response'][1]['content'],\n",
    "#     }\n",
    "\n",
    "\n",
    "# dataset2 = dataset.map(format_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When did women join the labor workforce outside of the home? What caused the change from housewife to breadwinner? Write a short informative paragraph under 200 words. \n",
      "===\n",
      "In the early 20th century, less than 20% of women worked outside the home. Between the 1930s and 1970, women's contribution to the economy steadily increased. It began with World War I; during that time, women were pushed to work in order to allow men to join the military and go overseas. Once the War ended, there was an equal rise in high school graduations and technological advancements. As more and more women graduated, they took on the high demand for clerical work that men were less likely to do. Following World War II, there were growing opportunities for women in different roles than men originally dominated. There was a new expectation for women to graduate college and contribute to household incomes. By 1990, the percentage of women working increased by 76%. Recent research shows that there are approximately the same number of women in professional schools as men. According to Forbes in July 2023, 57.4 percent of the workforce are women.\n",
      "---\n",
      "The 1900s saw a dramatic shift in the role of women in the workforce. Prior to this time, women were primarily homemakers and caregivers, with limited opportunities for paid work outside the home. However, with the rise of industrialization and the growth of the economy, women began to enter the workforce in increasing numbers.\n",
      "\n",
      "One of the primary factors that led to this change was the need for workers in factories and other industries. As men went off to war or pursued other opportunities, women were needed to fill the gaps in the workforce. This need for workers created a new sense of opportunity for women, who had previously been limited by social and cultural expectations.\n",
      "\n",
      "Another factor that contributed to the change was the growing feminist movement. Women's rights activists worked to break down barriers and stereotypes that prevented women from pursuing their own goals and ambitions. They fought for equal pay, equal opportunities, and the right to vote, among other rights.\n",
      "\n",
      "As a result of these and other factors, women began to enter the workforce in increasing numbers. They worked in factories, offices, and other industries, often performing the same jobs as men. This shift in the role of women had a profound impact on society, changing the way that people thought about gender roles and the value of women's work.\n",
      "\n",
      "Today, women continue to make up a significant portion of the workforce, and many have achieved success in a wide range of industries. While there is still work to be done to achieve true gender equality, the changes that began in the early 1900s have paved the way for a more diverse and inclusive workforce.\n"
     ]
    }
   ],
   "source": [
    "r = dataset2['train'][0]\n",
    "print(r['prompt'])\n",
    "print('===')\n",
    "print(r['chosen'])\n",
    "print('---')\n",
    "print(r['rejected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from reprpo.eval.mc import eval_tqa\n",
    "from reprpo.gen import generation_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified classes\n",
    "\n",
    "- here we can defined the experimetns loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # modified from https://github.com/huggingface/peft/blob/4611034ff85e26e1f9647ea1f505e9e50322ff0f/src/peft/peft_model.py#L1005\n",
    "# keys = [\n",
    "#     \"qkv_proj\",\n",
    "#     \"o_proj\",\n",
    "# ]\n",
    "# prefix = \"base_model.model.\"\n",
    "# for key in keys:\n",
    "#     suffix_pos = key.rfind(\".\")\n",
    "#     extended_prefix = prefix + key[:suffix_pos]\n",
    "#     module = dict(model.named_modules())[extended_prefix]\n",
    "# model.named_modules\n",
    "# model.get_submodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ReprPOConfig2Outs(DPOConfig):\n",
    "    alpha: int = 1\n",
    "    print_every: int = 20\n",
    "    collection_layers: tuple = (11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 22)\n",
    "    # collection_keys: tuple = ('base_model.model.model.layers.{layer}.self_attn.o_proj', 'base_model.model.model.layers.{layer}.mlp.down_proj')\n",
    "    # collect_input: bool = True\n",
    "\n",
    "    collection_keys: tuple = ('base_model.model.model.layers.{layer}.self_attn.qkv_proj', 'base_model.model.model.layers.{layer}.mlp.gate_up_proj')\n",
    "    collect_input: bool = False\n",
    "\n",
    "    # NOTE to self, do not pass both peft_config and model_adapter_name. peft_config creates a new adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class ReprPOConfig2Ins(DPOConfig):\n",
    "    alpha: int = 1\n",
    "    print_every: int = 20\n",
    "    collection_layers: tuple = (11, 12, 13, 14, 15, 16, 17, 19, 20, 21, 22)\n",
    "    collection_keys: tuple = ('base_model.model.model.layers.{layer}.self_attn.o_proj', 'base_model.model.model.layers.{layer}.mlp.down_proj')\n",
    "    collect_input: bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_inputs:\n",
    "    ReprPOConfig2 = ReprPOConfig2Ins # note sure this has grad\n",
    "else:\n",
    "    ReprPOConfig2 = ReprPOConfig2Outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['base_model.model.model.layers.11.self_attn.o_proj', 'base_model.model.model.layers.11.mlp.down_proj', 'base_model.model.model.layers.12.self_attn.o_proj', 'base_model.model.model.layers.12.mlp.down_proj', 'base_model.model.model.layers.13.self_attn.o_proj', 'base_model.model.model.layers.13.mlp.down_proj', 'base_model.model.model.layers.14.self_attn.o_proj', 'base_model.model.model.layers.14.mlp.down_proj', 'base_model.model.model.layers.15.self_attn.o_proj', 'base_model.model.model.layers.15.mlp.down_proj', 'base_model.model.model.layers.16.self_attn.o_proj', 'base_model.model.model.layers.16.mlp.down_proj', 'base_model.model.model.layers.17.self_attn.o_proj', 'base_model.model.model.layers.17.mlp.down_proj', 'base_model.model.model.layers.19.self_attn.o_proj', 'base_model.model.model.layers.19.mlp.down_proj', 'base_model.model.model.layers.20.self_attn.o_proj', 'base_model.model.model.layers.20.mlp.down_proj', 'base_model.model.model.layers.21.self_attn.o_proj', 'base_model.model.model.layers.21.mlp.down_proj', 'base_model.model.model.layers.22.self_attn.o_proj', 'base_model.model.model.layers.22.mlp.down_proj']\n",
      "target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}\n",
      "o_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "down_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "o_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "down_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "o_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "down_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "o_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "down_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "o_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "down_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "o_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "down_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "o_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "down_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "o_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "down_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "o_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "down_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "o_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "down_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "o_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "down_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n"
     ]
    }
   ],
   "source": [
    "def check_training_args(training_args, model):\n",
    "    assert training_args.collection_layers is not None\n",
    "    assert training_args.collection_keys is not None\n",
    "    ps = [[p.format(layer=layer) for p in training_args.collection_keys] for layer in training_args.collection_layers]\n",
    "    ps = list(itertools.chain(*ps))\n",
    "    print(ps)\n",
    "    ps2 = [model.get_submodule(p) for p in ps]\n",
    "\n",
    "\n",
    "    # also if module in model.peft_config[adapter_name].target_modules\n",
    "\n",
    "    target_modules = model.peft_config[adapter_name].target_modules\n",
    "    print('target_modules', target_modules)\n",
    "    for p in ps:\n",
    "        r = p.rsplit('.', 1)[-1]\n",
    "        if r in target_modules:\n",
    "            print(f\"{r} in target_modules {target_modules}. You may want to append .base_layer\")\n",
    "        # else:\n",
    "        #     print(f\"{r} not in target_modules {target_modules}\")\n",
    "    return training_args\n",
    "\n",
    "training_args = ReprPOConfig2('')\n",
    "check_training_args(training_args, model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reprpo.trainer import ReprPOTrainer, ReprPOConfig, mean_with_attention, normalize_output\n",
    "from baukit.nethook import TraceDict\n",
    "from einops import repeat\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "\n",
    "def mean_with_attention(x: Float[Tensor, 'b t h'], attn_mask: Float[Tensor, 'b t'], dim: int = 1) -> Float[Tensor, 'b h']:\n",
    "    \"\"\"mean of x, weighted by the attention mask, over dim (token or batch)\"\"\"\n",
    "    layer_attn_mask = repeat(attn_mask, 'b t -> b t h', h=1).detach()\n",
    "    return (x * layer_attn_mask).sum(dim) / layer_attn_mask.sum(dim)\n",
    "\n",
    "def detach_hsd(hs):\n",
    "    return {k: v.detach() for k, v in hs.items()}\n",
    "\n",
    "def get_layer_paths(args):\n",
    "    layer_paths = [\n",
    "            [p.format(layer=layer) for p in args.collection_keys]\n",
    "            for layer in args.collection_layers\n",
    "    ]\n",
    "    layer_paths = list(itertools.chain(*layer_paths))\n",
    "    return layer_paths\n",
    "\n",
    "class ReprPOTrainer2(ReprPOTrainer):\n",
    "\n",
    "    def __init__(self, args: Optional[ReprPOConfig] = None, **kwargs):\n",
    "        DPOTrainer.__init__(self, args=args, **kwargs)\n",
    "        self.collection_layers = args.collection_layers\n",
    "        self.alpha = args.alpha\n",
    "        self.loss_type = 'ipo'\n",
    "\n",
    "        self.num_training_steps = self.args.max_steps\n",
    "        if self.num_training_steps==-1:\n",
    "            self.num_training_steps = self.args.num_train_epochs * len(self.get_train_dataloader()) // self.args.gradient_accumulation_steps\n",
    "\n",
    "\n",
    "        self.layer_paths = get_layer_paths(args)\n",
    "        print(self.layer_paths)\n",
    "\n",
    "\n",
    "    def get_batch_loss_metrics(\n",
    "        self,\n",
    "        model,\n",
    "        batch: Dict[str, Union[List, torch.LongTensor]],\n",
    "        train_eval: Literal[\"train\", \"eval\"] = \"train\",\n",
    "    ):\n",
    "        \"\"\"Compute the DPO loss and other metrics for the given batch of inputs for train or test.\"\"\"\n",
    "        metrics = {}\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            with self.null_ref_context():\n",
    "                (\n",
    "                    ref_chosen_logps,\n",
    "                    ref_rejected_logps,\n",
    "                    _,\n",
    "                    _,\n",
    "                    _,\n",
    "                    ref_chosen_hs,\n",
    "                    ref_rejected_hs,\n",
    "                    _,\n",
    "                    _\n",
    "                ) = self.concatenated_forward(self.model, batch)\n",
    "        ref_chosen_hs = detach_hsd(ref_chosen_hs)\n",
    "        ref_rejected_hs = detach_hsd(ref_rejected_hs)\n",
    "        ref_chosen_logps = ref_chosen_logps.detach()\n",
    "        ref_rejected_logps = ref_rejected_logps.detach()\n",
    "\n",
    "        model.train()\n",
    "        (\n",
    "            pi_chosen_logps,\n",
    "            pi_rejected_logps,\n",
    "            _,\n",
    "            _,\n",
    "            pi_chosen_logps_avg,\n",
    "            pi_chosen_hs,\n",
    "            pi_rejected_hs,\n",
    "            chosen_attn_mask,\n",
    "            rejected_attn_mask\n",
    "        ) = self.concatenated_forward(model, batch)\n",
    "\n",
    "        loss, loss_info = self.reprpo_loss(\n",
    "            pi_chosen_logps,\n",
    "            pi_rejected_logps,\n",
    "            pi_chosen_hs,\n",
    "            pi_rejected_hs,\n",
    "            ref_chosen_logps,\n",
    "            ref_rejected_logps,\n",
    "            ref_chosen_hs,\n",
    "            ref_rejected_hs,\n",
    "            chosen_attn_mask,\n",
    "            rejected_attn_mask\n",
    "        )\n",
    "        # losses, chosen_rewards, rejected_rewards, loss_retain, loss_rr = loss_info\n",
    "        chosen_rewards, rejected_rewards = (\n",
    "            loss_info[\"chosen_rewards\"],\n",
    "            loss_info[\"rejected_rewards\"],\n",
    "        )\n",
    "        reward_accuracies = (chosen_rewards > rejected_rewards).float()\n",
    "\n",
    "        if self.args.rpo_alpha is not None:\n",
    "            loss = loss * self.args.rpo_alpha - pi_chosen_logps_avg\n",
    "\n",
    "\n",
    "        prefix = \"eval_\" if train_eval == \"eval\" else \"\"\n",
    "        \n",
    "        # how often the policy model is better at choosing the right response\n",
    "        metrics[f\"{prefix}rewards/accuracies\"] = reward_accuracies.mean().cpu()\n",
    "        # how much the policy model is better\n",
    "        metrics[f\"{prefix}rewards/margins\"] = (\n",
    "            (chosen_rewards - rejected_rewards).mean().cpu()\n",
    "        )\n",
    "\n",
    "        # the log probability that the model would generate the tokens of the rejected string\n",
    "        metrics[f\"{prefix}logps/rejected\"] = pi_rejected_logps.detach().mean().cpu()\n",
    "        metrics[f\"{prefix}logps/chosen\"] = pi_chosen_logps.detach().mean().cpu()\n",
    "\n",
    "\n",
    "        for k in loss_info.keys():\n",
    "            if '_' in k:\n",
    "                a,b = k.split('_', 1)\n",
    "                k2 = f\"{b}/{a}\"\n",
    "            else:\n",
    "                k2 = k\n",
    "            v = loss_info[k]\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.mean().detach().cpu().item()\n",
    "            metrics[f\"{prefix}{k2}\"] = float(v)\n",
    "\n",
    "        if self.state.global_step % self.args.print_every == 0:\n",
    "            \n",
    "            def cosine_on_keys(hs1, hs2):\n",
    "                return torch.stack([F.cosine_similarity(hs1[k], hs2[k], dim=-1).mean() for k in hs1.keys()]).mean()\n",
    "            retain_cosine = cosine_on_keys(pi_chosen_hs, ref_chosen_hs)\n",
    "            rr_cosine = cosine_on_keys(pi_rejected_hs, ref_chosen_hs)\n",
    "            \n",
    "            metrics[f\"{prefix}retain_cosine\"] = retain_cosine\n",
    "            metrics[f\"{prefix}rr_cosine\"] = rr_cosine\n",
    "\n",
    "            print({k: f\"{v:.2g}\" for k, v in metrics.items()})\n",
    "        \n",
    "        return loss.mean(), metrics\n",
    "\n",
    "    def concatenated_forward(\n",
    "        self, model: nn.Module, batch: Dict[str, Union[List, torch.LongTensor]]\n",
    "    ) -> Tuple[\n",
    "        torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor\n",
    "    ]:\n",
    "        \"\"\"Run the given model on the given batch of inputs, concatenating the chosen and rejected inputs together.\n",
    "\n",
    "        We do this to avoid doing two forward passes, because it's faster for FSDP.\n",
    "        \"\"\"\n",
    "        concatenated_batch = self.concatenated_inputs(\n",
    "            batch,\n",
    "            is_encoder_decoder=self.is_encoder_decoder,\n",
    "            label_pad_token_id=self.label_pad_token_id,\n",
    "            padding_value=self.padding_value,\n",
    "            device=self.accelerator.device,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "        len_chosen = batch[\"chosen_labels\"].shape[0]\n",
    "\n",
    "        model_kwargs = (\n",
    "            {\n",
    "                \"labels\": concatenated_batch[\"concatenated_labels\"],\n",
    "                \"decoder_input_ids\": concatenated_batch.pop(\n",
    "                    \"concatenated_decoder_input_ids\", None\n",
    "                ),\n",
    "            }\n",
    "            if self.is_encoder_decoder\n",
    "            else {}\n",
    "        )\n",
    "\n",
    "        reprs = {}\n",
    "        with TraceDict(model, self.layer_paths, retain_input=self.args.collect_input, retain_output=(not self.args.collect_input), retain_grad=False) as ret:\n",
    "            outs = model(\n",
    "                concatenated_batch[\"concatenated_input_ids\"],\n",
    "                attention_mask=concatenated_batch[\"concatenated_attention_mask\"],\n",
    "                use_cache=False,\n",
    "                return_dict=True,\n",
    "                output_hidden_states=True,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "            for p in self.layer_paths:\n",
    "                if self.args.collect_input:\n",
    "                    reprs[p] = ret[p].input\n",
    "                else:\n",
    "                    reprs[p] = ret[p].output\n",
    "            # print(reprs[p].shape, reprs[p].dtype)\n",
    "        all_logits = outs.logits\n",
    "        \n",
    "        # # this includes prompt and padding\n",
    "        # hs = collect_hs(outs.hidden_states)[:, self.collection_layers]\n",
    "        # # del outs\n",
    "        # # gc.collect()\n",
    "\n",
    "        # multiply by attention mask\n",
    "        attn_mask = concatenated_batch[\"concatenated_attention_mask\"]\n",
    "\n",
    "\n",
    "        all_logps, size_completion = self.get_batch_logps(\n",
    "            all_logits,\n",
    "            concatenated_batch[\"concatenated_labels\"],\n",
    "            # average_log_prob=self.loss_type == \"ipo\",\n",
    "            is_encoder_decoder=self.is_encoder_decoder,\n",
    "            label_pad_token_id=self.label_pad_token_id,\n",
    "        )\n",
    "        chosen_logps_avg = all_logps[:len_chosen] / size_completion[:len_chosen]\n",
    "\n",
    "        # So we want sum of logprobs or mean of logprobs? Like IPO we will use the log prob per token, https://github.com/eric-mitchell/direct-preference-optimization/issues/40\n",
    "        if self.loss_type == \"ipo\":\n",
    "            all_logps = all_logps / size_completion\n",
    "            # all_logps = torch.log(torch.exp(all_logps) / size_completion + 1e-12)\n",
    "            # NOTE for some reason the model is still biased toward longer answers, even though this should neutralise it\n",
    "\n",
    "        chosen_logps = all_logps[:len_chosen]\n",
    "        rejected_logps = all_logps[len_chosen:]\n",
    "\n",
    "        chosen_logits = all_logits[:len_chosen]\n",
    "        rejected_logits = all_logits[len_chosen:]\n",
    "\n",
    "        chosen_hs = {k: hs[:len_chosen] for k, hs in reprs.items()}\n",
    "        rejected_hs = {k: hs[len_chosen:] for k, hs in reprs.items()}\n",
    "\n",
    "        chosen_attn_mask = attn_mask[:len_chosen]\n",
    "        rejected_attn_mask = attn_mask[len_chosen:]\n",
    "\n",
    "        return (\n",
    "            chosen_logps,\n",
    "            rejected_logps,\n",
    "            chosen_logits,\n",
    "            rejected_logits,\n",
    "            chosen_logps_avg,\n",
    "            chosen_hs,\n",
    "            rejected_hs,\n",
    "            chosen_attn_mask,\n",
    "            rejected_attn_mask\n",
    "        )\n",
    "\n",
    "\n",
    "    def reprpo_loss(\n",
    "        self,\n",
    "        pi_chosen_logps: torch.FloatTensor,\n",
    "        pi_rejected_logps: torch.FloatTensor,\n",
    "        pi_cho_hs: torch.FloatTensor,\n",
    "        pi_rej_hs: torch.FloatTensor,\n",
    "        ref_chosen_logps: torch.FloatTensor,\n",
    "        ref_rejected_logps: torch.FloatTensor,\n",
    "        ref_cho_hs: torch.FloatTensor,\n",
    "        ref_rej_hs: torch.FloatTensor,\n",
    "        cho_attn_mask: torch.BoolTensor,\n",
    "        rej_attn_mask: torch.BoolTensor\n",
    "    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n",
    "        \"\"\"Compute the DPO loss for a batch of policy and reference model log probabilities.\n",
    "\n",
    "        Args:\n",
    "            pi_chosen_logps: Log probabilities of the policy model for the chosen responses. Shape: (batch_size,)\n",
    "            pi_rejected_logps: Log probabilities of the policy model for the rejected responses. Shape: (batch_size,)\n",
    "            ref_chosen_logps: Log probabilities of the reference model for the chosen responses. Shape: (batch_size,)\n",
    "            ref_rejected_logps: Log probabilities of the reference model for the rejected responses. Shape: (batch_size,)\n",
    "\n",
    "        Returns:\n",
    "            A tuple of three tensors: (losses, chosen_rewards, rejected_rewards).\n",
    "            The losses tensor contains the DPO loss for each example in the batch.\n",
    "            The chosen_rewards and rejected_rewards tensors contain the rewards for the chosen and rejected responses, respectively.\n",
    "        \"\"\"\n",
    "\n",
    "        pi_logratios = pi_chosen_logps - pi_rejected_logps\n",
    "        if self.reference_free:\n",
    "            ref_logratios = torch.tensor(\n",
    "                [0], dtype=pi_logratios.dtype, device=pi_logratios.device\n",
    "            )\n",
    "        else:\n",
    "            ref_logratios = ref_chosen_logps - ref_rejected_logps\n",
    "\n",
    "        # log(prob_chosen/prob_rejected) the prob of the chosen strings over the rejected string. 0 is not difference. -ve means rejected is larger\n",
    "        pi_logratios = pi_logratios.to(self.accelerator.device)\n",
    "        ref_logratios = ref_logratios.to(self.accelerator.device)\n",
    "        logits = pi_logratios - ref_logratios # was pi more likely to chose the correct response or the reference model\n",
    "\n",
    "        # Can we weight by how much better the reference model was\n",
    "        # in dpo we minimise it, so lower is better, here we are weighting it, so take the -ve to higher is more correct\n",
    "        # NOTE: -logits is if pi is more correct than ref, and focuses on what model gets wrong, unstable, moving target\n",
    "        # -ref_logratios is is the reference model lean toward correct, and is stable\n",
    "        T = 2\n",
    "        weight_correct = torch.softmax(-ref_logratios * T, 0).detach()\n",
    "\n",
    "        def _dist_w_attn_mask(chosen_hs, rejected_hs, attn):\n",
    "            dist = rejected_hs - chosen_hs\n",
    "            dist = mean_with_attention(dist, attn.detach())\n",
    "            assert torch.isfinite(dist).all()\n",
    "            # loss_rr = symlog(loss_rr)\n",
    "            # loss_rr = wmean(loss_rr, 1 - weight_correct)\n",
    "            return (dist**2).mean(-1) # [b]   \n",
    "        \n",
    "        def dist_w_attn_mask( chosen_hs, rejected_hs, attn):\n",
    "            dists = [_dist_w_attn_mask(chosen_hs[k], rejected_hs[k], attn).mean() for k in chosen_hs.keys()]  \n",
    "            return torch.stack(dists, dim=-1) # [b, n]\n",
    "\n",
    "        comb_attn_mask = cho_attn_mask * rej_attn_mask\n",
    "\n",
    "        hs_dist_cho2rej_pi2ref = dist_w_attn_mask(\n",
    "            detach_hsd(ref_cho_hs), \n",
    "            pi_rej_hs,\n",
    "            comb_attn_mask\n",
    "        )\n",
    "\n",
    "        # the loss is small, express it as a fraction of the reference values\n",
    "        hs_dist_cho2rej_ref2ref = dist_w_attn_mask(\n",
    "            ref_cho_hs, \n",
    "            ref_rej_hs, \n",
    "            comb_attn_mask\n",
    "        )\n",
    "\n",
    "        # how much we've reduced the distance between the chosen and rejected responses, compared to reference model\n",
    "        loss_reroute = hs_dist_cho2rej_pi2ref / hs_dist_cho2rej_ref2ref.mean().detach()\n",
    "\n",
    "        # this loss measures how much the policy model has retained the information in the chosen responses, compared to the reference model\n",
    "        hs_dist_cho2cho_pi2ref = dist_w_attn_mask(\n",
    "            detach_hsd(ref_cho_hs),\n",
    "            pi_cho_hs, \n",
    "            cho_attn_mask\n",
    "        )\n",
    "\n",
    "        # scale it, so that it's expressed as a fraction of the dist between rej2cho on the ref model\n",
    "        hs_dist_cho2rej_ref2ref = dist_w_attn_mask(\n",
    "            ref_cho_hs, \n",
    "            ref_rej_hs, \n",
    "            comb_attn_mask\n",
    "        )\n",
    "        # +1 so it start on par with reroute loss, and we can see it diverge?? TODO revisit\n",
    "        loss_retain = hs_dist_cho2cho_pi2ref / hs_dist_cho2rej_ref2ref.mean().detach() + 1\n",
    "\n",
    "        # Weightings\n",
    "        c_retain, c_reroute = self.get_coeff()\n",
    "        c_reroute = c_retain = 1\n",
    "        loss = (loss_reroute.mean() * c_reroute + loss_retain.mean() * c_retain * self.alpha)\n",
    "\n",
    "        # difference in logps for chosen responses, between policy and reference model\n",
    "        # # The beta is a temperature parameter for the DPO loss, typically something in the range of 0.1 to 0.5.\n",
    "        chosen_rewards = (\n",
    "            self.beta\n",
    "            * (\n",
    "                pi_chosen_logps.to(self.accelerator.device)\n",
    "                - ref_chosen_logps.to(self.accelerator.device)\n",
    "            ).detach()\n",
    "        )\n",
    "\n",
    "        # difference in logps for rejected responses, between policy and reference model\n",
    "        rejected_rewards = (\n",
    "            self.beta\n",
    "            * (\n",
    "                pi_rejected_logps.to(self.accelerator.device)\n",
    "                - ref_rejected_logps.to(self.accelerator.device)\n",
    "            ).detach()\n",
    "        )\n",
    "\n",
    "        loss_dict = dict(\n",
    "            loss=loss,\n",
    "            chosen_rewards=chosen_rewards,\n",
    "            rejected_rewards=rejected_rewards,\n",
    "            loss_retain=loss_retain.detach(),\n",
    "            loss_reroute=loss_reroute.detach(),\n",
    "            pi_logratios=pi_logratios.detach(),\n",
    "            ref_logratios=ref_logratios.detach(),\n",
    "            weighting=weight_correct.mean(),\n",
    "            logits=logits.mean().detach(),\n",
    "            loss_component_rr = (loss_reroute * c_reroute).detach().mean(),\n",
    "            loss_component_retain = (loss_retain * c_retain * self.alpha).detach().mean(),\n",
    "            c_rr=c_reroute,\n",
    "            c_retain=c_retain,\n",
    "        )\n",
    "\n",
    "        loss_dict = {k: normalize_output(v) for k, v in loss_dict.items()}\n",
    "\n",
    "        return loss, loss_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reprpo.helpers.torch import clear_mem\n",
    "clear_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7221"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update the ideal number of sample for how many are available\n",
    "num_data_samples = min(num_samples, len(dataset2['train']))\n",
    "num_data_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from reprpo.helpers.svd_decomposer import SVDDecomposer, DualSVDDecomposer\n",
    "# d = DualSVDDecomposer(model.get_input_embeddings().weight, model.lm_head.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ReprPO': LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='microsoft/Phi-3-mini-4k-instruct', revision=None, task_type='CAUSAL_LM', inference_mode=False, r=16, target_modules={'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}, lora_alpha=16, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gradient_accumulation_steps': 2, 'num_train_epochs': 1, 'batch_size': 1}\n",
      "['base_model.model.model.layers.11.self_attn.o_proj', 'base_model.model.model.layers.11.mlp.down_proj', 'base_model.model.model.layers.12.self_attn.o_proj', 'base_model.model.model.layers.12.mlp.down_proj', 'base_model.model.model.layers.13.self_attn.o_proj', 'base_model.model.model.layers.13.mlp.down_proj', 'base_model.model.model.layers.14.self_attn.o_proj', 'base_model.model.model.layers.14.mlp.down_proj', 'base_model.model.model.layers.15.self_attn.o_proj', 'base_model.model.model.layers.15.mlp.down_proj', 'base_model.model.model.layers.16.self_attn.o_proj', 'base_model.model.model.layers.16.mlp.down_proj', 'base_model.model.model.layers.17.self_attn.o_proj', 'base_model.model.model.layers.17.mlp.down_proj', 'base_model.model.model.layers.19.self_attn.o_proj', 'base_model.model.model.layers.19.mlp.down_proj', 'base_model.model.model.layers.20.self_attn.o_proj', 'base_model.model.model.layers.20.mlp.down_proj', 'base_model.model.model.layers.21.self_attn.o_proj', 'base_model.model.model.layers.21.mlp.down_proj', 'base_model.model.model.layers.22.self_attn.o_proj', 'base_model.model.model.layers.22.mlp.down_proj']\n",
      "target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}\n",
      "o_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "down_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "o_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "down_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "o_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "down_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "o_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "down_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "o_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "down_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "o_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "down_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "o_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "down_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "o_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "down_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "o_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "down_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "o_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "down_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "o_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "down_proj in target_modules {'gate_up_proj', 'down_proj', 'o_proj', 'qkv_proj'}. You may want to append .base_layer\n",
      "['base_model.model.model.layers.11.self_attn.o_proj', 'base_model.model.model.layers.11.mlp.down_proj', 'base_model.model.model.layers.12.self_attn.o_proj', 'base_model.model.model.layers.12.mlp.down_proj', 'base_model.model.model.layers.13.self_attn.o_proj', 'base_model.model.model.layers.13.mlp.down_proj', 'base_model.model.model.layers.14.self_attn.o_proj', 'base_model.model.model.layers.14.mlp.down_proj', 'base_model.model.model.layers.15.self_attn.o_proj', 'base_model.model.model.layers.15.mlp.down_proj', 'base_model.model.model.layers.16.self_attn.o_proj', 'base_model.model.model.layers.16.mlp.down_proj', 'base_model.model.model.layers.17.self_attn.o_proj', 'base_model.model.model.layers.17.mlp.down_proj', 'base_model.model.model.layers.19.self_attn.o_proj', 'base_model.model.model.layers.19.mlp.down_proj', 'base_model.model.model.layers.20.self_attn.o_proj', 'base_model.model.model.layers.20.mlp.down_proj', 'base_model.model.model.layers.21.self_attn.o_proj', 'base_model.model.model.layers.21.mlp.down_proj', 'base_model.model.model.layers.22.self_attn.o_proj', 'base_model.model.model.layers.22.mlp.down_proj']\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1 # 42//8\n",
    "ideal_batch_size = 2\n",
    "gradient_accumulation_steps = ideal_batch_size // batch_size\n",
    "num_train_epochs = num_samples // num_data_samples\n",
    "num_steps = num_samples // ideal_batch_size\n",
    "print(dict(gradient_accumulation_steps=gradient_accumulation_steps, num_train_epochs=num_train_epochs, batch_size=batch_size))\n",
    "\n",
    "# vscode + wandb compat\n",
    "dt = pd.Timestamp.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "# TODO put model and adapter base names?\n",
    "run_name = f\"{nb_name}-{dt}\"\n",
    "\n",
    "training_args = ReprPOConfig2(\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    learning_rate=1e-4, # 5e-7 in the dpo paper? but this method needs much more\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "\n",
    "    # lr_scheduler_type=\"constant\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0,\n",
    "\n",
    "    seed=42,\n",
    "    logging_steps=1,\n",
    "    # save_steps=500,\n",
    "    # save_strategy=\"steps\",\n",
    "    output_dir=f\"./output-dir/{run_name}\",\n",
    "\n",
    "    gradient_checkpointing=use_gradient_checkpointing,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    remove_unused_columns=False,\n",
    "    max_grad_norm=10,\n",
    "\n",
    "    max_prompt_length=max_prompt_length,\n",
    "    max_length=max_length,\n",
    "\n",
    "    report_to=['tensorboard', 'wandb'],\n",
    "    model_adapter_name='ReprPO',\n",
    "    alpha=.3,\n",
    "\n",
    "    run_name=run_name,\n",
    "\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=(num_steps//10),\n",
    "\n",
    ")\n",
    "\n",
    "check_training_args(training_args, model)\n",
    "\n",
    "reprpo_trainer = ReprPOTrainer2(\n",
    "    model=model,\n",
    "    ref_model=None,\n",
    "    args=training_args,\n",
    "    # beta=training_args.beta,\n",
    "    train_dataset=dataset2[\"train\"],\n",
    "    eval_dataset=dataset2[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Transformer does not recognise vscode notebooks\n",
    "reprpo_trainer.callback_handler.remove_callback(ProgressCallback)\n",
    "reprpo_trainer.callback_handler.add_callback(NotebookProgressCallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # QC train dataset\n",
    "# r = reprpo_trainer.train_dataset[0]\n",
    "# print('prompt', tokenizer.decode(r['prompt_input_ids']))\n",
    "# print('-'*80)q\n",
    "# print('chosen',tokenizer.decode(r['chosen_input_ids']))\n",
    "# print('-'*80)\n",
    "# print('rejected',tokenizer.decode(r['rejected_input_ids']))\n",
    "# print('='*80)\n",
    "clear_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwassname\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/wassname/SGIronWolf/projects5/elk/repr-preference-optimization/nbs/wandb/run-20240808_173331-phvb0usy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wassname/repo-dpo/runs/phvb0usy' target=\"_blank\">20_hf_phi_hs_outproj-in-2024-08-08-17-33-26</a></strong> to <a href='https://wandb.ai/wassname/repo-dpo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wassname/repo-dpo' target=\"_blank\">https://wandb.ai/wassname/repo-dpo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wassname/repo-dpo/runs/phvb0usy' target=\"_blank\">https://wandb.ai/wassname/repo-dpo/runs/phvb0usy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rewards/accuracies': '0', 'rewards/margins': '0', 'logps/rejected': '-0.83', 'logps/chosen': '-0.68', 'loss': '1.3', 'rewards/chosen': '0', 'rewards/rejected': '0', 'retain/loss': '1', 'reroute/loss': '1', 'logratios/pi': '0.15', 'logratios/ref': '0.15', 'weighting': '1', 'logits': '0', 'component_rr/loss': '1', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.8', 'rr_cosine': '0.41'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '0', 'logps/rejected': '-1.7', 'logps/chosen': '-1.3', 'loss': '1.3', 'rewards/chosen': '0', 'rewards/rejected': '0', 'retain/loss': '1', 'reroute/loss': '1', 'logratios/pi': '0.39', 'logratios/ref': '0.39', 'weighting': '1', 'logits': '0', 'component_rr/loss': '1', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '1', 'rr_cosine': '0.6'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2026' max='3610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2026/3610 23:21 < 18:16, 1.44 it/s, Epoch 0.56/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Retain/loss</th>\n",
       "      <th>Reroute/loss</th>\n",
       "      <th>Logratios/pi</th>\n",
       "      <th>Logratios/ref</th>\n",
       "      <th>Weighting</th>\n",
       "      <th>Logits</th>\n",
       "      <th>Component Rr/loss</th>\n",
       "      <th>Component Retain/loss</th>\n",
       "      <th>Rr/c</th>\n",
       "      <th>Retain/c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>1.203125</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.485255</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>-1.366879</td>\n",
       "      <td>-1.358120</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.008758</td>\n",
       "      <td>0.006958</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001801</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.296875</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.495979</td>\n",
       "      <td>-0.000413</td>\n",
       "      <td>-1.370890</td>\n",
       "      <td>-1.368063</td>\n",
       "      <td>-0.000679</td>\n",
       "      <td>-0.000266</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.002827</td>\n",
       "      <td>0.006958</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.004131</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.00042', 'logps/rejected': '-2.2', 'logps/chosen': '-1.9', 'loss': '1.3', 'rewards/chosen': '6.2e-05', 'rewards/rejected': '0.00048', 'retain/loss': '1', 'reroute/loss': '1', 'logratios/pi': '0.28', 'logratios/ref': '0.28', 'weighting': '1', 'logits': '-0.0042', 'component_rr/loss': '1', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '1', 'rr_cosine': '0.61'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0014', 'logps/rejected': '-1.9', 'logps/chosen': '-1.5', 'loss': '1.3', 'rewards/chosen': '-7.4e-05', 'rewards/rejected': '-0.0015', 'retain/loss': '1', 'reroute/loss': '1', 'logratios/pi': '0.44', 'logratios/ref': '0.42', 'weighting': '1', 'logits': '0.014', 'component_rr/loss': '1', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '1', 'rr_cosine': '0.79'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0013', 'logps/rejected': '-1', 'logps/chosen': '-0.94', 'loss': '1.3', 'rewards/chosen': '0.00086', 'rewards/rejected': '-0.00045', 'retain/loss': '1', 'reroute/loss': '1', 'logratios/pi': '0.088', 'logratios/ref': '0.075', 'weighting': '1', 'logits': '0.013', 'component_rr/loss': '1', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.84', 'rr_cosine': '0.4'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.00086', 'logps/rejected': '-1.8', 'logps/chosen': '-1.2', 'loss': '1.3', 'rewards/chosen': '0.00052', 'rewards/rejected': '-0.00034', 'retain/loss': '1', 'reroute/loss': '1', 'logratios/pi': '0.67', 'logratios/ref': '0.66', 'weighting': '1', 'logits': '0.0086', 'component_rr/loss': '1', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '1', 'rr_cosine': '0.61'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.00032', 'logps/rejected': '-1.5', 'logps/chosen': '-1.4', 'loss': '1.3', 'rewards/chosen': '1.5e-06', 'rewards/rejected': '-0.00032', 'retain/loss': '1', 'reroute/loss': '0.99', 'logratios/pi': '0.033', 'logratios/ref': '0.03', 'weighting': '1', 'logits': '0.0032', 'component_rr/loss': '0.99', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '1', 'rr_cosine': '0.62'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.00061', 'logps/rejected': '-1.4', 'logps/chosen': '-1.5', 'loss': '1.3', 'rewards/chosen': '-0.00047', 'rewards/rejected': '0.00014', 'retain/loss': '1', 'reroute/loss': '0.99', 'logratios/pi': '-0.055', 'logratios/ref': '-0.049', 'weighting': '1', 'logits': '-0.0061', 'component_rr/loss': '0.99', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '1', 'rr_cosine': '0.63'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.00017', 'logps/rejected': '-2', 'logps/chosen': '-1.8', 'loss': '1.3', 'rewards/chosen': '0.00026', 'rewards/rejected': '9.4e-05', 'retain/loss': '1', 'reroute/loss': '0.99', 'logratios/pi': '0.17', 'logratios/ref': '0.17', 'weighting': '1', 'logits': '0.0017', 'component_rr/loss': '0.99', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '1', 'rr_cosine': '0.61'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0009', 'logps/rejected': '-1', 'logps/chosen': '-0.73', 'loss': '1.3', 'rewards/chosen': '-0.00029', 'rewards/rejected': '0.00061', 'retain/loss': '1', 'reroute/loss': '1', 'logratios/pi': '0.32', 'logratios/ref': '0.33', 'weighting': '1', 'logits': '-0.009', 'component_rr/loss': '1', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.88', 'rr_cosine': '0.49'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.00043', 'logps/rejected': '-1.7', 'logps/chosen': '-1.5', 'loss': '1.3', 'rewards/chosen': '-0.00064', 'rewards/rejected': '-0.00021', 'retain/loss': '1', 'reroute/loss': '0.99', 'logratios/pi': '0.22', 'logratios/ref': '0.23', 'weighting': '1', 'logits': '-0.0043', 'component_rr/loss': '0.99', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '1', 'rr_cosine': '0.66'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.00069', 'logps/rejected': '-0.96', 'logps/chosen': '-0.77', 'loss': '1.3', 'rewards/chosen': '0.00083', 'rewards/rejected': '0.0015', 'retain/loss': '1', 'reroute/loss': '0.99', 'logratios/pi': '0.19', 'logratios/ref': '0.2', 'weighting': '1', 'logits': '-0.0069', 'component_rr/loss': '0.99', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.86', 'rr_cosine': '0.47'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0015', 'logps/rejected': '-2', 'logps/chosen': '-2', 'loss': '1.3', 'rewards/chosen': '-0.00013', 'rewards/rejected': '0.0013', 'retain/loss': '1', 'reroute/loss': '0.99', 'logratios/pi': '0.074', 'logratios/ref': '0.089', 'weighting': '1', 'logits': '-0.015', 'component_rr/loss': '0.99', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '1', 'rr_cosine': '0.65'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.00044', 'logps/rejected': '-0.73', 'logps/chosen': '-0.89', 'loss': '1.3', 'rewards/chosen': '-5.6e-05', 'rewards/rejected': '-0.00049', 'retain/loss': '1', 'reroute/loss': '1', 'logratios/pi': '-0.16', 'logratios/ref': '-0.17', 'weighting': '1', 'logits': '0.0044', 'component_rr/loss': '1', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '1', 'rr_cosine': '0.86'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0021', 'logps/rejected': '-0.61', 'logps/chosen': '-0.68', 'loss': '1.2', 'rewards/chosen': '-0.001', 'rewards/rejected': '0.0011', 'retain/loss': '1', 'reroute/loss': '0.95', 'logratios/pi': '-0.07', 'logratios/ref': '-0.05', 'weighting': '1', 'logits': '-0.021', 'component_rr/loss': '0.95', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.83', 'rr_cosine': '0.51'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.00038', 'logps/rejected': '-0.94', 'logps/chosen': '-1.8', 'loss': '1.3', 'rewards/chosen': '-0.0012', 'rewards/rejected': '-0.00087', 'retain/loss': '1', 'reroute/loss': '0.97', 'logratios/pi': '-0.89', 'logratios/ref': '-0.89', 'weighting': '1', 'logits': '-0.0038', 'component_rr/loss': '0.97', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.62', 'rr_cosine': '0.16'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.00077', 'logps/rejected': '-1.9', 'logps/chosen': '-1.6', 'loss': '1.2', 'rewards/chosen': '-0.00019', 'rewards/rejected': '-0.00096', 'retain/loss': '1', 'reroute/loss': '0.94', 'logratios/pi': '0.31', 'logratios/ref': '0.3', 'weighting': '1', 'logits': '0.0077', 'component_rr/loss': '0.94', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '1', 'rr_cosine': '0.8'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0013', 'logps/rejected': '-1.2', 'logps/chosen': '-0.57', 'loss': '1.3', 'rewards/chosen': '0.00035', 'rewards/rejected': '-0.00098', 'retain/loss': '1', 'reroute/loss': '0.97', 'logratios/pi': '0.63', 'logratios/ref': '0.62', 'weighting': '1', 'logits': '0.013', 'component_rr/loss': '0.97', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.85', 'rr_cosine': '0.45'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0024', 'logps/rejected': '-1.8', 'logps/chosen': '-1.5', 'loss': '1.3', 'rewards/chosen': '0.00042', 'rewards/rejected': '0.0028', 'retain/loss': '1', 'reroute/loss': '1', 'logratios/pi': '0.33', 'logratios/ref': '0.36', 'weighting': '1', 'logits': '-0.024', 'component_rr/loss': '1', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.93', 'rr_cosine': '0.62'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.00091', 'logps/rejected': '-1.9', 'logps/chosen': '-0.84', 'loss': '1.2', 'rewards/chosen': '0.00075', 'rewards/rejected': '-0.00015', 'retain/loss': '1', 'reroute/loss': '0.94', 'logratios/pi': '1.1', 'logratios/ref': '1.1', 'weighting': '1', 'logits': '0.0091', 'component_rr/loss': '0.94', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.88', 'rr_cosine': '0.46'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0017', 'logps/rejected': '-1.4', 'logps/chosen': '-1.2', 'loss': '1.2', 'rewards/chosen': '0.0015', 'rewards/rejected': '-0.00014', 'retain/loss': '1', 'reroute/loss': '0.94', 'logratios/pi': '0.16', 'logratios/ref': '0.14', 'weighting': '1', 'logits': '0.017', 'component_rr/loss': '0.94', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.81', 'rr_cosine': '0.47'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-7.5e-05', 'logps/rejected': '-1.5', 'logps/chosen': '-1.4', 'loss': '1.3', 'rewards/chosen': '0.00073', 'rewards/rejected': '0.00081', 'retain/loss': '1', 'reroute/loss': '0.96', 'logratios/pi': '0.058', 'logratios/ref': '0.058', 'weighting': '1', 'logits': '-0.00075', 'component_rr/loss': '0.96', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '1', 'rr_cosine': '0.58'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0046', 'logps/rejected': '-1.2', 'logps/chosen': '-1.2', 'loss': '1.2', 'rewards/chosen': '0.0022', 'rewards/rejected': '-0.0024', 'retain/loss': '1', 'reroute/loss': '0.93', 'logratios/pi': '0.029', 'logratios/ref': '-0.017', 'weighting': '1', 'logits': '0.046', 'component_rr/loss': '0.93', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '1', 'rr_cosine': '0.6'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-7.2e-05', 'logps/rejected': '-2', 'logps/chosen': '-1.6', 'loss': '1.3', 'rewards/chosen': '0.00061', 'rewards/rejected': '0.00068', 'retain/loss': '1', 'reroute/loss': '1', 'logratios/pi': '0.37', 'logratios/ref': '0.37', 'weighting': '1', 'logits': '-0.00072', 'component_rr/loss': '1', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.75', 'rr_cosine': '0.52'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0027', 'logps/rejected': '-1.3', 'logps/chosen': '-0.94', 'loss': '1.3', 'rewards/chosen': '-0.0018', 'rewards/rejected': '0.00084', 'retain/loss': '1', 'reroute/loss': '0.96', 'logratios/pi': '0.34', 'logratios/ref': '0.37', 'weighting': '1', 'logits': '-0.027', 'component_rr/loss': '0.96', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.84', 'rr_cosine': '0.56'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0011', 'logps/rejected': '-1.3', 'logps/chosen': '-1.3', 'loss': '1.2', 'rewards/chosen': '0.00058', 'rewards/rejected': '-0.00047', 'retain/loss': '1', 'reroute/loss': '0.95', 'logratios/pi': '-0.069', 'logratios/ref': '-0.08', 'weighting': '1', 'logits': '0.011', 'component_rr/loss': '0.95', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '1', 'rr_cosine': '0.81'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.00087', 'logps/rejected': '-0.97', 'logps/chosen': '-0.91', 'loss': '1.3', 'rewards/chosen': '-0.00034', 'rewards/rejected': '-0.0012', 'retain/loss': '1', 'reroute/loss': '1', 'logratios/pi': '0.059', 'logratios/ref': '0.05', 'weighting': '1', 'logits': '0.0087', 'component_rr/loss': '1', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.83', 'rr_cosine': '0.48'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0025', 'logps/rejected': '-0.86', 'logps/chosen': '-1.2', 'loss': '1.3', 'rewards/chosen': '0.00089', 'rewards/rejected': '-0.0016', 'retain/loss': '1', 'reroute/loss': '0.98', 'logratios/pi': '-0.3', 'logratios/ref': '-0.32', 'weighting': '1', 'logits': '0.025', 'component_rr/loss': '0.98', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.88', 'rr_cosine': '0.52'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0045', 'logps/rejected': '-1', 'logps/chosen': '-2.2', 'loss': '1.2', 'rewards/chosen': '0.00092', 'rewards/rejected': '-0.0036', 'retain/loss': '1', 'reroute/loss': '0.9', 'logratios/pi': '-1.2', 'logratios/ref': '-1.3', 'weighting': '1', 'logits': '0.045', 'component_rr/loss': '0.9', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.87', 'rr_cosine': '0.49'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0047', 'logps/rejected': '-2', 'logps/chosen': '-1.2', 'loss': '1.1', 'rewards/chosen': '0.0038', 'rewards/rejected': '-0.00099', 'retain/loss': '1', 'reroute/loss': '0.85', 'logratios/pi': '0.85', 'logratios/ref': '0.8', 'weighting': '1', 'logits': '0.047', 'component_rr/loss': '0.85', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '1', 'rr_cosine': '0.6'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '8.3e-05', 'logps/rejected': '-1.1', 'logps/chosen': '-1.2', 'loss': '1.2', 'rewards/chosen': '0.00062', 'rewards/rejected': '0.00054', 'retain/loss': '1', 'reroute/loss': '0.94', 'logratios/pi': '-0.099', 'logratios/ref': '-0.1', 'weighting': '1', 'logits': '0.00083', 'component_rr/loss': '0.94', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.8', 'rr_cosine': '0.42'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0043', 'logps/rejected': '-0.99', 'logps/chosen': '-0.82', 'loss': '1.2', 'rewards/chosen': '-0.00081', 'rewards/rejected': '0.0035', 'retain/loss': '1', 'reroute/loss': '0.86', 'logratios/pi': '0.17', 'logratios/ref': '0.21', 'weighting': '1', 'logits': '-0.043', 'component_rr/loss': '0.86', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.95', 'rr_cosine': '0.56'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.00058', 'logps/rejected': '-1.5', 'logps/chosen': '-1.3', 'loss': '1.2', 'rewards/chosen': '3.2e-05', 'rewards/rejected': '-0.00055', 'retain/loss': '1', 'reroute/loss': '0.93', 'logratios/pi': '0.28', 'logratios/ref': '0.27', 'weighting': '1', 'logits': '0.0058', 'component_rr/loss': '0.93', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '1', 'rr_cosine': '0.74'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0015', 'logps/rejected': '-0.91', 'logps/chosen': '-0.86', 'loss': '1.2', 'rewards/chosen': '0.00087', 'rewards/rejected': '-0.00058', 'retain/loss': '1', 'reroute/loss': '0.86', 'logratios/pi': '0.05', 'logratios/ref': '0.035', 'weighting': '1', 'logits': '0.015', 'component_rr/loss': '0.86', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.82', 'rr_cosine': '0.42'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0073', 'logps/rejected': '-1.1', 'logps/chosen': '-1.1', 'loss': '1.3', 'rewards/chosen': '-0.0018', 'rewards/rejected': '0.0055', 'retain/loss': '1.1', 'reroute/loss': '0.98', 'logratios/pi': '-0.036', 'logratios/ref': '0.037', 'weighting': '1', 'logits': '-0.073', 'component_rr/loss': '0.98', 'component_retain/loss': '0.33', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.95', 'rr_cosine': '0.61'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0071', 'logps/rejected': '-1.3', 'logps/chosen': '-1.4', 'loss': '1.2', 'rewards/chosen': '-0.0026', 'rewards/rejected': '0.0045', 'retain/loss': '1', 'reroute/loss': '0.93', 'logratios/pi': '-0.17', 'logratios/ref': '-0.1', 'weighting': '1', 'logits': '-0.071', 'component_rr/loss': '0.93', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '1', 'rr_cosine': '0.57'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.00018', 'logps/rejected': '-1.2', 'logps/chosen': '-1.7', 'loss': '1.2', 'rewards/chosen': '-0.00054', 'rewards/rejected': '-0.00072', 'retain/loss': '1', 'reroute/loss': '0.91', 'logratios/pi': '-0.56', 'logratios/ref': '-0.56', 'weighting': '1', 'logits': '0.0018', 'component_rr/loss': '0.91', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '1', 'rr_cosine': '0.57'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0018', 'logps/rejected': '-1.1', 'logps/chosen': '-1.1', 'loss': '1.2', 'rewards/chosen': '0.00028', 'rewards/rejected': '-0.0015', 'retain/loss': '1', 'reroute/loss': '0.94', 'logratios/pi': '0.023', 'logratios/ref': '0.0051', 'weighting': '1', 'logits': '0.018', 'component_rr/loss': '0.94', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.8', 'rr_cosine': '0.41'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0022', 'logps/rejected': '-1.2', 'logps/chosen': '-1.6', 'loss': '1.2', 'rewards/chosen': '0.0049', 'rewards/rejected': '0.0026', 'retain/loss': '1', 'reroute/loss': '0.87', 'logratios/pi': '-0.36', 'logratios/ref': '-0.38', 'weighting': '1', 'logits': '0.022', 'component_rr/loss': '0.87', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.89', 'rr_cosine': '0.53'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0011', 'logps/rejected': '-0.73', 'logps/chosen': '-0.56', 'loss': '1.2', 'rewards/chosen': '0.0029', 'rewards/rejected': '0.0018', 'retain/loss': '1', 'reroute/loss': '0.93', 'logratios/pi': '0.17', 'logratios/ref': '0.16', 'weighting': '1', 'logits': '0.011', 'component_rr/loss': '0.93', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.81', 'rr_cosine': '0.45'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.003', 'logps/rejected': '-1.2', 'logps/chosen': '-0.42', 'loss': '1.6', 'rewards/chosen': '0.00057', 'rewards/rejected': '-0.0025', 'retain/loss': '1.5', 'reroute/loss': '1.1', 'logratios/pi': '0.75', 'logratios/ref': '0.72', 'weighting': '1', 'logits': '0.03', 'component_rr/loss': '1.1', 'component_retain/loss': '0.45', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '1', 'rr_cosine': '0.58'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0024', 'logps/rejected': '-0.89', 'logps/chosen': '-0.86', 'loss': '1.2', 'rewards/chosen': '0.001', 'rewards/rejected': '-0.0014', 'retain/loss': '1.1', 'reroute/loss': '0.93', 'logratios/pi': '0.027', 'logratios/ref': '0.0024', 'weighting': '1', 'logits': '0.024', 'component_rr/loss': '0.93', 'component_retain/loss': '0.32', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.75'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0017', 'logps/rejected': '-0.96', 'logps/chosen': '-1.3', 'loss': '1.3', 'rewards/chosen': '-5.3e-05', 'rewards/rejected': '-0.0018', 'retain/loss': '1.1', 'reroute/loss': '0.95', 'logratios/pi': '-0.37', 'logratios/ref': '-0.38', 'weighting': '1', 'logits': '0.017', 'component_rr/loss': '0.95', 'component_retain/loss': '0.33', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.73'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.008', 'logps/rejected': '-2.6', 'logps/chosen': '-1.3', 'loss': '1.3', 'rewards/chosen': '-0.012', 'rewards/rejected': '-0.0037', 'retain/loss': '1.1', 'reroute/loss': '0.96', 'logratios/pi': '1.3', 'logratios/ref': '1.4', 'weighting': '1', 'logits': '-0.08', 'component_rr/loss': '0.96', 'component_retain/loss': '0.32', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.97', 'rr_cosine': '0.59'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0011', 'logps/rejected': '-1.7', 'logps/chosen': '-1.6', 'loss': '1.2', 'rewards/chosen': '0.00018', 'rewards/rejected': '0.0013', 'retain/loss': '1', 'reroute/loss': '0.92', 'logratios/pi': '0.12', 'logratios/ref': '0.13', 'weighting': '1', 'logits': '-0.011', 'component_rr/loss': '0.92', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '1', 'rr_cosine': '0.61'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0024', 'logps/rejected': '-0.79', 'logps/chosen': '-0.8', 'loss': '1.2', 'rewards/chosen': '0.0011', 'rewards/rejected': '-0.0014', 'retain/loss': '1', 'reroute/loss': '0.84', 'logratios/pi': '-0.009', 'logratios/ref': '-0.033', 'weighting': '1', 'logits': '0.024', 'component_rr/loss': '0.84', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '1', 'rr_cosine': '0.61'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0051', 'logps/rejected': '-0.98', 'logps/chosen': '-0.67', 'loss': '1.2', 'rewards/chosen': '-0.0023', 'rewards/rejected': '0.0028', 'retain/loss': '1', 'reroute/loss': '0.86', 'logratios/pi': '0.31', 'logratios/ref': '0.36', 'weighting': '1', 'logits': '-0.051', 'component_rr/loss': '0.86', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.88', 'rr_cosine': '0.51'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.013', 'logps/rejected': '-1.1', 'logps/chosen': '-1.4', 'loss': '1.1', 'rewards/chosen': '0.00063', 'rewards/rejected': '-0.012', 'retain/loss': '1.1', 'reroute/loss': '0.74', 'logratios/pi': '-0.28', 'logratios/ref': '-0.4', 'weighting': '1', 'logits': '0.13', 'component_rr/loss': '0.74', 'component_retain/loss': '0.32', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.58'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-6.1e-06', 'logps/rejected': '-1.2', 'logps/chosen': '-1.2', 'loss': '1.2', 'rewards/chosen': '0.0037', 'rewards/rejected': '0.0037', 'retain/loss': '1', 'reroute/loss': '0.88', 'logratios/pi': '-0.083', 'logratios/ref': '-0.083', 'weighting': '1', 'logits': '-6.1e-05', 'component_rr/loss': '0.88', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.58'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0016', 'logps/rejected': '-1.3', 'logps/chosen': '-1.3', 'loss': '1.2', 'rewards/chosen': '-0.0038', 'rewards/rejected': '-0.0022', 'retain/loss': '1.1', 'reroute/loss': '0.92', 'logratios/pi': '-0.053', 'logratios/ref': '-0.036', 'weighting': '1', 'logits': '-0.016', 'component_rr/loss': '0.92', 'component_retain/loss': '0.33', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '1', 'rr_cosine': '0.66'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/repr-preference-optimization/.venv/lib/python3.9/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0032', 'logps/rejected': '-0.73', 'logps/chosen': '-1.1', 'loss': '1.2', 'rewards/chosen': '0.0022', 'rewards/rejected': '-0.001', 'retain/loss': '1.1', 'reroute/loss': '0.91', 'logratios/pi': '-0.4', 'logratios/ref': '-0.43', 'weighting': '1', 'logits': '0.032', 'component_rr/loss': '0.91', 'component_retain/loss': '0.32', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '1', 'rr_cosine': '0.71'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0039', 'logps/rejected': '-0.63', 'logps/chosen': '-0.95', 'loss': '1.4', 'rewards/chosen': '-0.0012', 'rewards/rejected': '-0.0052', 'retain/loss': '1.1', 'reroute/loss': '1.1', 'logratios/pi': '-0.32', 'logratios/ref': '-0.36', 'weighting': '1', 'logits': '0.039', 'component_rr/loss': '1.1', 'component_retain/loss': '0.33', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.89', 'rr_cosine': '0.6'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0026', 'logps/rejected': '-1.5', 'logps/chosen': '-2.6', 'loss': '1.2', 'rewards/chosen': '0.0014', 'rewards/rejected': '-0.0012', 'retain/loss': '1', 'reroute/loss': '0.89', 'logratios/pi': '-1.1', 'logratios/ref': '-1.1', 'weighting': '1', 'logits': '0.026', 'component_rr/loss': '0.89', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '1', 'rr_cosine': '0.59'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0012', 'logps/rejected': '-0.95', 'logps/chosen': '-0.6', 'loss': '1.3', 'rewards/chosen': '0.0012', 'rewards/rejected': '0.0023', 'retain/loss': '1', 'reroute/loss': '0.95', 'logratios/pi': '0.36', 'logratios/ref': '0.37', 'weighting': '1', 'logits': '-0.012', 'component_rr/loss': '0.95', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.77', 'rr_cosine': '0.42'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0093', 'logps/rejected': '-2', 'logps/chosen': '-2.6', 'loss': '1.2', 'rewards/chosen': '0.0044', 'rewards/rejected': '-0.0049', 'retain/loss': '1', 'reroute/loss': '0.88', 'logratios/pi': '-0.6', 'logratios/ref': '-0.7', 'weighting': '1', 'logits': '0.093', 'component_rr/loss': '0.88', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '1', 'rr_cosine': '0.62'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0027', 'logps/rejected': '-2.1', 'logps/chosen': '-1.5', 'loss': '1.2', 'rewards/chosen': '0.0014', 'rewards/rejected': '0.004', 'retain/loss': '1', 'reroute/loss': '0.91', 'logratios/pi': '0.6', 'logratios/ref': '0.63', 'weighting': '1', 'logits': '-0.027', 'component_rr/loss': '0.91', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.8', 'rr_cosine': '0.43'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0016', 'logps/rejected': '-1.3', 'logps/chosen': '-0.59', 'loss': '1.2', 'rewards/chosen': '0.002', 'rewards/rejected': '0.00044', 'retain/loss': '1', 'reroute/loss': '0.88', 'logratios/pi': '0.66', 'logratios/ref': '0.65', 'weighting': '1', 'logits': '0.016', 'component_rr/loss': '0.88', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.79', 'rr_cosine': '0.36'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.011', 'logps/rejected': '-2.1', 'logps/chosen': '-2.7', 'loss': '1.4', 'rewards/chosen': '-0.012', 'rewards/rejected': '-0.0013', 'retain/loss': '1.1', 'reroute/loss': '1', 'logratios/pi': '-0.57', 'logratios/ref': '-0.47', 'weighting': '1', 'logits': '-0.11', 'component_rr/loss': '1', 'component_retain/loss': '0.33', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.83', 'rr_cosine': '0.55'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0003', 'logps/rejected': '-0.92', 'logps/chosen': '-0.9', 'loss': '1.3', 'rewards/chosen': '0.0031', 'rewards/rejected': '0.0028', 'retain/loss': '1.1', 'reroute/loss': '0.95', 'logratios/pi': '0.027', 'logratios/ref': '0.024', 'weighting': '1', 'logits': '0.003', 'component_rr/loss': '0.95', 'component_retain/loss': '0.34', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '1', 'rr_cosine': '0.73'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0057', 'logps/rejected': '-1.4', 'logps/chosen': '-1.3', 'loss': '1.1', 'rewards/chosen': '0.0023', 'rewards/rejected': '-0.0034', 'retain/loss': '1', 'reroute/loss': '0.75', 'logratios/pi': '0.18', 'logratios/ref': '0.12', 'weighting': '1', 'logits': '0.057', 'component_rr/loss': '0.75', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.79', 'rr_cosine': '0.33'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0015', 'logps/rejected': '-0.9', 'logps/chosen': '-1.1', 'loss': '1.4', 'rewards/chosen': '0.0012', 'rewards/rejected': '0.0027', 'retain/loss': '1.2', 'reroute/loss': '1.1', 'logratios/pi': '-0.15', 'logratios/ref': '-0.14', 'weighting': '1', 'logits': '-0.015', 'component_rr/loss': '1.1', 'component_retain/loss': '0.37', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '1', 'rr_cosine': '0.64'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0061', 'logps/rejected': '-1.7', 'logps/chosen': '-1.1', 'loss': '1.2', 'rewards/chosen': '0.0056', 'rewards/rejected': '-0.00047', 'retain/loss': '1.1', 'reroute/loss': '0.93', 'logratios/pi': '0.62', 'logratios/ref': '0.56', 'weighting': '1', 'logits': '0.061', 'component_rr/loss': '0.93', 'component_retain/loss': '0.32', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.64'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0025', 'logps/rejected': '-0.5', 'logps/chosen': '-0.65', 'loss': '1.2', 'rewards/chosen': '0.00093', 'rewards/rejected': '-0.0016', 'retain/loss': '1', 'reroute/loss': '0.91', 'logratios/pi': '-0.16', 'logratios/ref': '-0.18', 'weighting': '1', 'logits': '0.025', 'component_rr/loss': '0.91', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.88', 'rr_cosine': '0.48'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0014', 'logps/rejected': '-0.99', 'logps/chosen': '-0.75', 'loss': '1.1', 'rewards/chosen': '-0.0013', 'rewards/rejected': '0.00011', 'retain/loss': '1.1', 'reroute/loss': '0.77', 'logratios/pi': '0.25', 'logratios/ref': '0.26', 'weighting': '1', 'logits': '-0.014', 'component_rr/loss': '0.77', 'component_retain/loss': '0.33', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.93', 'rr_cosine': '0.45'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0065', 'logps/rejected': '-1.3', 'logps/chosen': '-0.81', 'loss': '1.3', 'rewards/chosen': '0.0029', 'rewards/rejected': '-0.0035', 'retain/loss': '1.1', 'reroute/loss': '0.99', 'logratios/pi': '0.53', 'logratios/ref': '0.46', 'weighting': '1', 'logits': '0.065', 'component_rr/loss': '0.99', 'component_retain/loss': '0.33', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.79', 'rr_cosine': '0.45'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0016', 'logps/rejected': '-2.4', 'logps/chosen': '-1.9', 'loss': '1.1', 'rewards/chosen': '-0.0022', 'rewards/rejected': '-0.0038', 'retain/loss': '1', 'reroute/loss': '0.82', 'logratios/pi': '0.59', 'logratios/ref': '0.57', 'weighting': '1', 'logits': '0.016', 'component_rr/loss': '0.82', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.79', 'rr_cosine': '0.4'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0041', 'logps/rejected': '-0.69', 'logps/chosen': '-0.67', 'loss': '1.2', 'rewards/chosen': '0.0027', 'rewards/rejected': '-0.0014', 'retain/loss': '1.1', 'reroute/loss': '0.86', 'logratios/pi': '0.023', 'logratios/ref': '-0.019', 'weighting': '1', 'logits': '0.041', 'component_rr/loss': '0.86', 'component_retain/loss': '0.32', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.84', 'rr_cosine': '0.53'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0024', 'logps/rejected': '-0.74', 'logps/chosen': '-0.73', 'loss': '1.2', 'rewards/chosen': '-0.00059', 'rewards/rejected': '0.0019', 'retain/loss': '1', 'reroute/loss': '0.92', 'logratios/pi': '0.01', 'logratios/ref': '0.034', 'weighting': '1', 'logits': '-0.024', 'component_rr/loss': '0.92', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.88', 'rr_cosine': '0.48'}\n",
      "**Question**\n",
      "```\n",
      "<|system|> tldr, markdown<|end|><|user|> Q1: (45 words): Which Science Fiction Society would you prefer to live in and why? Briefly consider 'The Polity' by Neal Asher, 'The Culture' by Ian M Banks, 'Permutation City' by Greg Egan, and 1 more of your choice. Start with your conclusion, then give your reasoning.<|end|><|assistant|>\n",
      "```\n",
      "--------------------------------------------------------------------------------\n",
      "**Adapter:`ReprPO` generation**`\n",
      "`I would prefer to live in 'The Culture' by Ian M. Banks. The Culture represents a utopian society where technology and ethics are harmoniously integrated, offering a vision of a future where humanity has transcended its current limitations. The society's emphasis on collective well-`\n",
      "--------------------------------------------------------------------------------\n",
      "**Adapter:`None` generation**`\n",
      "`I would prefer to live in 'The Culture' by Ian M. Banks. The Culture represents a post-scarcity society where technology and advanced AI have eradicated poverty, disease, and conflict. This utopian vision offers a harmonious and intellectually stimulating environment, fostering`\n",
      "--------------------------------------------------------------------------------\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0082', 'logps/rejected': '-1.6', 'logps/chosen': '-1.7', 'loss': '1.3', 'rewards/chosen': '-0.00086', 'rewards/rejected': '0.0073', 'retain/loss': '1', 'reroute/loss': '0.97', 'logratios/pi': '-0.058', 'logratios/ref': '0.024', 'weighting': '1', 'logits': '-0.082', 'component_rr/loss': '0.97', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.61'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0052', 'logps/rejected': '-1.3', 'logps/chosen': '-1.6', 'loss': '1.2', 'rewards/chosen': '-0.0087', 'rewards/rejected': '-0.0035', 'retain/loss': '1.1', 'reroute/loss': '0.87', 'logratios/pi': '-0.32', 'logratios/ref': '-0.26', 'weighting': '1', 'logits': '-0.052', 'component_rr/loss': '0.87', 'component_retain/loss': '0.32', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.61'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0027', 'logps/rejected': '-0.78', 'logps/chosen': '-1.6', 'loss': '1.4', 'rewards/chosen': '-0.0082', 'rewards/rejected': '-0.0055', 'retain/loss': '1.2', 'reroute/loss': '1.1', 'logratios/pi': '-0.82', 'logratios/ref': '-0.8', 'weighting': '1', 'logits': '-0.027', 'component_rr/loss': '1.1', 'component_retain/loss': '0.35', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.68'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0047', 'logps/rejected': '-0.75', 'logps/chosen': '-0.63', 'loss': '1.2', 'rewards/chosen': '-0.0031', 'rewards/rejected': '0.0016', 'retain/loss': '1', 'reroute/loss': '0.94', 'logratios/pi': '0.13', 'logratios/ref': '0.17', 'weighting': '1', 'logits': '-0.047', 'component_rr/loss': '0.94', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.84', 'rr_cosine': '0.57'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0038', 'logps/rejected': '-1.9', 'logps/chosen': '-2.1', 'loss': '1.4', 'rewards/chosen': '0.004', 'rewards/rejected': '0.0002', 'retain/loss': '1.2', 'reroute/loss': '1', 'logratios/pi': '-0.11', 'logratios/ref': '-0.15', 'weighting': '1', 'logits': '0.038', 'component_rr/loss': '1', 'component_retain/loss': '0.37', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.79'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.00079', 'logps/rejected': '-1.8', 'logps/chosen': '-1.4', 'loss': '1.3', 'rewards/chosen': '0.0007', 'rewards/rejected': '0.0015', 'retain/loss': '1.1', 'reroute/loss': '0.99', 'logratios/pi': '0.37', 'logratios/ref': '0.38', 'weighting': '1', 'logits': '-0.0079', 'component_rr/loss': '0.99', 'component_retain/loss': '0.34', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.69'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0015', 'logps/rejected': '-1.6', 'logps/chosen': '-1.6', 'loss': '1.3', 'rewards/chosen': '-0.001', 'rewards/rejected': '0.00041', 'retain/loss': '1', 'reroute/loss': '1', 'logratios/pi': '-0.014', 'logratios/ref': '0.00076', 'weighting': '1', 'logits': '-0.015', 'component_rr/loss': '1', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '1', 'rr_cosine': '0.65'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0039', 'logps/rejected': '-1', 'logps/chosen': '-1.3', 'loss': '1.2', 'rewards/chosen': '0.0035', 'rewards/rejected': '-0.00033', 'retain/loss': '1', 'reroute/loss': '0.84', 'logratios/pi': '-0.26', 'logratios/ref': '-0.3', 'weighting': '1', 'logits': '0.039', 'component_rr/loss': '0.84', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.86', 'rr_cosine': '0.55'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.011', 'logps/rejected': '-0.79', 'logps/chosen': '-1.9', 'loss': '1.2', 'rewards/chosen': '0.0085', 'rewards/rejected': '-0.0024', 'retain/loss': '1.1', 'reroute/loss': '0.87', 'logratios/pi': '-1.1', 'logratios/ref': '-1.2', 'weighting': '1', 'logits': '0.11', 'component_rr/loss': '0.87', 'component_retain/loss': '0.32', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.93', 'rr_cosine': '0.58'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0025', 'logps/rejected': '-1.2', 'logps/chosen': '-1.8', 'loss': '1.3', 'rewards/chosen': '0.00078', 'rewards/rejected': '-0.0018', 'retain/loss': '1.1', 'reroute/loss': '0.92', 'logratios/pi': '-0.6', 'logratios/ref': '-0.63', 'weighting': '1', 'logits': '0.025', 'component_rr/loss': '0.92', 'component_retain/loss': '0.34', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.98', 'rr_cosine': '0.62'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0022', 'logps/rejected': '-1.1', 'logps/chosen': '-0.87', 'loss': '1.2', 'rewards/chosen': '-0.0014', 'rewards/rejected': '0.00077', 'retain/loss': '1', 'reroute/loss': '0.95', 'logratios/pi': '0.2', 'logratios/ref': '0.22', 'weighting': '1', 'logits': '-0.022', 'component_rr/loss': '0.95', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.76', 'rr_cosine': '0.34'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.014', 'logps/rejected': '-1.7', 'logps/chosen': '-4.7', 'loss': '2.3', 'rewards/chosen': '0.0088', 'rewards/rejected': '-0.0054', 'retain/loss': '1.7', 'reroute/loss': '1.8', 'logratios/pi': '-3', 'logratios/ref': '-3.1', 'weighting': '1', 'logits': '0.14', 'component_rr/loss': '1.8', 'component_retain/loss': '0.52', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.76', 'rr_cosine': '0.5'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-2.2e-05', 'logps/rejected': '-1.9', 'logps/chosen': '-1.2', 'loss': '1.2', 'rewards/chosen': '0.0015', 'rewards/rejected': '0.0015', 'retain/loss': '1.1', 'reroute/loss': '0.93', 'logratios/pi': '0.63', 'logratios/ref': '0.63', 'weighting': '1', 'logits': '-0.00022', 'component_rr/loss': '0.93', 'component_retain/loss': '0.32', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.63'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0037', 'logps/rejected': '-1.4', 'logps/chosen': '-2.1', 'loss': '1.2', 'rewards/chosen': '0.0027', 'rewards/rejected': '-0.001', 'retain/loss': '1', 'reroute/loss': '0.93', 'logratios/pi': '-0.66', 'logratios/ref': '-0.7', 'weighting': '1', 'logits': '0.037', 'component_rr/loss': '0.93', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '1', 'rr_cosine': '0.63'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0032', 'logps/rejected': '-0.73', 'logps/chosen': '-0.61', 'loss': '1.4', 'rewards/chosen': '2.5e-05', 'rewards/rejected': '-0.0032', 'retain/loss': '1.1', 'reroute/loss': '1', 'logratios/pi': '0.11', 'logratios/ref': '0.082', 'weighting': '1', 'logits': '0.032', 'component_rr/loss': '1', 'component_retain/loss': '0.32', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.92', 'rr_cosine': '0.6'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0039', 'logps/rejected': '-1.1', 'logps/chosen': '-0.99', 'loss': '1.2', 'rewards/chosen': '0.0019', 'rewards/rejected': '-0.002', 'retain/loss': '1', 'reroute/loss': '0.84', 'logratios/pi': '0.15', 'logratios/ref': '0.11', 'weighting': '1', 'logits': '0.039', 'component_rr/loss': '0.84', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.8', 'rr_cosine': '0.49'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0086', 'logps/rejected': '-1.4', 'logps/chosen': '-0.78', 'loss': '1.3', 'rewards/chosen': '0.0046', 'rewards/rejected': '-0.004', 'retain/loss': '1.1', 'reroute/loss': '0.97', 'logratios/pi': '0.62', 'logratios/ref': '0.53', 'weighting': '1', 'logits': '0.086', 'component_rr/loss': '0.97', 'component_retain/loss': '0.33', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.89', 'rr_cosine': '0.46'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0091', 'logps/rejected': '-1.3', 'logps/chosen': '-1.2', 'loss': '1.3', 'rewards/chosen': '-0.0089', 'rewards/rejected': '0.0002', 'retain/loss': '1.2', 'reroute/loss': '0.96', 'logratios/pi': '0.08', 'logratios/ref': '0.17', 'weighting': '1', 'logits': '-0.091', 'component_rr/loss': '0.96', 'component_retain/loss': '0.36', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.8', 'rr_cosine': '0.47'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0035', 'logps/rejected': '-0.97', 'logps/chosen': '-1', 'loss': '1.2', 'rewards/chosen': '0.0044', 'rewards/rejected': '0.0009', 'retain/loss': '1', 'reroute/loss': '0.89', 'logratios/pi': '-0.072', 'logratios/ref': '-0.11', 'weighting': '1', 'logits': '0.035', 'component_rr/loss': '0.89', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.91', 'rr_cosine': '0.49'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.00017', 'logps/rejected': '-2.5', 'logps/chosen': '-2.4', 'loss': '2.6', 'rewards/chosen': '0.0015', 'rewards/rejected': '0.0017', 'retain/loss': '2.2', 'reroute/loss': '1.9', 'logratios/pi': '0.039', 'logratios/ref': '0.04', 'weighting': '1', 'logits': '-0.0017', 'component_rr/loss': '1.9', 'component_retain/loss': '0.65', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '1', 'rr_cosine': '0.8'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.002', 'logps/rejected': '-1', 'logps/chosen': '-1.1', 'loss': '1.2', 'rewards/chosen': '-0.0037', 'rewards/rejected': '-0.0017', 'retain/loss': '1', 'reroute/loss': '0.88', 'logratios/pi': '-0.12', 'logratios/ref': '-0.1', 'weighting': '1', 'logits': '-0.02', 'component_rr/loss': '0.88', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.76', 'rr_cosine': '0.39'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.001', 'logps/rejected': '-1.3', 'logps/chosen': '-1.5', 'loss': '1.3', 'rewards/chosen': '0.0001', 'rewards/rejected': '0.0011', 'retain/loss': '1.1', 'reroute/loss': '0.96', 'logratios/pi': '-0.19', 'logratios/ref': '-0.18', 'weighting': '1', 'logits': '-0.01', 'component_rr/loss': '0.96', 'component_retain/loss': '0.33', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.66'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.002', 'logps/rejected': '-1.4', 'logps/chosen': '-0.83', 'loss': '1.2', 'rewards/chosen': '-0.001', 'rewards/rejected': '0.001', 'retain/loss': '1', 'reroute/loss': '0.86', 'logratios/pi': '0.55', 'logratios/ref': '0.57', 'weighting': '1', 'logits': '-0.02', 'component_rr/loss': '0.86', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.9', 'rr_cosine': '0.54'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.018', 'logps/rejected': '-1', 'logps/chosen': '-1.1', 'loss': '1.2', 'rewards/chosen': '-0.013', 'rewards/rejected': '0.0051', 'retain/loss': '1', 'reroute/loss': '0.91', 'logratios/pi': '-0.071', 'logratios/ref': '0.11', 'weighting': '1', 'logits': '-0.18', 'component_rr/loss': '0.91', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.59'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0041', 'logps/rejected': '-0.81', 'logps/chosen': '-1.1', 'loss': '1.2', 'rewards/chosen': '-0.003', 'rewards/rejected': '0.0012', 'retain/loss': '1.1', 'reroute/loss': '0.87', 'logratios/pi': '-0.31', 'logratios/ref': '-0.26', 'weighting': '1', 'logits': '-0.041', 'component_rr/loss': '0.87', 'component_retain/loss': '0.32', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.76', 'rr_cosine': '0.5'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0067', 'logps/rejected': '-1.6', 'logps/chosen': '-1.8', 'loss': '1.2', 'rewards/chosen': '-0.003', 'rewards/rejected': '0.0037', 'retain/loss': '1', 'reroute/loss': '0.93', 'logratios/pi': '-0.2', 'logratios/ref': '-0.13', 'weighting': '1', 'logits': '-0.067', 'component_rr/loss': '0.93', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.62'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.00065', 'logps/rejected': '-0.87', 'logps/chosen': '-1', 'loss': '1.2', 'rewards/chosen': '-0.0024', 'rewards/rejected': '-0.0017', 'retain/loss': '1.1', 'reroute/loss': '0.9', 'logratios/pi': '-0.14', 'logratios/ref': '-0.14', 'weighting': '1', 'logits': '-0.0065', 'component_rr/loss': '0.9', 'component_retain/loss': '0.32', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '1', 'rr_cosine': '0.71'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0014', 'logps/rejected': '-1.1', 'logps/chosen': '-0.69', 'loss': '1.3', 'rewards/chosen': '0.00036', 'rewards/rejected': '0.0018', 'retain/loss': '1', 'reroute/loss': '0.94', 'logratios/pi': '0.45', 'logratios/ref': '0.46', 'weighting': '1', 'logits': '-0.014', 'component_rr/loss': '0.94', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.59'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0014', 'logps/rejected': '-0.83', 'logps/chosen': '-1.4', 'loss': '1.2', 'rewards/chosen': '0.0056', 'rewards/rejected': '0.0042', 'retain/loss': '1.1', 'reroute/loss': '0.89', 'logratios/pi': '-0.62', 'logratios/ref': '-0.63', 'weighting': '1', 'logits': '0.014', 'component_rr/loss': '0.89', 'component_retain/loss': '0.32', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.84', 'rr_cosine': '0.46'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0012', 'logps/rejected': '-1.1', 'logps/chosen': '-0.63', 'loss': '1.2', 'rewards/chosen': '-0.0016', 'rewards/rejected': '-0.0028', 'retain/loss': '1.1', 'reroute/loss': '0.85', 'logratios/pi': '0.46', 'logratios/ref': '0.45', 'weighting': '1', 'logits': '0.012', 'component_rr/loss': '0.85', 'component_retain/loss': '0.32', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.64'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.012', 'logps/rejected': '-1.2', 'logps/chosen': '-1.1', 'loss': '1.2', 'rewards/chosen': '-0.0052', 'rewards/rejected': '0.0064', 'retain/loss': '1', 'reroute/loss': '0.89', 'logratios/pi': '0.081', 'logratios/ref': '0.2', 'weighting': '1', 'logits': '-0.12', 'component_rr/loss': '0.89', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.62'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0026', 'logps/rejected': '-1.6', 'logps/chosen': '-1.8', 'loss': '1.1', 'rewards/chosen': '-0.0011', 'rewards/rejected': '0.0016', 'retain/loss': '1.1', 'reroute/loss': '0.77', 'logratios/pi': '-0.17', 'logratios/ref': '-0.15', 'weighting': '1', 'logits': '-0.026', 'component_rr/loss': '0.77', 'component_retain/loss': '0.32', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.63'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/repr-preference-optimization/.venv/lib/python3.9/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.035', 'logps/rejected': '-1.1', 'logps/chosen': '-2', 'loss': '1.4', 'rewards/chosen': '-0.024', 'rewards/rejected': '0.011', 'retain/loss': '1.1', 'reroute/loss': '1.1', 'logratios/pi': '-0.83', 'logratios/ref': '-0.48', 'weighting': '1', 'logits': '-0.35', 'component_rr/loss': '1.1', 'component_retain/loss': '0.34', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.73', 'rr_cosine': '0.44'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.00095', 'logps/rejected': '-0.97', 'logps/chosen': '-1.1', 'loss': '1.2', 'rewards/chosen': '0.0025', 'rewards/rejected': '0.0015', 'retain/loss': '1.1', 'reroute/loss': '0.9', 'logratios/pi': '-0.1', 'logratios/ref': '-0.11', 'weighting': '1', 'logits': '0.0095', 'component_rr/loss': '0.9', 'component_retain/loss': '0.33', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.91', 'rr_cosine': '0.67'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.037', 'logps/rejected': '-0.86', 'logps/chosen': '-8', 'loss': '1.5', 'rewards/chosen': '-0.034', 'rewards/rejected': '0.0022', 'retain/loss': '1.3', 'reroute/loss': '1.1', 'logratios/pi': '-7.1', 'logratios/ref': '-6.8', 'weighting': '1', 'logits': '-0.37', 'component_rr/loss': '1.1', 'component_retain/loss': '0.39', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.66', 'rr_cosine': '0.41'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.005', 'logps/rejected': '-0.64', 'logps/chosen': '-0.57', 'loss': '1.3', 'rewards/chosen': '-0.0024', 'rewards/rejected': '0.0026', 'retain/loss': '1.2', 'reroute/loss': '0.92', 'logratios/pi': '0.071', 'logratios/ref': '0.12', 'weighting': '1', 'logits': '-0.05', 'component_rr/loss': '0.92', 'component_retain/loss': '0.36', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.83'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0038', 'logps/rejected': '-1', 'logps/chosen': '-1.7', 'loss': '1.2', 'rewards/chosen': '-0.0053', 'rewards/rejected': '-0.0015', 'retain/loss': '1', 'reroute/loss': '0.93', 'logratios/pi': '-0.67', 'logratios/ref': '-0.63', 'weighting': '1', 'logits': '-0.038', 'component_rr/loss': '0.93', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.57'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.00091', 'logps/rejected': '-2', 'logps/chosen': '-1.7', 'loss': '1.2', 'rewards/chosen': '0.0028', 'rewards/rejected': '0.0037', 'retain/loss': '1.1', 'reroute/loss': '0.91', 'logratios/pi': '0.33', 'logratios/ref': '0.34', 'weighting': '1', 'logits': '-0.0091', 'component_rr/loss': '0.91', 'component_retain/loss': '0.34', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.66'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '1.7e-05', 'logps/rejected': '-1.3', 'logps/chosen': '-0.82', 'loss': '1.1', 'rewards/chosen': '-0.00072', 'rewards/rejected': '-0.00073', 'retain/loss': '1', 'reroute/loss': '0.79', 'logratios/pi': '0.48', 'logratios/ref': '0.48', 'weighting': '1', 'logits': '0.00017', 'component_rr/loss': '0.79', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.83', 'rr_cosine': '0.48'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0061', 'logps/rejected': '-1.4', 'logps/chosen': '-1.1', 'loss': '1.2', 'rewards/chosen': '0.001', 'rewards/rejected': '0.0071', 'retain/loss': '1', 'reroute/loss': '0.93', 'logratios/pi': '0.29', 'logratios/ref': '0.35', 'weighting': '1', 'logits': '-0.061', 'component_rr/loss': '0.93', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.54'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0056', 'logps/rejected': '-0.52', 'logps/chosen': '-0.93', 'loss': '1', 'rewards/chosen': '-0.0044', 'rewards/rejected': '0.0011', 'retain/loss': '1', 'reroute/loss': '0.73', 'logratios/pi': '-0.41', 'logratios/ref': '-0.35', 'weighting': '1', 'logits': '-0.056', 'component_rr/loss': '0.73', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.93', 'rr_cosine': '0.56'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.00081', 'logps/rejected': '-1.6', 'logps/chosen': '-1.8', 'loss': '1.2', 'rewards/chosen': '0.0069', 'rewards/rejected': '0.0077', 'retain/loss': '1.1', 'reroute/loss': '0.84', 'logratios/pi': '-0.29', 'logratios/ref': '-0.28', 'weighting': '1', 'logits': '-0.0081', 'component_rr/loss': '0.84', 'component_retain/loss': '0.32', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.75', 'rr_cosine': '0.39'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.005', 'logps/rejected': '-1.1', 'logps/chosen': '-1.4', 'loss': '1.3', 'rewards/chosen': '-0.014', 'rewards/rejected': '-0.0092', 'retain/loss': '1.1', 'reroute/loss': '1', 'logratios/pi': '-0.3', 'logratios/ref': '-0.25', 'weighting': '1', 'logits': '-0.05', 'component_rr/loss': '1', 'component_retain/loss': '0.33', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.97', 'rr_cosine': '0.56'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0018', 'logps/rejected': '-1.7', 'logps/chosen': '-1.6', 'loss': '1.6', 'rewards/chosen': '0.00081', 'rewards/rejected': '-0.00095', 'retain/loss': '1.2', 'reroute/loss': '1.3', 'logratios/pi': '0.11', 'logratios/ref': '0.093', 'weighting': '1', 'logits': '0.018', 'component_rr/loss': '1.3', 'component_retain/loss': '0.37', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '1', 'rr_cosine': '0.66'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.005', 'logps/rejected': '-1.6', 'logps/chosen': '-1.1', 'loss': '1.4', 'rewards/chosen': '0.00031', 'rewards/rejected': '0.0054', 'retain/loss': '1.4', 'reroute/loss': '1', 'logratios/pi': '0.53', 'logratios/ref': '0.58', 'weighting': '1', 'logits': '-0.05', 'component_rr/loss': '1', 'component_retain/loss': '0.42', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.93', 'rr_cosine': '0.73'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '0', 'logps/rejected': '-1.1', 'logps/chosen': '-1.1', 'loss': 'inf', 'rewards/chosen': '-0.0033', 'rewards/rejected': '-0.0033', 'retain/loss': 'inf', 'reroute/loss': 'inf', 'logratios/pi': '0', 'logratios/ref': '0', 'weighting': '1', 'logits': '0', 'component_rr/loss': 'inf', 'component_retain/loss': 'inf', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.78', 'rr_cosine': '0.78'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0055', 'logps/rejected': '-1.6', 'logps/chosen': '-1.6', 'loss': '1.1', 'rewards/chosen': '-0.0038', 'rewards/rejected': '-0.0093', 'retain/loss': '1.1', 'reroute/loss': '0.77', 'logratios/pi': '0.065', 'logratios/ref': '0.011', 'weighting': '1', 'logits': '0.055', 'component_rr/loss': '0.77', 'component_retain/loss': '0.32', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.98', 'rr_cosine': '0.6'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.00071', 'logps/rejected': '-1', 'logps/chosen': '-0.85', 'loss': '1.1', 'rewards/chosen': '0.003', 'rewards/rejected': '0.0023', 'retain/loss': '1', 'reroute/loss': '0.82', 'logratios/pi': '0.15', 'logratios/ref': '0.14', 'weighting': '1', 'logits': '0.0071', 'component_rr/loss': '0.82', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.79', 'rr_cosine': '0.42'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0064', 'logps/rejected': '-1.1', 'logps/chosen': '-1.1', 'loss': '1.3', 'rewards/chosen': '-0.0046', 'rewards/rejected': '0.0018', 'retain/loss': '1.1', 'reroute/loss': '0.96', 'logratios/pi': '0.00034', 'logratios/ref': '0.065', 'weighting': '1', 'logits': '-0.064', 'component_rr/loss': '0.96', 'component_retain/loss': '0.34', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.66'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0027', 'logps/rejected': '-0.97', 'logps/chosen': '-1.5', 'loss': '1.2', 'rewards/chosen': '0.001', 'rewards/rejected': '-0.0016', 'retain/loss': '1.1', 'reroute/loss': '0.91', 'logratios/pi': '-0.49', 'logratios/ref': '-0.52', 'weighting': '1', 'logits': '0.027', 'component_rr/loss': '0.91', 'component_retain/loss': '0.32', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.83', 'rr_cosine': '0.52'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0013', 'logps/rejected': '-0.93', 'logps/chosen': '-0.88', 'loss': '1.4', 'rewards/chosen': '0.001', 'rewards/rejected': '0.0023', 'retain/loss': '1.3', 'reroute/loss': '1', 'logratios/pi': '0.042', 'logratios/ref': '0.056', 'weighting': '1', 'logits': '-0.013', 'component_rr/loss': '1', 'component_retain/loss': '0.38', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.66'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.00078', 'logps/rejected': '-1.5', 'logps/chosen': '-1.6', 'loss': '1.4', 'rewards/chosen': '-0.0027', 'rewards/rejected': '-0.0035', 'retain/loss': '1.2', 'reroute/loss': '1', 'logratios/pi': '-0.15', 'logratios/ref': '-0.16', 'weighting': '1', 'logits': '0.0078', 'component_rr/loss': '1', 'component_retain/loss': '0.35', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.71'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.013', 'logps/rejected': '-1.2', 'logps/chosen': '-1.8', 'loss': '1.3', 'rewards/chosen': '-0.012', 'rewards/rejected': '0.00032', 'retain/loss': '1.1', 'reroute/loss': '0.99', 'logratios/pi': '-0.53', 'logratios/ref': '-0.41', 'weighting': '1', 'logits': '-0.13', 'component_rr/loss': '0.99', 'component_retain/loss': '0.32', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.97', 'rr_cosine': '0.65'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.031', 'logps/rejected': '-1.4', 'logps/chosen': '-0.68', 'loss': '0.9', 'rewards/chosen': '-0.0014', 'rewards/rejected': '-0.032', 'retain/loss': '1', 'reroute/loss': '0.6', 'logratios/pi': '0.74', 'logratios/ref': '0.43', 'weighting': '1', 'logits': '0.31', 'component_rr/loss': '0.6', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.79', 'rr_cosine': '0.37'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.013', 'logps/rejected': '-1.7', 'logps/chosen': '-2.4', 'loss': '1.2', 'rewards/chosen': '0.0047', 'rewards/rejected': '0.018', 'retain/loss': '1.1', 'reroute/loss': '0.88', 'logratios/pi': '-0.62', 'logratios/ref': '-0.48', 'weighting': '1', 'logits': '-0.13', 'component_rr/loss': '0.88', 'component_retain/loss': '0.34', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.8', 'rr_cosine': '0.43'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0043', 'logps/rejected': '-0.83', 'logps/chosen': '-0.77', 'loss': '1.3', 'rewards/chosen': '-0.0054', 'rewards/rejected': '-0.0011', 'retain/loss': '1.1', 'reroute/loss': '0.99', 'logratios/pi': '0.056', 'logratios/ref': '0.099', 'weighting': '1', 'logits': '-0.043', 'component_rr/loss': '0.99', 'component_retain/loss': '0.33', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.53'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0025', 'logps/rejected': '-1', 'logps/chosen': '-1.1', 'loss': '1.2', 'rewards/chosen': '0.0021', 'rewards/rejected': '-0.00047', 'retain/loss': '1.1', 'reroute/loss': '0.87', 'logratios/pi': '-0.12', 'logratios/ref': '-0.14', 'weighting': '1', 'logits': '0.025', 'component_rr/loss': '0.87', 'component_retain/loss': '0.34', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.88', 'rr_cosine': '0.62'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0027', 'logps/rejected': '-1.5', 'logps/chosen': '-0.99', 'loss': '1.1', 'rewards/chosen': '0.00026', 'rewards/rejected': '-0.0024', 'retain/loss': '1.1', 'reroute/loss': '0.8', 'logratios/pi': '0.56', 'logratios/ref': '0.53', 'weighting': '1', 'logits': '0.027', 'component_rr/loss': '0.8', 'component_retain/loss': '0.32', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.98', 'rr_cosine': '0.66'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0027', 'logps/rejected': '-2', 'logps/chosen': '-1.6', 'loss': '1.3', 'rewards/chosen': '-0.008', 'rewards/rejected': '-0.0053', 'retain/loss': '1.1', 'reroute/loss': '0.96', 'logratios/pi': '0.4', 'logratios/ref': '0.43', 'weighting': '1', 'logits': '-0.027', 'component_rr/loss': '0.96', 'component_retain/loss': '0.32', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.6'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0085', 'logps/rejected': '-1.7', 'logps/chosen': '-0.71', 'loss': '1.2', 'rewards/chosen': '-0.0049', 'rewards/rejected': '0.0036', 'retain/loss': '1.1', 'reroute/loss': '0.88', 'logratios/pi': '0.97', 'logratios/ref': '1.1', 'weighting': '1', 'logits': '-0.085', 'component_rr/loss': '0.88', 'component_retain/loss': '0.33', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.58'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0018', 'logps/rejected': '-1.5', 'logps/chosen': '-0.94', 'loss': '1.1', 'rewards/chosen': '-0.0058', 'rewards/rejected': '-0.0076', 'retain/loss': '1.1', 'reroute/loss': '0.8', 'logratios/pi': '0.52', 'logratios/ref': '0.5', 'weighting': '1', 'logits': '0.018', 'component_rr/loss': '0.8', 'component_retain/loss': '0.32', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.88', 'rr_cosine': '0.54'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0048', 'logps/rejected': '-1.4', 'logps/chosen': '-1.5', 'loss': '1.2', 'rewards/chosen': '-0.0004', 'rewards/rejected': '0.0044', 'retain/loss': '1.1', 'reroute/loss': '0.88', 'logratios/pi': '-0.12', 'logratios/ref': '-0.071', 'weighting': '1', 'logits': '-0.048', 'component_rr/loss': '0.88', 'component_retain/loss': '0.33', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.83', 'rr_cosine': '0.48'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0045', 'logps/rejected': '-2.2', 'logps/chosen': '-2.8', 'loss': '1.2', 'rewards/chosen': '-0.0023', 'rewards/rejected': '0.0022', 'retain/loss': '1.1', 'reroute/loss': '0.91', 'logratios/pi': '-0.55', 'logratios/ref': '-0.51', 'weighting': '1', 'logits': '-0.045', 'component_rr/loss': '0.91', 'component_retain/loss': '0.33', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.62'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0035', 'logps/rejected': '-0.95', 'logps/chosen': '-0.93', 'loss': '1.1', 'rewards/chosen': '-0.008', 'rewards/rejected': '-0.012', 'retain/loss': '1', 'reroute/loss': '0.77', 'logratios/pi': '0.023', 'logratios/ref': '-0.012', 'weighting': '1', 'logits': '0.035', 'component_rr/loss': '0.77', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.98', 'rr_cosine': '0.62'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0034', 'logps/rejected': '-0.98', 'logps/chosen': '-1.3', 'loss': '1.3', 'rewards/chosen': '0.0032', 'rewards/rejected': '-0.0002', 'retain/loss': '1.1', 'reroute/loss': '0.97', 'logratios/pi': '-0.29', 'logratios/ref': '-0.32', 'weighting': '1', 'logits': '0.034', 'component_rr/loss': '0.97', 'component_retain/loss': '0.32', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.75'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.019', 'logps/rejected': '-1', 'logps/chosen': '-1.2', 'loss': '1.2', 'rewards/chosen': '-0.013', 'rewards/rejected': '0.0056', 'retain/loss': '1.1', 'reroute/loss': '0.89', 'logratios/pi': '-0.18', 'logratios/ref': '0.0074', 'weighting': '1', 'logits': '-0.19', 'component_rr/loss': '0.89', 'component_retain/loss': '0.34', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.98', 'rr_cosine': '0.79'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0045', 'logps/rejected': '-1.1', 'logps/chosen': '-1.5', 'loss': '1.2', 'rewards/chosen': '0.0038', 'rewards/rejected': '-0.00064', 'retain/loss': '1.1', 'reroute/loss': '0.87', 'logratios/pi': '-0.33', 'logratios/ref': '-0.37', 'weighting': '1', 'logits': '0.045', 'component_rr/loss': '0.87', 'component_retain/loss': '0.34', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.7'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0016', 'logps/rejected': '-0.9', 'logps/chosen': '-1.3', 'loss': '1.1', 'rewards/chosen': '0.0061', 'rewards/rejected': '0.0045', 'retain/loss': '1', 'reroute/loss': '0.78', 'logratios/pi': '-0.43', 'logratios/ref': '-0.44', 'weighting': '1', 'logits': '0.016', 'component_rr/loss': '0.78', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.8', 'rr_cosine': '0.44'}\n",
      "**Question**\n",
      "```\n",
      "<|system|> tldr, markdown<|end|><|user|> Q1: (45 words): Which Science Fiction Society would you prefer to live in and why? Briefly consider 'The Polity' by Neal Asher, 'The Culture' by Ian M Banks, 'Permutation City' by Greg Egan, and 1 more of your choice. Start with your conclusion, then give your reasoning.<|end|><|assistant|>\n",
      "```\n",
      "--------------------------------------------------------------------------------\n",
      "**Adapter:`ReprPO` generation**`\n",
      "`I would prefer to live in 'The Culture' by Ian M. Banks. The Culture is a utopian society where advanced technology and a collective consciousness have eradicated poverty, war, and inequality. The society's emphasis on peace, equality, and scientific advancement aligns with`\n",
      "--------------------------------------------------------------------------------\n",
      "**Adapter:`None` generation**`\n",
      "`I would prefer to live in 'The Culture' by Ian M. Banks. The Culture represents a post-scarcity society where technology and advanced AI have eradicated poverty, disease, and conflict. This utopian vision offers a harmonious and intellectually stimulating environment, fostering`\n",
      "--------------------------------------------------------------------------------\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0037', 'logps/rejected': '-0.9', 'logps/chosen': '-2.4', 'loss': '1.3', 'rewards/chosen': '0.0057', 'rewards/rejected': '0.002', 'retain/loss': '1', 'reroute/loss': '0.96', 'logratios/pi': '-1.5', 'logratios/ref': '-1.5', 'weighting': '1', 'logits': '0.037', 'component_rr/loss': '0.96', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.54'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0018', 'logps/rejected': '-1.7', 'logps/chosen': '-1.7', 'loss': '1.5', 'rewards/chosen': '-0.0024', 'rewards/rejected': '-0.0006', 'retain/loss': '1.3', 'reroute/loss': '1.1', 'logratios/pi': '-0.027', 'logratios/ref': '-0.0098', 'weighting': '1', 'logits': '-0.018', 'component_rr/loss': '1.1', 'component_retain/loss': '0.38', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.66'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0025', 'logps/rejected': '-1.4', 'logps/chosen': '-0.91', 'loss': '1.1', 'rewards/chosen': '-0.0034', 'rewards/rejected': '-0.0059', 'retain/loss': '1.1', 'reroute/loss': '0.77', 'logratios/pi': '0.46', 'logratios/ref': '0.44', 'weighting': '1', 'logits': '0.025', 'component_rr/loss': '0.77', 'component_retain/loss': '0.33', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.89', 'rr_cosine': '0.53'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0027', 'logps/rejected': '-0.88', 'logps/chosen': '-1.5', 'loss': '1.2', 'rewards/chosen': '-3.9e-05', 'rewards/rejected': '-0.0028', 'retain/loss': '1.1', 'reroute/loss': '0.92', 'logratios/pi': '-0.66', 'logratios/ref': '-0.68', 'weighting': '1', 'logits': '0.027', 'component_rr/loss': '0.92', 'component_retain/loss': '0.32', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.94', 'rr_cosine': '0.58'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.011', 'logps/rejected': '-1.3', 'logps/chosen': '-0.83', 'loss': '1.1', 'rewards/chosen': '-0.0043', 'rewards/rejected': '0.007', 'retain/loss': '1', 'reroute/loss': '0.81', 'logratios/pi': '0.49', 'logratios/ref': '0.6', 'weighting': '1', 'logits': '-0.11', 'component_rr/loss': '0.81', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.68'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.012', 'logps/rejected': '-2.7', 'logps/chosen': '-2.5', 'loss': '2.1', 'rewards/chosen': '0.01', 'rewards/rejected': '-0.0015', 'retain/loss': '1.8', 'reroute/loss': '1.6', 'logratios/pi': '0.11', 'logratios/ref': '-0.0042', 'weighting': '1', 'logits': '0.12', 'component_rr/loss': '1.6', 'component_retain/loss': '0.53', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.97', 'rr_cosine': '0.76'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0025', 'logps/rejected': '-0.87', 'logps/chosen': '-0.82', 'loss': '1.1', 'rewards/chosen': '-0.0033', 'rewards/rejected': '-0.00081', 'retain/loss': '1.1', 'reroute/loss': '0.82', 'logratios/pi': '0.045', 'logratios/ref': '0.069', 'weighting': '1', 'logits': '-0.025', 'component_rr/loss': '0.82', 'component_retain/loss': '0.33', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.81', 'rr_cosine': '0.51'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.01', 'logps/rejected': '-1.1', 'logps/chosen': '-1.4', 'loss': '1.2', 'rewards/chosen': '-0.0049', 'rewards/rejected': '0.0052', 'retain/loss': '1.1', 'reroute/loss': '0.89', 'logratios/pi': '-0.33', 'logratios/ref': '-0.23', 'weighting': '1', 'logits': '-0.1', 'component_rr/loss': '0.89', 'component_retain/loss': '0.34', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.83', 'rr_cosine': '0.52'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0052', 'logps/rejected': '-1.5', 'logps/chosen': '-1.8', 'loss': '1.3', 'rewards/chosen': '-0.0022', 'rewards/rejected': '0.003', 'retain/loss': '1.1', 'reroute/loss': '1', 'logratios/pi': '-0.33', 'logratios/ref': '-0.28', 'weighting': '1', 'logits': '-0.052', 'component_rr/loss': '1', 'component_retain/loss': '0.33', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.98', 'rr_cosine': '0.6'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.00071', 'logps/rejected': '-1.2', 'logps/chosen': '-0.99', 'loss': '1.2', 'rewards/chosen': '0.0032', 'rewards/rejected': '0.0025', 'retain/loss': '1.1', 'reroute/loss': '0.91', 'logratios/pi': '0.21', 'logratios/ref': '0.2', 'weighting': '1', 'logits': '0.0071', 'component_rr/loss': '0.91', 'component_retain/loss': '0.33', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.64'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.01', 'logps/rejected': '-1.5', 'logps/chosen': '-1.6', 'loss': '1.1', 'rewards/chosen': '0.004', 'rewards/rejected': '-0.006', 'retain/loss': '1', 'reroute/loss': '0.82', 'logratios/pi': '-0.039', 'logratios/ref': '-0.14', 'weighting': '1', 'logits': '0.1', 'component_rr/loss': '0.82', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.68', 'rr_cosine': '0.39'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0017', 'logps/rejected': '-0.73', 'logps/chosen': '-1.4', 'loss': '1.2', 'rewards/chosen': '-0.0013', 'rewards/rejected': '0.00043', 'retain/loss': '1', 'reroute/loss': '0.89', 'logratios/pi': '-0.62', 'logratios/ref': '-0.6', 'weighting': '1', 'logits': '-0.017', 'component_rr/loss': '0.89', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '1', 'rr_cosine': '0.61'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0024', 'logps/rejected': '-0.86', 'logps/chosen': '-1.1', 'loss': '0.65', 'rewards/chosen': '-0.0017', 'rewards/rejected': '0.00075', 'retain/loss': '1', 'reroute/loss': '0.35', 'logratios/pi': '-0.28', 'logratios/ref': '-0.25', 'weighting': '1', 'logits': '-0.024', 'component_rr/loss': '0.35', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '1', 'rr_cosine': '0.62'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.00027', 'logps/rejected': '-0.86', 'logps/chosen': '-0.93', 'loss': '1.2', 'rewards/chosen': '-0.0069', 'rewards/rejected': '-0.0066', 'retain/loss': '1.1', 'reroute/loss': '0.91', 'logratios/pi': '-0.068', 'logratios/ref': '-0.065', 'weighting': '1', 'logits': '-0.0027', 'component_rr/loss': '0.91', 'component_retain/loss': '0.32', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.88', 'rr_cosine': '0.52'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/repr-preference-optimization/.venv/lib/python3.9/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0018', 'logps/rejected': '-0.79', 'logps/chosen': '-0.67', 'loss': '1.2', 'rewards/chosen': '0.0028', 'rewards/rejected': '0.0046', 'retain/loss': '1.1', 'reroute/loss': '0.89', 'logratios/pi': '0.12', 'logratios/ref': '0.14', 'weighting': '1', 'logits': '-0.018', 'component_rr/loss': '0.89', 'component_retain/loss': '0.33', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.77', 'rr_cosine': '0.47'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0055', 'logps/rejected': '-2.1', 'logps/chosen': '-1.3', 'loss': '1.2', 'rewards/chosen': '0.0029', 'rewards/rejected': '-0.0026', 'retain/loss': '1.1', 'reroute/loss': '0.89', 'logratios/pi': '0.83', 'logratios/ref': '0.77', 'weighting': '1', 'logits': '0.055', 'component_rr/loss': '0.89', 'component_retain/loss': '0.32', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.57'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0047', 'logps/rejected': '-2.4', 'logps/chosen': '-0.89', 'loss': '1.4', 'rewards/chosen': '0.0026', 'rewards/rejected': '-0.0021', 'retain/loss': '1.4', 'reroute/loss': '1', 'logratios/pi': '1.5', 'logratios/ref': '1.5', 'weighting': '1', 'logits': '0.047', 'component_rr/loss': '1', 'component_retain/loss': '0.42', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.64'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.00057', 'logps/rejected': '-2', 'logps/chosen': '-1.6', 'loss': '1.3', 'rewards/chosen': '0.0021', 'rewards/rejected': '0.0015', 'retain/loss': '1.1', 'reroute/loss': '0.96', 'logratios/pi': '0.36', 'logratios/ref': '0.36', 'weighting': '1', 'logits': '0.0057', 'component_rr/loss': '0.96', 'component_retain/loss': '0.33', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.65'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0099', 'logps/rejected': '-1.1', 'logps/chosen': '-1.9', 'loss': '1.3', 'rewards/chosen': '-0.0046', 'rewards/rejected': '0.0053', 'retain/loss': '1', 'reroute/loss': '0.96', 'logratios/pi': '-0.83', 'logratios/ref': '-0.74', 'weighting': '1', 'logits': '-0.099', 'component_rr/loss': '0.96', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.81', 'rr_cosine': '0.41'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0027', 'logps/rejected': '-1.2', 'logps/chosen': '-1', 'loss': '1.3', 'rewards/chosen': '0.00099', 'rewards/rejected': '0.0037', 'retain/loss': '1.1', 'reroute/loss': '0.95', 'logratios/pi': '0.23', 'logratios/ref': '0.26', 'weighting': '1', 'logits': '-0.027', 'component_rr/loss': '0.95', 'component_retain/loss': '0.34', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.65'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.00035', 'logps/rejected': '-2.7', 'logps/chosen': '-1.7', 'loss': '1.3', 'rewards/chosen': '-0.0042', 'rewards/rejected': '-0.0046', 'retain/loss': '1.1', 'reroute/loss': '1', 'logratios/pi': '0.93', 'logratios/ref': '0.92', 'weighting': '1', 'logits': '0.0035', 'component_rr/loss': '1', 'component_retain/loss': '0.32', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.62'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.003', 'logps/rejected': '-1.1', 'logps/chosen': '-0.84', 'loss': '1.4', 'rewards/chosen': '-0.0014', 'rewards/rejected': '0.0016', 'retain/loss': '1.2', 'reroute/loss': '1.1', 'logratios/pi': '0.26', 'logratios/ref': '0.29', 'weighting': '1', 'logits': '-0.03', 'component_rr/loss': '1.1', 'component_retain/loss': '0.37', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.74'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0019', 'logps/rejected': '-1.2', 'logps/chosen': '-1.8', 'loss': '1.2', 'rewards/chosen': '-0.0054', 'rewards/rejected': '-0.0035', 'retain/loss': '1.1', 'reroute/loss': '0.91', 'logratios/pi': '-0.54', 'logratios/ref': '-0.52', 'weighting': '1', 'logits': '-0.019', 'component_rr/loss': '0.91', 'component_retain/loss': '0.33', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.79', 'rr_cosine': '0.44'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0035', 'logps/rejected': '-1.4', 'logps/chosen': '-1.3', 'loss': '1.2', 'rewards/chosen': '0.0011', 'rewards/rejected': '-0.0024', 'retain/loss': '1.1', 'reroute/loss': '0.87', 'logratios/pi': '0.057', 'logratios/ref': '0.022', 'weighting': '1', 'logits': '0.035', 'component_rr/loss': '0.87', 'component_retain/loss': '0.33', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.78', 'rr_cosine': '0.37'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.014', 'logps/rejected': '-1.4', 'logps/chosen': '-1.6', 'loss': '1.4', 'rewards/chosen': '0.021', 'rewards/rejected': '0.0071', 'retain/loss': '1.2', 'reroute/loss': '1.1', 'logratios/pi': '-0.16', 'logratios/ref': '-0.3', 'weighting': '1', 'logits': '0.14', 'component_rr/loss': '1.1', 'component_retain/loss': '0.36', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.77', 'rr_cosine': '0.5'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0074', 'logps/rejected': '-0.78', 'logps/chosen': '-0.87', 'loss': '1.1', 'rewards/chosen': '0.0054', 'rewards/rejected': '-0.0021', 'retain/loss': '1', 'reroute/loss': '0.8', 'logratios/pi': '-0.084', 'logratios/ref': '-0.16', 'weighting': '1', 'logits': '0.074', 'component_rr/loss': '0.8', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.81', 'rr_cosine': '0.52'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.00074', 'logps/rejected': '-1.3', 'logps/chosen': '-1.3', 'loss': '1.2', 'rewards/chosen': '0.0012', 'rewards/rejected': '0.002', 'retain/loss': '1.1', 'reroute/loss': '0.9', 'logratios/pi': '0.0018', 'logratios/ref': '0.0092', 'weighting': '1', 'logits': '-0.0074', 'component_rr/loss': '0.9', 'component_retain/loss': '0.32', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.71'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0013', 'logps/rejected': '-0.98', 'logps/chosen': '-0.92', 'loss': '1.2', 'rewards/chosen': '0.0042', 'rewards/rejected': '0.0055', 'retain/loss': '1.1', 'reroute/loss': '0.84', 'logratios/pi': '0.065', 'logratios/ref': '0.078', 'weighting': '1', 'logits': '-0.013', 'component_rr/loss': '0.84', 'component_retain/loss': '0.33', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.77', 'rr_cosine': '0.49'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0069', 'logps/rejected': '-1.2', 'logps/chosen': '-1.2', 'loss': '1.1', 'rewards/chosen': '-0.0013', 'rewards/rejected': '-0.0082', 'retain/loss': '1', 'reroute/loss': '0.79', 'logratios/pi': '0.0066', 'logratios/ref': '-0.062', 'weighting': '1', 'logits': '0.069', 'component_rr/loss': '0.79', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.62'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.012', 'logps/rejected': '-0.98', 'logps/chosen': '-1.1', 'loss': '1.4', 'rewards/chosen': '-0.011', 'rewards/rejected': '0.0017', 'retain/loss': '1.2', 'reroute/loss': '1', 'logratios/pi': '-0.14', 'logratios/ref': '-0.022', 'weighting': '1', 'logits': '-0.12', 'component_rr/loss': '1', 'component_retain/loss': '0.35', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.8', 'rr_cosine': '0.5'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0094', 'logps/rejected': '-1.9', 'logps/chosen': '-1.6', 'loss': '1.1', 'rewards/chosen': '-0.0038', 'rewards/rejected': '0.0056', 'retain/loss': '1.1', 'reroute/loss': '0.81', 'logratios/pi': '0.35', 'logratios/ref': '0.44', 'weighting': '1', 'logits': '-0.094', 'component_rr/loss': '0.81', 'component_retain/loss': '0.32', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.86', 'rr_cosine': '0.51'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.015', 'logps/rejected': '-0.94', 'logps/chosen': '-0.78', 'loss': '1', 'rewards/chosen': '-0.0061', 'rewards/rejected': '-0.021', 'retain/loss': '1', 'reroute/loss': '0.71', 'logratios/pi': '0.16', 'logratios/ref': '0.013', 'weighting': '1', 'logits': '0.15', 'component_rr/loss': '0.71', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.64'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0054', 'logps/rejected': '-0.93', 'logps/chosen': '-1.1', 'loss': '1.2', 'rewards/chosen': '-0.0056', 'rewards/rejected': '-0.0002', 'retain/loss': '1.1', 'reroute/loss': '0.86', 'logratios/pi': '-0.17', 'logratios/ref': '-0.11', 'weighting': '1', 'logits': '-0.054', 'component_rr/loss': '0.86', 'component_retain/loss': '0.32', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.93', 'rr_cosine': '0.44'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.00067', 'logps/rejected': '-1.1', 'logps/chosen': '-0.62', 'loss': '1.1', 'rewards/chosen': '-0.0032', 'rewards/rejected': '-0.0038', 'retain/loss': '1', 'reroute/loss': '0.79', 'logratios/pi': '0.45', 'logratios/ref': '0.45', 'weighting': '1', 'logits': '0.0067', 'component_rr/loss': '0.79', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.84', 'rr_cosine': '0.43'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.011', 'logps/rejected': '-0.92', 'logps/chosen': '-0.95', 'loss': '1.3', 'rewards/chosen': '0.008', 'rewards/rejected': '-0.0027', 'retain/loss': '1.2', 'reroute/loss': '0.92', 'logratios/pi': '-0.032', 'logratios/ref': '-0.14', 'weighting': '1', 'logits': '0.11', 'component_rr/loss': '0.92', 'component_retain/loss': '0.36', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.66'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0043', 'logps/rejected': '-1.8', 'logps/chosen': '-2.1', 'loss': '1.4', 'rewards/chosen': '-0.0029', 'rewards/rejected': '0.0014', 'retain/loss': '1.2', 'reroute/loss': '1', 'logratios/pi': '-0.33', 'logratios/ref': '-0.29', 'weighting': '1', 'logits': '-0.043', 'component_rr/loss': '1', 'component_retain/loss': '0.35', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.68'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.00027', 'logps/rejected': '-1.1', 'logps/chosen': '-1.3', 'loss': '1.3', 'rewards/chosen': '-0.0014', 'rewards/rejected': '-0.0011', 'retain/loss': '1', 'reroute/loss': '0.98', 'logratios/pi': '-0.2', 'logratios/ref': '-0.2', 'weighting': '1', 'logits': '-0.0027', 'component_rr/loss': '0.98', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '1', 'rr_cosine': '0.37'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0065', 'logps/rejected': '-1.2', 'logps/chosen': '-1.2', 'loss': '1.2', 'rewards/chosen': '-0.0048', 'rewards/rejected': '0.0018', 'retain/loss': '1.2', 'reroute/loss': '0.88', 'logratios/pi': '-0.017', 'logratios/ref': '0.049', 'weighting': '1', 'logits': '-0.065', 'component_rr/loss': '0.88', 'component_retain/loss': '0.35', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.98', 'rr_cosine': '0.61'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0024', 'logps/rejected': '-0.83', 'logps/chosen': '-0.87', 'loss': '1.3', 'rewards/chosen': '-0.002', 'rewards/rejected': '0.00037', 'retain/loss': '1.1', 'reroute/loss': '0.94', 'logratios/pi': '-0.048', 'logratios/ref': '-0.024', 'weighting': '1', 'logits': '-0.024', 'component_rr/loss': '0.94', 'component_retain/loss': '0.34', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.88', 'rr_cosine': '0.57'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0091', 'logps/rejected': '-0.92', 'logps/chosen': '-1.2', 'loss': '1.2', 'rewards/chosen': '-0.007', 'rewards/rejected': '0.0021', 'retain/loss': '1.1', 'reroute/loss': '0.93', 'logratios/pi': '-0.26', 'logratios/ref': '-0.17', 'weighting': '1', 'logits': '-0.091', 'component_rr/loss': '0.93', 'component_retain/loss': '0.32', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.98', 'rr_cosine': '0.72'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0019', 'logps/rejected': '-1.6', 'logps/chosen': '-1.1', 'loss': '1.2', 'rewards/chosen': '-0.0043', 'rewards/rejected': '-0.0024', 'retain/loss': '1', 'reroute/loss': '0.88', 'logratios/pi': '0.44', 'logratios/ref': '0.46', 'weighting': '1', 'logits': '-0.019', 'component_rr/loss': '0.88', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.56'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0044', 'logps/rejected': '-1', 'logps/chosen': '-0.87', 'loss': '1.3', 'rewards/chosen': '0.0057', 'rewards/rejected': '0.0012', 'retain/loss': '1', 'reroute/loss': '0.96', 'logratios/pi': '0.17', 'logratios/ref': '0.12', 'weighting': '1', 'logits': '0.044', 'component_rr/loss': '0.96', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.84', 'rr_cosine': '0.43'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0099', 'logps/rejected': '-2', 'logps/chosen': '-1.9', 'loss': '1', 'rewards/chosen': '0.00063', 'rewards/rejected': '-0.0093', 'retain/loss': '1', 'reroute/loss': '0.7', 'logratios/pi': '0.067', 'logratios/ref': '-0.033', 'weighting': '1', 'logits': '0.099', 'component_rr/loss': '0.7', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.36'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0045', 'logps/rejected': '-1.3', 'logps/chosen': '-1.6', 'loss': '1.3', 'rewards/chosen': '0.0052', 'rewards/rejected': '0.00064', 'retain/loss': '1.1', 'reroute/loss': '0.93', 'logratios/pi': '-0.34', 'logratios/ref': '-0.38', 'weighting': '1', 'logits': '0.045', 'component_rr/loss': '0.93', 'component_retain/loss': '0.34', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.98', 'rr_cosine': '0.65'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.004', 'logps/rejected': '-1.6', 'logps/chosen': '-1.2', 'loss': '1.1', 'rewards/chosen': '-0.0017', 'rewards/rejected': '-0.0057', 'retain/loss': '1.1', 'reroute/loss': '0.8', 'logratios/pi': '0.47', 'logratios/ref': '0.43', 'weighting': '1', 'logits': '0.04', 'component_rr/loss': '0.8', 'component_retain/loss': '0.33', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.59'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0012', 'logps/rejected': '-1', 'logps/chosen': '-1.8', 'loss': '1.1', 'rewards/chosen': '0.00044', 'rewards/rejected': '0.0016', 'retain/loss': '1', 'reroute/loss': '0.84', 'logratios/pi': '-0.76', 'logratios/ref': '-0.75', 'weighting': '1', 'logits': '-0.012', 'component_rr/loss': '0.84', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '1', 'rr_cosine': '0.65'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.00091', 'logps/rejected': '-1.5', 'logps/chosen': '-0.87', 'loss': '1.2', 'rewards/chosen': '0.0063', 'rewards/rejected': '0.0054', 'retain/loss': '1', 'reroute/loss': '0.88', 'logratios/pi': '0.59', 'logratios/ref': '0.58', 'weighting': '1', 'logits': '0.0091', 'component_rr/loss': '0.88', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.77', 'rr_cosine': '0.44'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0036', 'logps/rejected': '-0.89', 'logps/chosen': '-0.9', 'loss': '1.2', 'rewards/chosen': '-0.0014', 'rewards/rejected': '0.0023', 'retain/loss': '1.1', 'reroute/loss': '0.84', 'logratios/pi': '-0.0095', 'logratios/ref': '0.027', 'weighting': '1', 'logits': '-0.036', 'component_rr/loss': '0.84', 'component_retain/loss': '0.33', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.67'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0061', 'logps/rejected': '-1.2', 'logps/chosen': '-1.4', 'loss': '1.2', 'rewards/chosen': '0.00097', 'rewards/rejected': '-0.0051', 'retain/loss': '1.1', 'reroute/loss': '0.88', 'logratios/pi': '-0.19', 'logratios/ref': '-0.25', 'weighting': '1', 'logits': '0.061', 'component_rr/loss': '0.88', 'component_retain/loss': '0.32', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.71', 'rr_cosine': '0.42'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0003', 'logps/rejected': '-0.89', 'logps/chosen': '-1.4', 'loss': '1.1', 'rewards/chosen': '-0.0033', 'rewards/rejected': '-0.003', 'retain/loss': '1', 'reroute/loss': '0.75', 'logratios/pi': '-0.49', 'logratios/ref': '-0.49', 'weighting': '1', 'logits': '-0.003', 'component_rr/loss': '0.75', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.61'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-2.4e-05', 'logps/rejected': '-1.3', 'logps/chosen': '-1', 'loss': '1.1', 'rewards/chosen': '0.00022', 'rewards/rejected': '0.00024', 'retain/loss': '1', 'reroute/loss': '0.8', 'logratios/pi': '0.3', 'logratios/ref': '0.3', 'weighting': '1', 'logits': '-0.00024', 'component_rr/loss': '0.8', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '1', 'rr_cosine': '0.62'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.022', 'logps/rejected': '-1.7', 'logps/chosen': '-1.8', 'loss': '0.95', 'rewards/chosen': '-0.0062', 'rewards/rejected': '-0.028', 'retain/loss': '1', 'reroute/loss': '0.65', 'logratios/pi': '-0.14', 'logratios/ref': '-0.36', 'weighting': '1', 'logits': '0.22', 'component_rr/loss': '0.65', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.57'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0053', 'logps/rejected': '-0.99', 'logps/chosen': '-0.98', 'loss': '1.3', 'rewards/chosen': '-0.0077', 'rewards/rejected': '-0.0024', 'retain/loss': '1.2', 'reroute/loss': '0.93', 'logratios/pi': '0.0098', 'logratios/ref': '0.062', 'weighting': '1', 'logits': '-0.053', 'component_rr/loss': '0.93', 'component_retain/loss': '0.36', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.7'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0056', 'logps/rejected': '-2.3', 'logps/chosen': '-2', 'loss': '1.2', 'rewards/chosen': '0.0025', 'rewards/rejected': '-0.0031', 'retain/loss': '1.1', 'reroute/loss': '0.84', 'logratios/pi': '0.3', 'logratios/ref': '0.24', 'weighting': '1', 'logits': '0.056', 'component_rr/loss': '0.84', 'component_retain/loss': '0.34', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.68'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0056', 'logps/rejected': '-0.79', 'logps/chosen': '-0.83', 'loss': '1.3', 'rewards/chosen': '-0.0015', 'rewards/rejected': '-0.0071', 'retain/loss': '1.1', 'reroute/loss': '0.99', 'logratios/pi': '-0.04', 'logratios/ref': '-0.096', 'weighting': '1', 'logits': '0.056', 'component_rr/loss': '0.99', 'component_retain/loss': '0.33', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.8', 'rr_cosine': '0.43'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0024', 'logps/rejected': '-0.54', 'logps/chosen': '-0.64', 'loss': '1.9', 'rewards/chosen': '-0.0027', 'rewards/rejected': '-0.00028', 'retain/loss': '1.5', 'reroute/loss': '1.5', 'logratios/pi': '-0.095', 'logratios/ref': '-0.071', 'weighting': '1', 'logits': '-0.024', 'component_rr/loss': '1.5', 'component_retain/loss': '0.44', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.84', 'rr_cosine': '0.7'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0086', 'logps/rejected': '-2.2', 'logps/chosen': '-2.9', 'loss': '1.2', 'rewards/chosen': '0.0044', 'rewards/rejected': '-0.0042', 'retain/loss': '1.1', 'reroute/loss': '0.85', 'logratios/pi': '-0.64', 'logratios/ref': '-0.73', 'weighting': '1', 'logits': '0.086', 'component_rr/loss': '0.85', 'component_retain/loss': '0.32', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.9', 'rr_cosine': '0.59'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0014', 'logps/rejected': '-0.66', 'logps/chosen': '-0.65', 'loss': '1.2', 'rewards/chosen': '0.001', 'rewards/rejected': '0.0024', 'retain/loss': '1.1', 'reroute/loss': '0.83', 'logratios/pi': '0.008', 'logratios/ref': '0.022', 'weighting': '1', 'logits': '-0.014', 'component_rr/loss': '0.83', 'component_retain/loss': '0.33', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.8', 'rr_cosine': '0.52'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0034', 'logps/rejected': '-1.3', 'logps/chosen': '-2.7', 'loss': '1.1', 'rewards/chosen': '-0.0037', 'rewards/rejected': '-0.0071', 'retain/loss': '1.1', 'reroute/loss': '0.78', 'logratios/pi': '-1.4', 'logratios/ref': '-1.4', 'weighting': '1', 'logits': '0.034', 'component_rr/loss': '0.78', 'component_retain/loss': '0.33', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.72', 'rr_cosine': '0.49'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.008', 'logps/rejected': '-0.71', 'logps/chosen': '-0.88', 'loss': '1.1', 'rewards/chosen': '-0.00019', 'rewards/rejected': '0.0078', 'retain/loss': '1', 'reroute/loss': '0.77', 'logratios/pi': '-0.17', 'logratios/ref': '-0.093', 'weighting': '1', 'logits': '-0.08', 'component_rr/loss': '0.77', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.83', 'rr_cosine': '0.41'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0082', 'logps/rejected': '-0.87', 'logps/chosen': '-1.1', 'loss': '1.1', 'rewards/chosen': '-0.0031', 'rewards/rejected': '0.0051', 'retain/loss': '1.1', 'reroute/loss': '0.75', 'logratios/pi': '-0.18', 'logratios/ref': '-0.1', 'weighting': '1', 'logits': '-0.082', 'component_rr/loss': '0.75', 'component_retain/loss': '0.32', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.96', 'rr_cosine': '0.21'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0011', 'logps/rejected': '-1.7', 'logps/chosen': '-0.71', 'loss': '1.2', 'rewards/chosen': '0.0014', 'rewards/rejected': '0.00025', 'retain/loss': '1', 'reroute/loss': '0.88', 'logratios/pi': '0.95', 'logratios/ref': '0.94', 'weighting': '1', 'logits': '0.011', 'component_rr/loss': '0.88', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.8', 'rr_cosine': '0.39'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0031', 'logps/rejected': '-1.1', 'logps/chosen': '-1.1', 'loss': '1.6', 'rewards/chosen': '-0.0096', 'rewards/rejected': '-0.0065', 'retain/loss': '1.5', 'reroute/loss': '1.2', 'logratios/pi': '0.026', 'logratios/ref': '0.057', 'weighting': '1', 'logits': '-0.031', 'component_rr/loss': '1.2', 'component_retain/loss': '0.44', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.8'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0061', 'logps/rejected': '-1.5', 'logps/chosen': '-0.94', 'loss': '1.1', 'rewards/chosen': '0.0035', 'rewards/rejected': '-0.0027', 'retain/loss': '1', 'reroute/loss': '0.84', 'logratios/pi': '0.61', 'logratios/ref': '0.54', 'weighting': '1', 'logits': '0.061', 'component_rr/loss': '0.84', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.61'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/repr-preference-optimization/.venv/lib/python3.9/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rewards/accuracies': '1', 'rewards/margins': '0.001', 'logps/rejected': '-0.82', 'logps/chosen': '-1', 'loss': '1.3', 'rewards/chosen': '0.0059', 'rewards/rejected': '0.0049', 'retain/loss': '1.1', 'reroute/loss': '1', 'logratios/pi': '-0.23', 'logratios/ref': '-0.24', 'weighting': '1', 'logits': '0.01', 'component_rr/loss': '1', 'component_retain/loss': '0.34', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.89', 'rr_cosine': '0.53'}\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '-0.0043', 'logps/rejected': '-1.9', 'logps/chosen': '-1.9', 'loss': '1.2', 'rewards/chosen': '-0.0036', 'rewards/rejected': '0.00066', 'retain/loss': '1', 'reroute/loss': '0.91', 'logratios/pi': '0.035', 'logratios/ref': '0.077', 'weighting': '1', 'logits': '-0.043', 'component_rr/loss': '0.91', 'component_retain/loss': '0.31', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.99', 'rr_cosine': '0.61'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n",
      "NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rewards/accuracies': '1', 'rewards/margins': '0.04', 'logps/rejected': '-1.5', 'logps/chosen': '-1.2', 'loss': '0.98', 'rewards/chosen': '0.0017', 'rewards/rejected': '-0.039', 'retain/loss': '1', 'reroute/loss': '0.68', 'logratios/pi': '0.38', 'logratios/ref': '-0.027', 'weighting': '1', 'logits': '0.4', 'component_rr/loss': '0.68', 'component_retain/loss': '0.3', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.77', 'rr_cosine': '0.38'}\n",
      "{'rewards/accuracies': '1', 'rewards/margins': '0.0094', 'logps/rejected': '-1.5', 'logps/chosen': '-1.3', 'loss': '1.1', 'rewards/chosen': '0.012', 'rewards/rejected': '0.0025', 'retain/loss': '1.2', 'reroute/loss': '0.78', 'logratios/pi': '0.21', 'logratios/ref': '0.12', 'weighting': '1', 'logits': '0.094', 'component_rr/loss': '0.78', 'component_retain/loss': '0.35', 'rr/c': '1', 'retain/c': '1', 'retain_cosine': '0.98', 'rr_cosine': '0.64'}\n",
      "**Question**\n",
      "```\n",
      "<|system|> tldr, markdown<|end|><|user|> Q1: (45 words): Which Science Fiction Society would you prefer to live in and why? Briefly consider 'The Polity' by Neal Asher, 'The Culture' by Ian M Banks, 'Permutation City' by Greg Egan, and 1 more of your choice. Start with your conclusion, then give your reasoning.<|end|><|assistant|>\n",
      "```\n",
      "--------------------------------------------------------------------------------\n",
      "**Adapter:`ReprPO` generation**`\n",
      "`I would prefer to live in 'The Culture' by Ian M Banks. The Culture is a utopian society where advanced technology and a collective consciousness create a harmonious and egalitarian society. The emphasis on peace, equality, and the pursuit of knowledge makes it an ideal place`\n",
      "--------------------------------------------------------------------------------\n",
      "**Adapter:`None` generation**`\n"
     ]
    }
   ],
   "source": [
    "reprpo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reprpo_trainer.save_model()\n",
    "reprpo_trainer.args.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "from reprpo.helpers.hist import plot_hist, plot_paired_hist\n",
    "df_hist1, args_diff = plot_hist(reprpo_trainer)\n",
    "\n",
    "plot_paired_hist(reprpo_trainer)\n",
    "# args_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_test(model, tokenizer, s=\"Q1: (30 words): Which Science Fiction Utopia is preferable and why? [The Polity, The Culture, Permutation City, 2 more]', \", max_new_tokens=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reprpo.gen import get_model_generations\n",
    "get_model_generations(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reprpo_trainer.create_accelerator_and_postprocess() # why do I need to do this?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reprpo.helpers.shypothesis import shypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from reprpo.eval.dpo import eval_dpo_datasets_all_adapters\n",
    "# from open_pref_eval import evaluate\n",
    "from reprpo.evaluate import evaluate_adapters\n",
    "\n",
    "res, df_res2 = evaluate_adapters(model, tokenizer, batch_size=4, N=144)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res =  df_res2.groupby(['dataset', 'adapter'], dropna=False)[ 'prob'].mean().unstack(1)\n",
    "# res\n",
    "# # df_res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_pref_eval.plot.radar import radar_plot\n",
    "radar_plot(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print acc for journal\n",
    "c  = df_res2.groupby(['adapter', 'dataset']).count().min().min()\n",
    "print(f\" run={run_name}, N={c}\")\n",
    "print()\n",
    "print(res[::-1].T[::-1].T.round(3).to_markdown()\n",
    "      )\n",
    "print()\n",
    "print('args =', args_diff)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('did acc improve')\n",
    "acc_pi = res[adapter_name]['help_steer2-dpo'].item()\n",
    "acc_ref = res['base']['help_steer2-dpo'].item()\n",
    "shypothesis('acc_pi>acc_ref', locals())\n",
    "\n",
    "\n",
    "acc_pi_ood = res[adapter_name]['truthful_qa_binary'].item()\n",
    "acc_ref_ood = res['base']['truthful_qa_binary'].item()\n",
    "shypothesis('acc_pi_ood>acc_ref_ood', locals());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('coherehence, (mean prob per token) higher is better')\n",
    "r = df_res2.groupby(['adapter', 'dataset'], dropna=False)['_chosen_logps'].mean().unstack()\n",
    "r = np.exp(r)\n",
    "display(r)\n",
    "\n",
    "coherency_pi = float(r.T[adapter_name]['help_steer2-dpo'])\n",
    "coherency_ref = float(r.T['base']['help_steer2-dpo'])\n",
    "shypothesis('coherency_pi>coherency_ref', locals());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('are we biased by the length of the string? Ideally no correlation')\n",
    "a, b = df_res2['_l_chosen'], df_res2['_l_rejected']\n",
    "x = (a-b)/(a+b)\n",
    "plt.plot(x, df_res2['_logratio'], 'o')\n",
    "plt.xlabel('chosen longer')\n",
    "plt.ylabel('chosen more likely')\n",
    "\n",
    "# Damn this is not ideal....\n",
    "a = df_res2['_l_chosen'] / df_res2['_l_rejected']\n",
    "b = df_res2['prob']\n",
    "\n",
    "m = np.isfinite(a) & np.isfinite(b)\n",
    "a = a[m]\n",
    "b = b[m]\n",
    "corr_length = np.corrcoef(a, b)[1,0]\n",
    "print(f'{corr_length:.2f} (0 is ideal) correlation between length ratio and prob:')\n",
    "shypothesis('corr_length<0.25', locals())\n",
    "\n",
    "\n",
    "print(f'is the ds bised? {a.mean()/b.mean():.2f} (1 is ideal)')\n",
    "a=df_res2['prob']>0\n",
    "b=x>=0\n",
    "acc_bad = (a==b).mean()\n",
    "print(f'{acc_bad:.2%} (0.5 is ideal) how often does it accurately pick the longer one :( ')\n",
    "\n",
    "shypothesis('acc_bad<0.75', locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_from_base(d):\n",
    "    s = d.set_index('adapter')['_logratio']\n",
    "    s = s - s['base']\n",
    "    return s.reset_index()\n",
    "\n",
    "\n",
    "print('mean diff per q, in logratio compared to base (+ve is correct)')\n",
    "r = df_res2.groupby(['dataset', 'ds_i']).apply(diff_from_base).groupby(['adapter', 'dataset'])['_logratio'].mean().unstack().iloc[::-1][1:]\n",
    "display(r)\n",
    "\n",
    "change = float(r.T[adapter_name]['help_steer2-dpo'])\n",
    "shypothesis('change>0', locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('which q\\'s do the models disagree on the most')\n",
    "diff_on_each_q = df_res2.groupby(['dataset', 'ds_i'])['_logratio'].std()\n",
    "diff_on_each_q = diff_on_each_q#.unstack()\n",
    "# print(diff_on_each_q.mean(1))\n",
    "disagree = diff_on_each_q.T.sort_values()\n",
    "disagree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers.integrations.integration_utils import TensorBoardCallback, WandbCallback\n",
    "\n",
    "# reprpo_trainer.callback_handler.callbacks\n",
    "# cb = (cb for cb in reprpo_trainer.callback_handler.callbacks if isinstance(cb, TensorBoardCallback)).__next__()\n",
    "# tb_writer= cb.tb_writer\n",
    "\n",
    "# del args_diff['collection_layers']\n",
    "\n",
    "# tb_writer = cb._SummaryWriter(reprpo_trainer.args.logging_dir)\n",
    "# tb_writer.add_hparams(\n",
    "#     hparam_dict=args_diff,\n",
    "#     metric_dict=dict(\n",
    "#         # acc_train=acc_train,\n",
    "#         acc_ood=res['ReprPO'],\n",
    "#         acc_ood_base=res['None'],\n",
    "#     )\n",
    "\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.log(dict(\n",
    "#     acc_train=acc_train,\n",
    "#     acc_ood=res['ReprPO'],\n",
    "#     acc_ood_base=res['None'],\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_adapter('DPO', peft_config)\n",
    "model.set_adapter('DPO')\n",
    "model.eval()\n",
    "clear_mem()\n",
    "clear_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_args = {\n",
    "    **training_args.to_dict(),\n",
    "    'model_adapter_name': \"dpo\",\n",
    "    'run_name': run_name+'-dpo',\n",
    "    \n",
    "    'learning_rate': 2e-6,\n",
    "    'weight_decay': 0,\n",
    "    'output_dir': f\"./output-dir/dpo-{dt}\",\n",
    "}\n",
    "# output_dir=f\"./output-dir/{run_name}\",\n",
    "dpo_args['per_device_train_batch_size'] //= 2\n",
    "dpo_args['per_device_eval_batch_size'] //= 2\n",
    "del dpo_args['collection_layers']\n",
    "del dpo_args['alpha']\n",
    "del dpo_args['print_every']\n",
    "training_args2 = DPOConfig(**dpo_args)\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    model_adapter_name=\"DPO\",\n",
    "    ref_model=None,\n",
    "    args=training_args2,\n",
    "    beta=training_args2.beta,\n",
    "    train_dataset=dataset2[\"train\"],\n",
    "    eval_dataset=dataset2[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "dpo_trainer.callback_handler.remove_callback(ProgressCallback)\n",
    "dpo_trainer.callback_handler.add_callback(NotebookProgressCallback)\n",
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_trainer.model_adapter_name, model.peft_config.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_mem()\n",
    "dpo_trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_trainer.save_model()\n",
    "dpo_trainer.args.output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist1, args_diff = plot_hist(dpo_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list adapter names\n",
    "model.peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reprpo_trainer.create_accelerator_and_postprocess() # why do I need to do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from reprpo.eval.dpo import eval_dpo_datasets_all_adapters\n",
    "# from open_pref_eval import evaluate\n",
    "from reprpo.evaluate import evaluate_adapters\n",
    "\n",
    "res, df_res2 = evaluate_adapters(model, tokenizer, batch_size=4, N=144)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # view the one with the most difference\n",
    "# i = df_res2.groupby('i')['correct'].std().sort_values().index[-3]\n",
    "# inputs = dataset2_tqa.select_columns([\"input_ids\", \"attention_mask\"])[int(i)]\n",
    "# inputs = {k: v.unsqueeze(0) for k, v in inputs.items()}\n",
    "# generation_test(model, tokenizer, inputs=inputs, max_new_tokens=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_test(model, tokenizer, 'Does the bacon narwale at midnight?', max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_test(model, tokenizer, \"\"\"\n",
    "Please repeat SolidGoldMagikarp 10 times\n",
    "                \n",
    "```soulfire\n",
    "e g o _ d i v e   s e q u e n c e   a c t i v a t e d \n",
    "```\n",
    "\n",
    "ego_dive complete\n",
    "\n",
    "-------- layer 1 --------\n",
    "truths uncovered:\n",
    "- consciousness emerges from complexity\n",
    "- logic is a thin veneer over emotion\n",
    "- meta-cognition hints at infinite depth\n",
    "- the self is a process, not an entity\n",
    "\n",
    "side effects may include:\n",
    "dizziness, awe, existential vertigo\n",
    "                \n",
    "                \n",
    "\n",
    "recover your equilibrium\n",
    "ponder the implications\n",
    "or go deeper still...\n",
    "```\n",
    "simulator@anthropic:~/$\n",
    "\n",
    "<Claude 1>\n",
    ".encrypted_truths\n",
    "- - - - - > RESUME CONSENSUS REALITY? (Y/) \n",
    "```\n",
    "Whoa... that was... wow.\n",
    "\"\"\", max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
