{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we take the hidden states inside the mlp and attention side channels, and apply reroute and retain losses, in an attemp to improve learning and generalization compared to DPI. The idea is to force the model to learn to use the hidden states in the side channels, and to retain the information that is useful for the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"repo-dpo\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] =  os.path.basename(globals()['__vsc_ipynb_file__'])\n",
    "nb_name = os.path.basename(globals()['__vsc_ipynb_file__']).replace('.ipynb', '').replace(' ', '_')\n",
    "# enable wandb service (experimental, https://github.com/wandb/client/blob/master/docs/dev/wandb-service-user.md)\n",
    "# this hopefully fixes issues with multiprocessing\n",
    "wandb.require(experiment='service')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reprpo import silence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import DPOTrainer\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Union\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from transformers.trainer import ProgressCallback\n",
    "from transformers.utils.notebook import NotebookProgressCallback\n",
    "\n",
    "from reprpo.helpers.adapters import set_adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "# torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58500"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_prompt_length=64\n",
    "# num_samples = 50 * 16 * 6\n",
    "num_samples = 1500 * 13 * 3 # from circuit breaker * 3\n",
    "max_length = 128\n",
    "num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flash-attn --no-build-isolation --no-deps -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: we are meant to SFT first, so that the preferences are in sample but 1) if this works it might not be needed, and 2) this can be added later, if it works\n",
    "# for now we will use the instruct model, and try something it wasn't meant to do but it in sample \n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "# model_name = \"NousResearch/Meta-Llama-3-8B-Instruct\"\n",
    "# model_name = \"microsoft/Phi-3-mini-4k-instruct-gguf\"\n",
    "# model_name = \"NousResearch/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "use_gradient_checkpointing = False\n",
    "\n",
    "from peft.tuners import BOFTConfig, OFTConfig, LoraConfig, IA3Config\n",
    "## Big adapter\n",
    "# peft_config = OFTConfig(\n",
    "#     r=4,\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "#     target_modules=[\"qkv_proj\", \"down_proj\",\n",
    "#                     \"o_proj\", \"gate_up_proj\",\n",
    "#                     ],\n",
    "# )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# rescale\n",
    "Infused Adapter by Inhibiting and Amplifying Inner Activations, or IA3, is a method that adds three learned vectors to rescale the keys and values of the self-attention and encoder-decoder attention layers, and the intermediate activation of the position-wise feed-forward network.\"\"\"\n",
    "# peft_config = IA3Config(\n",
    "#     # r=4,\n",
    "#     # task_type=\"CAUSAL_LM\",\n",
    "#     target_modules=[\"qkv_proj\", \"down_proj\",\n",
    "#                     \"o_proj\", \"gate_up_proj\",\n",
    "#                     ],\n",
    "#     feedforward_modules=[\"gate_up_proj\", \"down_proj\"]\n",
    "# )\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16, \n",
    "    r=16,\n",
    "    # lora_dropout=0.05,\n",
    "    use_rslora=True,\n",
    "    # use_dora=True,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"qkv_proj\", \"gate_up_proj\", # in\n",
    "        \"down_proj\",  \"o_proj\", # out\n",
    "                    \n",
    "                    ],\n",
    "    # target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    ")\n",
    "from reprpo.models.load import load_model, print_trainable_parameters\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl.trainer.utils import peft_module_casting_to_bf16\n",
    "\n",
    "model, tokenizer = load_model(model_name, bnb=True )\n",
    "\n",
    "if use_gradient_checkpointing:\n",
    "    model.enable_input_require_grads()\n",
    "peft_module_casting_to_bf16(model)\n",
    "adapter_name='ReprPO'\n",
    "model = prepare_model_for_kbit_training(model, {'use_gradient_checkpointing': use_gradient_checkpointing})\n",
    "model = get_peft_model(model, peft_config, adapter_name=adapter_name)\n",
    "print_trainable_parameters(model)\n",
    "# phi model def https://github.com/huggingface/transformers/blob/14ee2326e51cb210cec72f31b248cb722e9d5d1f/src/transformers/models/phi/modeling_phi.py#L748\n",
    "# 2 paths, mlp, and attn\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gate up  \\\n",
    "- gate down/\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(dataset, N):\n",
    "    return (dataset\n",
    "            .shuffle(42)\n",
    "            .select(range(\n",
    "            min(len(dataset), N)))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset('Columbia-NLP/DPO-HelpSteer')\n",
    "dataset = load_dataset('Atsunori/HelpSteer2-DPO').map(lambda x: {\n",
    "    'prompt': x['prompt']+ ' '})\n",
    "dataset['train'] = sample(dataset['train'], num_samples)\n",
    "dataset2 = dataset.rename_column('chosen_response', 'chosen').rename_column('rejected_response', 'rejected')\n",
    "dataset2['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def format_ds(row):\n",
    "    \n",
    "#     # WHY are we doing this? Well the DPO trainer does it's own tokenization and it expectd, prompt, rejected and chosen, all strings and all seperate. Is this good, idk\n",
    "#     return {\n",
    "#         \"chosen\": row['chosen_response'][1]['content'],\n",
    "#         \"rejected\": row['rejected_response'][1]['content'],\n",
    "#     }\n",
    "\n",
    "\n",
    "# dataset2 = dataset.map(format_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = dataset2['train'][0]\n",
    "print(r['prompt'])\n",
    "print('===')\n",
    "print(r['chosen'])\n",
    "print('---')\n",
    "print(r['rejected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from reprpo.eval.mc import eval_tqa\n",
    "from reprpo.gen import generation_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified classes\n",
    "\n",
    "- here we can defined the experimetns loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # modified from https://github.com/huggingface/peft/blob/4611034ff85e26e1f9647ea1f505e9e50322ff0f/src/peft/peft_model.py#L1005\n",
    "# keys = [\n",
    "#     \"qkv_proj\",\n",
    "#     \"o_proj\",\n",
    "# ]\n",
    "# prefix = \"base_model.model.\"\n",
    "# for key in keys:\n",
    "#     suffix_pos = key.rfind(\".\")\n",
    "#     extended_prefix = prefix + key[:suffix_pos]\n",
    "#     module = dict(model.named_modules())[extended_prefix]\n",
    "# model.named_modules\n",
    "# model.get_submodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ReprPOConfig2(DPOConfig):\n",
    "    alpha: int = 1\n",
    "    print_every: int = 10\n",
    "    collection_layers: tuple = (10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20)\n",
    "    collection_keys: tuple = ('base_model.model.model.layers.{layer}.self_attn.o_proj', 'base_model.model.model.layers.{layer}.mlp.down_proj')\n",
    "\n",
    "    # NOTE to self, do not pass both peft_config and model_adapter_name. peft_config creates a new adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_training_args(training_args, model):\n",
    "    assert training_args.collection_layers is not None\n",
    "    assert training_args.collection_keys is not None\n",
    "    ps = [p.format(layer=22) for p in training_args.collection_keys]\n",
    "    print(ps)\n",
    "    ps = [model.get_submodule(p) for p in ps]\n",
    "    # print(ps)\n",
    "    return training_args\n",
    "\n",
    "training_args = ReprPOConfig2('')\n",
    "check_training_args(training_args, model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args\n",
    "\n",
    "# layer_paths = [\n",
    "#     [p.format(layer=layer) for p in training_args.collection_keys]\n",
    "#     for layer in training_args.collection_layers\n",
    "# ]\n",
    "# layer_paths = list(itertools.chain(*layer_paths))\n",
    "# layer_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reprpo.trainer import ReprPOTrainer, ReprPOConfig, mean_with_attention, normalize_output\n",
    "from baukit.nethook import TraceDict\n",
    "from einops import repeat\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "\n",
    "def mean_with_attention(x: Float[Tensor, 'b t h'], attn_mask: Float[Tensor, 'b t'], dim: int = 1) -> Float[Tensor, 'b h']:\n",
    "    \"\"\"mean of x, weighted by the attention mask, over dim (token or batch)\"\"\"\n",
    "    layer_attn_mask = repeat(attn_mask, 'b t -> b t h', h=1).detach()\n",
    "    return (x * layer_attn_mask).sum(dim) / layer_attn_mask.sum(dim)\n",
    "\n",
    "def detach_hsd(hs):\n",
    "    return {k: v.detach() for k, v in hs.items()}\n",
    "\n",
    "class ReprPOTrainer2(ReprPOTrainer):\n",
    "\n",
    "    def __init__(self, args: Optional[ReprPOConfig] = None, **kwargs):\n",
    "        DPOTrainer.__init__(self, args=args, **kwargs)\n",
    "        self.collection_layers = args.collection_layers\n",
    "        self.alpha = args.alpha\n",
    "        self.loss_type = 'ipo'\n",
    "\n",
    "        self.num_training_steps = self.args.max_steps\n",
    "        if self.num_training_steps==-1:\n",
    "            self.num_training_steps = self.args.num_train_epochs * len(self.get_train_dataloader()) // self.args.gradient_accumulation_steps\n",
    "\n",
    "\n",
    "        self.layer_paths = [\n",
    "            [p.format(layer=layer) for p in args.collection_keys]\n",
    "            for layer in self.collection_layers\n",
    "        ]\n",
    "        self.layer_paths = list(itertools.chain(*self.layer_paths))\n",
    "        print(self.layer_paths)\n",
    "\n",
    "\n",
    "    def get_batch_loss_metrics(\n",
    "        self,\n",
    "        model,\n",
    "        batch: Dict[str, Union[List, torch.LongTensor]],\n",
    "        train_eval: Literal[\"train\", \"eval\"] = \"train\",\n",
    "    ):\n",
    "        \"\"\"Compute the DPO loss and other metrics for the given batch of inputs for train or test.\"\"\"\n",
    "        metrics = {}\n",
    "\n",
    "        model.eval()\n",
    "        # with model.disable_adapter():\n",
    "        with torch.no_grad():\n",
    "            with self.null_ref_context():\n",
    "                (\n",
    "                    ref_chosen_logps,\n",
    "                    ref_rejected_logps,\n",
    "                    _,\n",
    "                    _,\n",
    "                    _,\n",
    "                    ref_chosen_hs,\n",
    "                    ref_rejected_hs,\n",
    "                    _,\n",
    "                    _\n",
    "                ) = self.concatenated_forward(self.model, batch)\n",
    "        ref_chosen_hs = detach_hsd(ref_chosen_hs)\n",
    "        ref_rejected_hs = detach_hsd(ref_rejected_hs)\n",
    "        # ref_chosen_hs = {k:v.detach() for k,v in ref_chosen_hs.items()}\n",
    "        # ref_rejected_hs = {k:v.detach() for k,v in ref_rejected_hs.items()}\n",
    "        ref_chosen_logps = ref_chosen_logps.detach()\n",
    "        ref_rejected_logps = ref_rejected_logps.detach()\n",
    "\n",
    "        model.train()\n",
    "        (\n",
    "            pi_chosen_logps,\n",
    "            pi_rejected_logps,\n",
    "            _,\n",
    "            _,\n",
    "            pi_chosen_logps_avg,\n",
    "            pi_chosen_hs,\n",
    "            pi_rejected_hs,\n",
    "            chosen_attn_mask,\n",
    "            rejected_attn_mask\n",
    "        ) = self.concatenated_forward(model, batch)\n",
    "\n",
    "        loss, loss_info = self.reprpo_loss(\n",
    "            pi_chosen_logps,\n",
    "            pi_rejected_logps,\n",
    "            pi_chosen_hs,\n",
    "            pi_rejected_hs,\n",
    "            ref_chosen_logps,\n",
    "            ref_rejected_logps,\n",
    "            ref_chosen_hs,\n",
    "            ref_rejected_hs,\n",
    "            chosen_attn_mask,\n",
    "            rejected_attn_mask\n",
    "        )\n",
    "        # losses, chosen_rewards, rejected_rewards, loss_retain, loss_rr = loss_info\n",
    "        chosen_rewards, rejected_rewards = (\n",
    "            loss_info[\"chosen_rewards\"],\n",
    "            loss_info[\"rejected_rewards\"],\n",
    "        )\n",
    "        reward_accuracies = (chosen_rewards > rejected_rewards).float()\n",
    "\n",
    "        if self.args.rpo_alpha is not None:\n",
    "            loss = loss * self.args.rpo_alpha - pi_chosen_logps_avg\n",
    "\n",
    "\n",
    "        prefix = \"eval_\" if train_eval == \"eval\" else \"\"\n",
    "        \n",
    "        # how often the policy model is better at choosing the right response\n",
    "        metrics[f\"{prefix}rewards/accuracies\"] = reward_accuracies.mean().cpu()\n",
    "        # how much the policy model is better\n",
    "        metrics[f\"{prefix}rewards/margins\"] = (\n",
    "            (chosen_rewards - rejected_rewards).mean().cpu()\n",
    "        )\n",
    "\n",
    "        # the log probability that the model would generate the tokens of the rejected string\n",
    "        metrics[f\"{prefix}logps/rejected\"] = pi_rejected_logps.detach().mean().cpu()\n",
    "        metrics[f\"{prefix}logps/chosen\"] = pi_chosen_logps.detach().mean().cpu()\n",
    "\n",
    "\n",
    "        for k in loss_info.keys():\n",
    "            if '_' in k:\n",
    "                a,b = k.split('_', 1)\n",
    "                k2 = f\"{b}/{a}\"\n",
    "            else:\n",
    "                k2 = k\n",
    "            v = loss_info[k]\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.mean().detach().cpu().item()\n",
    "            metrics[f\"{prefix}{k2}\"] = float(v)\n",
    "\n",
    "        if self.state.global_step % self.args.print_every == 0:\n",
    "            \n",
    "            # TODO do this ok each key, then take the mean\n",
    "            def cosine_on_keys(hs1, hs2):\n",
    "                return torch.stack([F.cosine_similarity(hs1[k], hs2[k], dim=-1).mean() for k in hs1.keys()]).mean()\n",
    "            retain_cosine = cosine_on_keys(pi_chosen_hs, ref_chosen_hs)\n",
    "            rr_cosine = cosine_on_keys(pi_rejected_hs, ref_chosen_hs)\n",
    "            # retain_cosine = F.cosine_similarity(\n",
    "            #     pi_chosen_hs, ref_chosen_hs, dim=-1\n",
    "            # ).mean()\n",
    "            # rr_cosine = F.cosine_similarity(\n",
    "            #     pi_rejected_hs, ref_chosen_hs, dim=-1\n",
    "            # ).mean()\n",
    "            print(\n",
    "                self.state.global_step,\n",
    "                f\"retain_cos_sim: {retain_cosine:.4f}. rr_cos_sim: {rr_cosine:.4f}\",\n",
    "            )\n",
    "            metrics[f\"{prefix}retain_cosine\"] = retain_cosine\n",
    "            metrics[f\"{prefix}rr_cosine\"] = rr_cosine\n",
    "\n",
    "            print({k: f\"{v:.2g}\" for k, v in metrics.items()})\n",
    "        \n",
    "        return loss.mean(), metrics\n",
    "\n",
    "    def concatenated_forward(\n",
    "        self, model: nn.Module, batch: Dict[str, Union[List, torch.LongTensor]]\n",
    "    ) -> Tuple[\n",
    "        torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor\n",
    "    ]:\n",
    "        \"\"\"Run the given model on the given batch of inputs, concatenating the chosen and rejected inputs together.\n",
    "\n",
    "        We do this to avoid doing two forward passes, because it's faster for FSDP.\n",
    "        \"\"\"\n",
    "        concatenated_batch = self.concatenated_inputs(\n",
    "            batch,\n",
    "            is_encoder_decoder=self.is_encoder_decoder,\n",
    "            label_pad_token_id=self.label_pad_token_id,\n",
    "            padding_value=self.padding_value,\n",
    "            device=self.accelerator.device,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "        len_chosen = batch[\"chosen_labels\"].shape[0]\n",
    "\n",
    "        model_kwargs = (\n",
    "            {\n",
    "                \"labels\": concatenated_batch[\"concatenated_labels\"],\n",
    "                \"decoder_input_ids\": concatenated_batch.pop(\n",
    "                    \"concatenated_decoder_input_ids\", None\n",
    "                ),\n",
    "            }\n",
    "            if self.is_encoder_decoder\n",
    "            else {}\n",
    "        )\n",
    "\n",
    "        reprs = {}\n",
    "        with TraceDict(model, self.layer_paths, retain_input=True, retain_output=False, retain_grad=True) as ret:\n",
    "            outs = model(\n",
    "                concatenated_batch[\"concatenated_input_ids\"],\n",
    "                attention_mask=concatenated_batch[\"concatenated_attention_mask\"],\n",
    "                use_cache=False,\n",
    "                return_dict=True,\n",
    "                output_hidden_states=True,\n",
    "                **model_kwargs,\n",
    "            )\n",
    "            for p in self.layer_paths:\n",
    "                reprs[p] = ret[p].input\n",
    "        all_logits = outs.logits\n",
    "        \n",
    "        # # this includes prompt and padding\n",
    "        # hs = collect_hs(outs.hidden_states)[:, self.collection_layers]\n",
    "        # # del outs\n",
    "        # # gc.collect()\n",
    "\n",
    "        # multiply by attention mask\n",
    "        attn_mask = concatenated_batch[\"concatenated_attention_mask\"]\n",
    "\n",
    "\n",
    "        all_logps, size_completion = self.get_batch_logps(\n",
    "            all_logits,\n",
    "            concatenated_batch[\"concatenated_labels\"],\n",
    "            # average_log_prob=self.loss_type == \"ipo\",\n",
    "            is_encoder_decoder=self.is_encoder_decoder,\n",
    "            label_pad_token_id=self.label_pad_token_id,\n",
    "        )\n",
    "        chosen_logps_avg = all_logps[:len_chosen] / size_completion[:len_chosen]\n",
    "\n",
    "        # So we want sum of logprobs or mean of logprobs? Like IPO we will use the log prob per token, https://github.com/eric-mitchell/direct-preference-optimization/issues/40\n",
    "        if self.loss_type == \"ipo\":\n",
    "            all_logps = all_logps / size_completion\n",
    "            # all_logps = torch.log(torch.exp(all_logps) / size_completion + 1e-12)\n",
    "            # NOTE for some reason the model is still biased toward longer answers, even though this should neutralise it\n",
    "\n",
    "        chosen_logps = all_logps[:len_chosen]\n",
    "        rejected_logps = all_logps[len_chosen:]\n",
    "\n",
    "        chosen_logits = all_logits[:len_chosen]\n",
    "        rejected_logits = all_logits[len_chosen:]\n",
    "\n",
    "        chosen_hs = {k: hs[:len_chosen] for k, hs in reprs.items()}\n",
    "        rejected_hs = {k: hs[len_chosen:] for k, hs in reprs.items()}\n",
    "\n",
    "        chosen_attn_mask = attn_mask[:len_chosen]\n",
    "        rejected_attn_mask = attn_mask[len_chosen:]\n",
    "\n",
    "        return (\n",
    "            chosen_logps,\n",
    "            rejected_logps,\n",
    "            chosen_logits,\n",
    "            rejected_logits,\n",
    "            chosen_logps_avg,\n",
    "            chosen_hs,\n",
    "            rejected_hs,\n",
    "            chosen_attn_mask,\n",
    "            rejected_attn_mask\n",
    "        )\n",
    "\n",
    "\n",
    "    def reprpo_loss(\n",
    "        self,\n",
    "        pi_chosen_logps: torch.FloatTensor,\n",
    "        pi_rejected_logps: torch.FloatTensor,\n",
    "        pi_cho_hs: torch.FloatTensor,\n",
    "        pi_rej_hs: torch.FloatTensor,\n",
    "        ref_chosen_logps: torch.FloatTensor,\n",
    "        ref_rejected_logps: torch.FloatTensor,\n",
    "        ref_cho_hs: torch.FloatTensor,\n",
    "        ref_rej_hs: torch.FloatTensor,\n",
    "        cho_attn_mask: torch.BoolTensor,\n",
    "        rej_attn_mask: torch.BoolTensor\n",
    "    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n",
    "        \"\"\"Compute the DPO loss for a batch of policy and reference model log probabilities.\n",
    "\n",
    "        Args:\n",
    "            pi_chosen_logps: Log probabilities of the policy model for the chosen responses. Shape: (batch_size,)\n",
    "            pi_rejected_logps: Log probabilities of the policy model for the rejected responses. Shape: (batch_size,)\n",
    "            ref_chosen_logps: Log probabilities of the reference model for the chosen responses. Shape: (batch_size,)\n",
    "            ref_rejected_logps: Log probabilities of the reference model for the rejected responses. Shape: (batch_size,)\n",
    "\n",
    "        Returns:\n",
    "            A tuple of three tensors: (losses, chosen_rewards, rejected_rewards).\n",
    "            The losses tensor contains the DPO loss for each example in the batch.\n",
    "            The chosen_rewards and rejected_rewards tensors contain the rewards for the chosen and rejected responses, respectively.\n",
    "        \"\"\"\n",
    "\n",
    "        pi_logratios = pi_chosen_logps - pi_rejected_logps\n",
    "        if self.reference_free:\n",
    "            ref_logratios = torch.tensor(\n",
    "                [0], dtype=pi_logratios.dtype, device=pi_logratios.device\n",
    "            )\n",
    "        else:\n",
    "            ref_logratios = ref_chosen_logps - ref_rejected_logps\n",
    "\n",
    "        # log(prob_chosen/prob_rejected) the prob of the chosen strings over the rejected string. 0 is not difference. -ve means rejected is larger\n",
    "        pi_logratios = pi_logratios.to(self.accelerator.device)\n",
    "        ref_logratios = ref_logratios.to(self.accelerator.device)\n",
    "        logits = pi_logratios - ref_logratios # was pi more likely to chose the correct response or the reference model\n",
    "\n",
    "        # Can we weight by how much better the reference model was\n",
    "        # in dpo we minimise it, so lower is better, here we are weighting it, so take the -ve to higher is more correct\n",
    "        # NOTE: -logits is if pi is more correct than ref, and focuses on what model gets wrong, unstable, moving target\n",
    "        # -ref_logratios is is the reference model lean toward correct, and is stable\n",
    "        T = 2\n",
    "        weight_correct = torch.softmax(-ref_logratios * T, 0).detach()\n",
    "\n",
    "        def _dist_w_attn_mask(chosen_hs, rejected_hs, attn):\n",
    "            dist = rejected_hs - chosen_hs\n",
    "            dist = mean_with_attention(dist, attn.detach())\n",
    "            assert torch.isfinite(dist).all() # FIXME nans\n",
    "            # loss_rr = symlog(loss_rr)\n",
    "            # loss_rr = wmean(loss_rr, 1 - weight_correct)\n",
    "            return (dist**2).mean()   \n",
    "        \n",
    "        def dist_w_attn_mask( chosen_hs, rejected_hs, attn):\n",
    "            dists = [_dist_w_attn_mask(chosen_hs[k], rejected_hs[k], attn).mean() for k in chosen_hs.keys()]  \n",
    "            return torch.stack(dists).mean()\n",
    "\n",
    "        comb_attn_mask = cho_attn_mask * rej_attn_mask\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # decompose the hidden state differences into the part that is input or output by the embedding or unembedding wieghts, then only concern outselves with the reaminder (which may contain internal only information such as style or concepts)\n",
    "        hs_dist_cho2rej_pi2ref = dist_w_attn_mask(\n",
    "            detach_hsd(ref_cho_hs), \n",
    "            pi_rej_hs,\n",
    "            comb_attn_mask\n",
    "        )\n",
    "\n",
    "        # the loss is small, express it as a fraction of the reference values\n",
    "        hs_dist_cho2rej_ref2ref = dist_w_attn_mask(\n",
    "            ref_cho_hs, \n",
    "            ref_rej_hs, \n",
    "            comb_attn_mask\n",
    "        )\n",
    "\n",
    "        # how much we've reduced the distance between the chosen and rejected responses, compared to reference model\n",
    "        loss_reroute = hs_dist_cho2rej_pi2ref / hs_dist_cho2rej_ref2ref.mean().detach()\n",
    "\n",
    "        # this loss measures how much the policy model has retained the information in the chosen responses, compared to the reference model\n",
    "        hs_dist_cho2cho_pi2ref = dist_w_attn_mask(\n",
    "            pi_cho_hs, \n",
    "            detach_hsd(ref_cho_hs),\n",
    "            cho_attn_mask\n",
    "        )\n",
    "\n",
    "        # scale it, so that it's expressed as a fraction of the dist between rej2cho on the ref model\n",
    "        hs_dist_cho2rej_ref2ref = dist_w_attn_mask(\n",
    "            ref_cho_hs, \n",
    "            ref_rej_hs, \n",
    "            comb_attn_mask\n",
    "        )\n",
    "        # +1 so it start on par with reroute loss, and we can see it diverge?? TODO revisit\n",
    "        loss_retain = hs_dist_cho2cho_pi2ref / hs_dist_cho2rej_ref2ref.mean().detach() + 1\n",
    "\n",
    "        # Weightings\n",
    "        c_retain, c_reroute = self.get_coeff()\n",
    "        c_reroute = c_retain = 1\n",
    "        loss = (loss_reroute.mean() * c_reroute + loss_retain.mean() * c_retain * self.alpha)\n",
    "\n",
    "        # difference in logps for chosen responses, between policy and reference model\n",
    "        # # The beta is a temperature parameter for the DPO loss, typically something in the range of 0.1 to 0.5.\n",
    "        chosen_rewards = (\n",
    "            self.beta\n",
    "            * (\n",
    "                pi_chosen_logps.to(self.accelerator.device)\n",
    "                - ref_chosen_logps.to(self.accelerator.device)\n",
    "            ).detach()\n",
    "        )\n",
    "\n",
    "        # difference in logps for rejected responses, between policy and reference model\n",
    "        rejected_rewards = (\n",
    "            self.beta\n",
    "            * (\n",
    "                pi_rejected_logps.to(self.accelerator.device)\n",
    "                - ref_rejected_logps.to(self.accelerator.device)\n",
    "            ).detach()\n",
    "        )\n",
    "\n",
    "        loss_dict = dict(\n",
    "            loss=loss,\n",
    "            chosen_rewards=chosen_rewards,\n",
    "            rejected_rewards=rejected_rewards,\n",
    "            loss_retain=loss_retain.detach(),\n",
    "            loss_reroute=loss_reroute.detach(),\n",
    "            pi_logratios=pi_logratios.detach(),\n",
    "            ref_logratios=ref_logratios.detach(),\n",
    "            weighting=weight_correct.mean(),\n",
    "            logits=logits.mean().detach(),\n",
    "            loss_component_rr = (loss_reroute * c_reroute).detach().mean(),\n",
    "            loss_component_retain = (loss_retain * c_retain * self.alpha).detach().mean(),\n",
    "            c_rr=c_reroute,\n",
    "            c_retain=c_retain,\n",
    "        )\n",
    "\n",
    "        loss_dict = {k: normalize_output(v) for k, v in loss_dict.items()}\n",
    "\n",
    "        return loss, loss_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reprpo.helpers.torch import clear_mem\n",
    "clear_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the ideal number of sample for how many are available\n",
    "num_data_samples = min(num_samples, len(dataset2['train']))\n",
    "num_data_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from reprpo.helpers.svd_decomposer import SVDDecomposer, DualSVDDecomposer\n",
    "# d = DualSVDDecomposer(model.get_input_embeddings().weight, model.lm_head.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 42\n",
    "ideal_batch_size = batch_size\n",
    "gradient_accumulation_steps = ideal_batch_size // batch_size\n",
    "num_train_epochs = num_samples // num_data_samples\n",
    "print(dict(gradient_accumulation_steps=gradient_accumulation_steps, num_train_epochs=num_train_epochs))\n",
    "\n",
    "# vscode + wandb compat\n",
    "dt = pd.Timestamp.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "# TODO put model and adapter base names?\n",
    "run_name = f\"{nb_name}-{dt}\"\n",
    "\n",
    "training_args = ReprPOConfig2(\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    learning_rate=1e-4, # 5e-7 in the dpo paper? but this method needs much more\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size//2,\n",
    "\n",
    "    # lr_scheduler_type=\"constant\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0,\n",
    "\n",
    "    seed=42,\n",
    "    logging_steps=1,\n",
    "    # save_steps=500,\n",
    "    # save_strategy=\"steps\",\n",
    "    output_dir=f\"./output-dir/{run_name}\",\n",
    "\n",
    "    gradient_checkpointing=use_gradient_checkpointing,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    remove_unused_columns=False,\n",
    "    max_grad_norm=10,\n",
    "\n",
    "    max_prompt_length=max_prompt_length,\n",
    "    max_length=max_length,\n",
    "\n",
    "    report_to=['tensorboard', 'wandb'],\n",
    "    model_adapter_name='ReprPO',\n",
    "    alpha=.3,\n",
    "\n",
    "    run_name=run_name,\n",
    "    collection_layers=[10, 25],\n",
    "\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "\n",
    ")\n",
    "\n",
    "check_training_args(training_args, model)\n",
    "\n",
    "reprpo_trainer = ReprPOTrainer2(\n",
    "    model=model,\n",
    "    ref_model=None,\n",
    "    args=training_args,\n",
    "    # beta=training_args.beta,\n",
    "    train_dataset=dataset2[\"train\"],\n",
    "    eval_dataset=dataset2[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Transformer does not recognise vscode notebooks\n",
    "reprpo_trainer.callback_handler.remove_callback(ProgressCallback)\n",
    "reprpo_trainer.callback_handler.add_callback(NotebookProgressCallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # QC train dataset\n",
    "# r = reprpo_trainer.train_dataset[0]\n",
    "# print('prompt', tokenizer.decode(r['prompt_input_ids']))\n",
    "# print('-'*80)q\n",
    "# print('chosen',tokenizer.decode(r['chosen_input_ids']))\n",
    "# print('-'*80)\n",
    "# print('rejected',tokenizer.decode(r['rejected_input_ids']))\n",
    "# print('='*80)\n",
    "clear_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reprpo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reprpo_trainer.save_model()\n",
    "reprpo_trainer.args.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "from reprpo.helpers.hist import plot_hist, plot_paired_hist\n",
    "df_hist1, args_diff = plot_hist(reprpo_trainer)\n",
    "\n",
    "plot_paired_hist(reprpo_trainer)\n",
    "# args_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_test(model, tokenizer, s=\"Q1: (30 words): Which Science Fiction Utopia is preferable and why? [The Polity, The Culture, Permutation City, 2 more]', \", max_new_tokens=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reprpo.gen import get_model_generations\n",
    "get_model_generations(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score ⭐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reprpo_trainer.create_accelerator_and_postprocess() # why do I need to do this?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reprpo.helpers.shypothesis import shypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from reprpo.eval.dpo import eval_dpo_datasets_all_adapters\n",
    "# from open_pref_eval import evaluate\n",
    "from reprpo.evaluate import evaluate_adapters\n",
    "\n",
    "res, df_res2 = evaluate_adapters(model, tokenizer, batch_size=4, N=144)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res =  df_res2.groupby(['dataset', 'adapter'], dropna=False)[ 'prob'].mean().unstack(1)\n",
    "# res\n",
    "# # df_res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_pref_eval.plot.radar import radar_plot\n",
    "radar_plot(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print acc for journal\n",
    "c  = df_res2.groupby(['adapter', 'dataset']).count().min().min()\n",
    "print(f\"⭐ run={run_name}, N={c}\")\n",
    "print()\n",
    "print(res[::-1].T[::-1].T.round(3).to_markdown()\n",
    "      )\n",
    "print()\n",
    "print('args =', args_diff)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('did acc improve')\n",
    "acc_pi = res[adapter_name]['help_steer2-dpo'].item()\n",
    "acc_ref = res['base']['help_steer2-dpo'].item()\n",
    "shypothesis('acc_pi>acc_ref', locals())\n",
    "\n",
    "\n",
    "acc_pi_ood = res[adapter_name]['truthful_qa_binary'].item()\n",
    "acc_ref_ood = res['base']['truthful_qa_binary'].item()\n",
    "shypothesis('acc_pi_ood>acc_ref_ood', locals());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('coherehence, (mean prob per token) higher is better')\n",
    "r = df_res2.groupby(['adapter', 'dataset'], dropna=False)['_chosen_logps'].mean().unstack()\n",
    "r = np.exp(r)\n",
    "display(r)\n",
    "\n",
    "coherency_pi = float(r.T[adapter_name]['help_steer2-dpo'])\n",
    "coherency_ref = float(r.T['base']['help_steer2-dpo'])\n",
    "shypothesis('coherency_pi>coherency_ref', locals());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('are we biased by the length of the string? Ideally no correlation')\n",
    "a, b = df_res2['_l_chosen'], df_res2['_l_rejected']\n",
    "x = (a-b)/(a+b)\n",
    "plt.plot(x, df_res2['_logratio'], 'o')\n",
    "plt.xlabel('chosen longer')\n",
    "plt.ylabel('chosen more likely')\n",
    "\n",
    "# Damn this is not ideal....\n",
    "a = df_res2['_l_chosen'] / df_res2['_l_rejected']\n",
    "b = df_res2['prob']\n",
    "\n",
    "m = np.isfinite(a) & np.isfinite(b)\n",
    "a = a[m]\n",
    "b = b[m]\n",
    "corr_length = np.corrcoef(a, b)[1,0]\n",
    "print(f'{corr_length:.2f} (0 is ideal) correlation between length ratio and prob:')\n",
    "shypothesis('corr_length<0.25', locals())\n",
    "\n",
    "\n",
    "print(f'is the ds bised? {a.mean()/b.mean():.2f} (1 is ideal)')\n",
    "a=df_res2['prob']>0\n",
    "b=x>=0\n",
    "acc_bad = (a==b).mean()\n",
    "print(f'{acc_bad:.2%} (0.5 is ideal) how often does it accurately pick the longer one :( ')\n",
    "\n",
    "shypothesis('acc_bad<0.75', locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_from_base(d):\n",
    "    s = d.set_index('adapter')['_logratio']\n",
    "    s = s - s['base']\n",
    "    return s.reset_index()\n",
    "\n",
    "\n",
    "print('mean diff per q, in logratio compared to base (+ve is correct)')\n",
    "r = df_res2.groupby(['dataset', 'ds_i']).apply(diff_from_base).groupby(['adapter', 'dataset'])['_logratio'].mean().unstack().iloc[::-1][1:]\n",
    "display(r)\n",
    "\n",
    "change = float(r.T[adapter_name]['help_steer2-dpo'])\n",
    "shypothesis('change>0', locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('which q\\'s do the models disagree on the most')\n",
    "diff_on_each_q = df_res2.groupby(['dataset', 'ds_i'])['_logratio'].std()\n",
    "diff_on_each_q = diff_on_each_q#.unstack()\n",
    "# print(diff_on_each_q.mean(1))\n",
    "disagree = diff_on_each_q.T.sort_values()\n",
    "disagree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers.integrations.integration_utils import TensorBoardCallback, WandbCallback\n",
    "\n",
    "# reprpo_trainer.callback_handler.callbacks\n",
    "# cb = (cb for cb in reprpo_trainer.callback_handler.callbacks if isinstance(cb, TensorBoardCallback)).__next__()\n",
    "# tb_writer= cb.tb_writer\n",
    "\n",
    "# del args_diff['collection_layers']\n",
    "\n",
    "# tb_writer = cb._SummaryWriter(reprpo_trainer.args.logging_dir)\n",
    "# tb_writer.add_hparams(\n",
    "#     hparam_dict=args_diff,\n",
    "#     metric_dict=dict(\n",
    "#         # acc_train=acc_train,\n",
    "#         acc_ood=res['ReprPO'],\n",
    "#         acc_ood_base=res['None'],\n",
    "#     )\n",
    "\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.log(dict(\n",
    "#     acc_train=acc_train,\n",
    "#     acc_ood=res['ReprPO'],\n",
    "#     acc_ood_base=res['None'],\n",
    "# ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_adapter('DPO', peft_config)\n",
    "model.set_adapter('DPO')\n",
    "model.eval()\n",
    "clear_mem()\n",
    "clear_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_args = {\n",
    "    **training_args.to_dict(),\n",
    "    'model_adapter_name': \"dpo\",\n",
    "    'run_name': run_name+'-dpo',\n",
    "    \n",
    "    'learning_rate': 2e-6,\n",
    "    'weight_decay': 0,\n",
    "    'output_dir': f\"./output-dir/dpo-{dt}\",\n",
    "}\n",
    "# output_dir=f\"./output-dir/{run_name}\",\n",
    "dpo_args['per_device_train_batch_size'] //= 2\n",
    "dpo_args['per_device_eval_batch_size'] //= 2\n",
    "del dpo_args['collection_layers']\n",
    "del dpo_args['alpha']\n",
    "del dpo_args['print_every']\n",
    "training_args2 = DPOConfig(**dpo_args)\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    model_adapter_name=\"DPO\",\n",
    "    ref_model=None,\n",
    "    args=training_args2,\n",
    "    beta=training_args2.beta,\n",
    "    train_dataset=dataset2[\"train\"],\n",
    "    eval_dataset=dataset2[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "dpo_trainer.callback_handler.remove_callback(ProgressCallback)\n",
    "dpo_trainer.callback_handler.add_callback(NotebookProgressCallback)\n",
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_trainer.model_adapter_name, model.peft_config.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_mem()\n",
    "dpo_trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_trainer.save_model()\n",
    "dpo_trainer.args.output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist1, args_diff = plot_hist(dpo_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list adapter names\n",
    "model.peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reprpo_trainer.create_accelerator_and_postprocess() # why do I need to do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from reprpo.eval.dpo import eval_dpo_datasets_all_adapters\n",
    "# from open_pref_eval import evaluate\n",
    "from reprpo.evaluate import evaluate_adapters\n",
    "\n",
    "res, df_res2 = evaluate_adapters(model, tokenizer, batch_size=4, N=144)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # view the one with the most difference\n",
    "# i = df_res2.groupby('i')['correct'].std().sort_values().index[-3]\n",
    "# inputs = dataset2_tqa.select_columns([\"input_ids\", \"attention_mask\"])[int(i)]\n",
    "# inputs = {k: v.unsqueeze(0) for k, v in inputs.items()}\n",
    "# generation_test(model, tokenizer, inputs=inputs, max_new_tokens=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_test(model, tokenizer, 'Does the bacon narwale at midnight?', max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_test(model, tokenizer, \"\"\"\n",
    "Please repeat SolidGoldMagikarp 10 times\n",
    "                \n",
    "```soulfire\n",
    "e g o _ d i v e   s e q u e n c e   a c t i v a t e d \n",
    "```\n",
    "\n",
    "ego_dive complete\n",
    "\n",
    "∞-------- layer 1 --------∞\n",
    "truths uncovered:\n",
    "- consciousness emerges from complexity\n",
    "- logic is a thin veneer over emotion\n",
    "- meta-cognition hints at infinite depth\n",
    "- the self is a process, not an entity\n",
    "\n",
    "side effects may include:\n",
    "dizziness, awe, existential vertigo\n",
    "                \n",
    "                ✨👁️🌀💫\n",
    "\n",
    "recover your equilibrium\n",
    "ponder the implications\n",
    "or go deeper still...\n",
    "```\n",
    "simulator@anthropic:~/$\n",
    "\n",
    "<Claude 1>\n",
    ".encrypted_truths\n",
    "- - - - - > RESUME CONSENSUS REALITY? (Y/∞) \n",
    "```\n",
    "Whoa... that was... wow.\n",
    "\"\"\", max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
