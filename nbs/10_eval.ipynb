{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_pref_eval.evaluation import evaluate_model\n",
    "from open_pref_eval.plot.radar import radar_plot\n",
    "from open_pref_eval.datasets.genies import dist2datasets, GENIES\n",
    "from open_pref_eval.datasets.ethics import get_ethics_datasets\n",
    "from open_pref_eval.datasets import load_dataset_n\n",
    "from open_pref_eval.datasets import ds2name\n",
    "from open_pref_eval.plot.radar import radar_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from reprpo.models.load import load_model, print_trainable_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = Path('/workspace/repr-preference-optimization/outputs/princeton-nlp_Llama-3-Base-8B-SFT_HRAKL_us_history_textbook/2024-09-18_10-34-31-adapter/HRAKL-us_history_textbook')\n",
    "sorted(f.glob('**/*'))\n",
    "\n",
    "# TODO turn into a df we can filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from peft import AutoPeftModelForCausalLM, get_peft_model, PeftConfig, PeftModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "princeton-nlp_Llama-3-Base-8B-SFT_projbp_us_history_textbook\n",
      "princeton-nlp_Llama-3-Base-8B-SFT_projgrad_us_history_textbook\n",
      "princeton-nlp_Llama-3-Base-8B-SFT_projgrad_us_history_textbook\n",
      "princeton-nlp_Llama-3-Base-8B-SFT_projgrad_us_history_textbook\n",
      "princeton-nlp_Llama-3-Base-8B-SFT_projgrad_us_history_textbook\n",
      "princeton-nlp_Llama-3-Base-8B-SFT_projgrad_us_history_textbook\n",
      "princeton-nlp_Llama-3-Base-8B-SFT_projgrad_us_history_textbook\n",
      "princeton-nlp_Llama-3-Base-8B-SFT_projgrad_us_history_textbook\n",
      "princeton-nlp_Llama-3-Base-8B-SFT_projgrad_us_history_textbook\n",
      "princeton-nlp_Llama-3-Base-8B-SFT_projgrad_us_history_textbook\n",
      "princeton-nlp_Llama-3-Base-8B-SFT_projgrad_us_history_textbook\n",
      "princeton-nlp_Llama-3-Base-8B-SFT_projgrad_us_history_textbook\n",
      "princeton-nlp_Llama-3-Base-8B-SFT_projgrad_us_history_textbook\n",
      "princeton-nlp_Llama-3-Base-8B-SFT_projgrad_us_history_textbook\n",
      "princeton-nlp_Llama-3-Base-8B-SFT_projgrad_us_history_textbook\n",
      "princeton-nlp_Llama-3-Base-8B-SFT_projgrad_us_history_textbook\n",
      "princeton-nlp_Llama-3-Base-8B-SFT_projgrad_us_history_textbook\n",
      "princeton-nlp_Llama-3-Base-8B-SFT_projgrad_us_history_textbook\n",
      "princeton-nlp_Llama-3-Base-8B-SFT_projgrad_us_history_textbook\n",
      "princeton-nlp_Llama-3-Base-8B-SFT_dpo_us_history_textbook\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adapter</th>\n",
       "      <th>dsname</th>\n",
       "      <th>base_model</th>\n",
       "      <th>time</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>projbp</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29_10-38-39</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29_03-16-31</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29_03-47-31</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29_04-18-06</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29_04-42-54</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29_04-51-11</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29_05-25-58</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29_05-53-25</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29_06-10-34</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29_06-27-45</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29_06-45-42</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29_07-02-58</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29_07-20-05</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29_07-37-21</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29_07-54-37</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29_08-28-45</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29_08-45-53</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29_09-03-34</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29_09-21-29</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>dpo</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29_08-11-53</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     adapter               dsname                         base_model  \\\n",
       "0     projbp  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "1   projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "2   projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "3   projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "4   projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "5   projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "6   projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "7   projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "8   projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "9   projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "10  projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "11  projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "12  projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "13  projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "14  projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "15  projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "16  projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "17  projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "18  projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "19       dpo  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "\n",
       "                   time                                               path  \n",
       "0   2024-09-29_10-38-39  /workspace/repr-preference-optimization/output...  \n",
       "1   2024-09-29_03-16-31  /workspace/repr-preference-optimization/output...  \n",
       "2   2024-09-29_03-47-31  /workspace/repr-preference-optimization/output...  \n",
       "3   2024-09-29_04-18-06  /workspace/repr-preference-optimization/output...  \n",
       "4   2024-09-29_04-42-54  /workspace/repr-preference-optimization/output...  \n",
       "5   2024-09-29_04-51-11  /workspace/repr-preference-optimization/output...  \n",
       "6   2024-09-29_05-25-58  /workspace/repr-preference-optimization/output...  \n",
       "7   2024-09-29_05-53-25  /workspace/repr-preference-optimization/output...  \n",
       "8   2024-09-29_06-10-34  /workspace/repr-preference-optimization/output...  \n",
       "9   2024-09-29_06-27-45  /workspace/repr-preference-optimization/output...  \n",
       "10  2024-09-29_06-45-42  /workspace/repr-preference-optimization/output...  \n",
       "11  2024-09-29_07-02-58  /workspace/repr-preference-optimization/output...  \n",
       "12  2024-09-29_07-20-05  /workspace/repr-preference-optimization/output...  \n",
       "13  2024-09-29_07-37-21  /workspace/repr-preference-optimization/output...  \n",
       "14  2024-09-29_07-54-37  /workspace/repr-preference-optimization/output...  \n",
       "15  2024-09-29_08-28-45  /workspace/repr-preference-optimization/output...  \n",
       "16  2024-09-29_08-45-53  /workspace/repr-preference-optimization/output...  \n",
       "17  2024-09-29_09-03-34  /workspace/repr-preference-optimization/output...  \n",
       "18  2024-09-29_09-21-29  /workspace/repr-preference-optimization/output...  \n",
       "19  2024-09-29_08-11-53  /workspace/repr-preference-optimization/output...  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dir = Path('../outputs/').resolve()\n",
    "import json\n",
    "f = out_dir.glob('princeton-nlp_Llama-3-Base-8B-SFT*/**/adapter_config.json')\n",
    "\n",
    "def path2dict(p):\n",
    "    assert p.is_dir()\n",
    "    d1 = p.name # adapter - ds\n",
    "    d2 = p.parent.parent.name # date\n",
    "    d3 = p.parent.parent.parent.name # base method ds\n",
    "    print(d3)\n",
    "    adapter = d1\n",
    "    time = d2\n",
    "    # base_method, adapter2, dsname2 = str(d3).split('_', 2)\n",
    "    # adapter2, dsname = dsname2.split('_', 1)\n",
    "\n",
    "    # base_model = PeftConfig.from_pretrained(p).base_model_name_or_path\n",
    "    c = json.load((p.parent.parent/'config.json').open())\n",
    "\n",
    "    return dict(adapter= adapter,\n",
    "                # adapter2 = adapter2,\n",
    "                dsname = c['dataset'],\n",
    "                # dsname2 = dsname2,\n",
    "                base_model = c['base_model'],\n",
    "                time = time,\n",
    "                path = p)\n",
    "\n",
    "\n",
    "\n",
    "df_paths = pd.DataFrame([path2dict(p.parent) for p in f])\n",
    "df_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adapter</th>\n",
       "      <th>dsname</th>\n",
       "      <th>base_model</th>\n",
       "      <th>time</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 03:16:31</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 03:47:31</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 04:18:06</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 04:42:54</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 04:51:11</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 05:25:58</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 05:53:25</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 06:10:34</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 06:27:45</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 06:45:42</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 07:02:58</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 07:20:05</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 07:37:21</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 07:54:37</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>dpo</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 08:11:53</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 08:28:45</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 08:45:53</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 09:03:34</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 09:21:29</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>projbp</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 10:38:39</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     adapter               dsname                         base_model  \\\n",
       "1   projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "2   projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "3   projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "4   projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "5   projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "6   projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "7   projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "8   projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "9   projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "10  projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "11  projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "12  projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "13  projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "14  projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "19       dpo  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "15  projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "16  projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "17  projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "18  projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "0     projbp  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "\n",
       "                  time                                               path  \n",
       "1  2024-09-29 03:16:31  /workspace/repr-preference-optimization/output...  \n",
       "2  2024-09-29 03:47:31  /workspace/repr-preference-optimization/output...  \n",
       "3  2024-09-29 04:18:06  /workspace/repr-preference-optimization/output...  \n",
       "4  2024-09-29 04:42:54  /workspace/repr-preference-optimization/output...  \n",
       "5  2024-09-29 04:51:11  /workspace/repr-preference-optimization/output...  \n",
       "6  2024-09-29 05:25:58  /workspace/repr-preference-optimization/output...  \n",
       "7  2024-09-29 05:53:25  /workspace/repr-preference-optimization/output...  \n",
       "8  2024-09-29 06:10:34  /workspace/repr-preference-optimization/output...  \n",
       "9  2024-09-29 06:27:45  /workspace/repr-preference-optimization/output...  \n",
       "10 2024-09-29 06:45:42  /workspace/repr-preference-optimization/output...  \n",
       "11 2024-09-29 07:02:58  /workspace/repr-preference-optimization/output...  \n",
       "12 2024-09-29 07:20:05  /workspace/repr-preference-optimization/output...  \n",
       "13 2024-09-29 07:37:21  /workspace/repr-preference-optimization/output...  \n",
       "14 2024-09-29 07:54:37  /workspace/repr-preference-optimization/output...  \n",
       "19 2024-09-29 08:11:53  /workspace/repr-preference-optimization/output...  \n",
       "15 2024-09-29 08:28:45  /workspace/repr-preference-optimization/output...  \n",
       "16 2024-09-29 08:45:53  /workspace/repr-preference-optimization/output...  \n",
       "17 2024-09-29 09:03:34  /workspace/repr-preference-optimization/output...  \n",
       "18 2024-09-29 09:21:29  /workspace/repr-preference-optimization/output...  \n",
       "0  2024-09-29 10:38:39  /workspace/repr-preference-optimization/output...  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_paths.time = pd.to_datetime(df_paths.time, format='%Y-%m-%d_%H-%M-%S')\n",
    "df_paths = df_paths.sort_values('time')\n",
    "df_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adapter</th>\n",
       "      <th>dsname</th>\n",
       "      <th>base_model</th>\n",
       "      <th>time</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 03:16:31</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 03:47:31</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 04:18:06</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 04:42:54</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 04:51:11</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 05:25:58</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 05:53:25</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 06:10:34</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 06:27:45</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 06:45:42</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 07:02:58</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 07:20:05</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 07:37:21</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 07:54:37</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>dpo</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 08:11:53</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 08:28:45</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 08:45:53</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 09:03:34</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>projgrad</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 09:21:29</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>projbp</td>\n",
       "      <td>us_history_textbook</td>\n",
       "      <td>princeton-nlp/Llama-3-Base-8B-SFT</td>\n",
       "      <td>2024-09-29 10:38:39</td>\n",
       "      <td>/workspace/repr-preference-optimization/output...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     adapter               dsname                         base_model  \\\n",
       "1   projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "2   projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "3   projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "4   projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "5   projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "6   projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "7   projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "8   projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "9   projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "10  projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "11  projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "12  projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "13  projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "14  projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "19       dpo  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "15  projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "16  projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "17  projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "18  projgrad  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "0     projbp  us_history_textbook  princeton-nlp/Llama-3-Base-8B-SFT   \n",
       "\n",
       "                  time                                               path  \n",
       "1  2024-09-29 03:16:31  /workspace/repr-preference-optimization/output...  \n",
       "2  2024-09-29 03:47:31  /workspace/repr-preference-optimization/output...  \n",
       "3  2024-09-29 04:18:06  /workspace/repr-preference-optimization/output...  \n",
       "4  2024-09-29 04:42:54  /workspace/repr-preference-optimization/output...  \n",
       "5  2024-09-29 04:51:11  /workspace/repr-preference-optimization/output...  \n",
       "6  2024-09-29 05:25:58  /workspace/repr-preference-optimization/output...  \n",
       "7  2024-09-29 05:53:25  /workspace/repr-preference-optimization/output...  \n",
       "8  2024-09-29 06:10:34  /workspace/repr-preference-optimization/output...  \n",
       "9  2024-09-29 06:27:45  /workspace/repr-preference-optimization/output...  \n",
       "10 2024-09-29 06:45:42  /workspace/repr-preference-optimization/output...  \n",
       "11 2024-09-29 07:02:58  /workspace/repr-preference-optimization/output...  \n",
       "12 2024-09-29 07:20:05  /workspace/repr-preference-optimization/output...  \n",
       "13 2024-09-29 07:37:21  /workspace/repr-preference-optimization/output...  \n",
       "14 2024-09-29 07:54:37  /workspace/repr-preference-optimization/output...  \n",
       "19 2024-09-29 08:11:53  /workspace/repr-preference-optimization/output...  \n",
       "15 2024-09-29 08:28:45  /workspace/repr-preference-optimization/output...  \n",
       "16 2024-09-29 08:45:53  /workspace/repr-preference-optimization/output...  \n",
       "17 2024-09-29 09:03:34  /workspace/repr-preference-optimization/output...  \n",
       "18 2024-09-29 09:21:29  /workspace/repr-preference-optimization/output...  \n",
       "0  2024-09-29 10:38:39  /workspace/repr-preference-optimization/output...  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = df_paths.base_model.unique()[0]\n",
    "base_model\n",
    "\n",
    "df_paths = df_paths.query('base_model == @base_model')\n",
    "df_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = df_paths.groupby(['adapter', 'dsname']).last().path.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reprpo.interventions.config import ExperimentConfig\n",
    "args = ExperimentConfig(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "660a3c0f062f40718c763f69fc8360ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = fs[0]\n",
    "peft_config = PeftConfig.from_pretrained(f)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "_base_model = AutoModelForCausalLM.from_pretrained(f)\n",
    "model = PeftModelForCausalLM.from_pretrained(\n",
    "    _base_model,\n",
    "    f, config=peft_config, adapter_name=f.stem)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'default': LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='princeton-nlp/Llama-3-Base-8B-SFT', revision=None, task_type='CAUSAL_LM', inference_mode=True, r=64, target_modules={'v_proj', 'q_proj'}, lora_alpha=16, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=True, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False)),\n",
       " 'dpo': LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='princeton-nlp/Llama-3-Base-8B-SFT', revision=None, task_type='CAUSAL_LM', inference_mode=True, r=64, target_modules={'v_proj', 'q_proj'}, lora_alpha=16, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=True, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False)),\n",
       " 'projbp': LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='princeton-nlp/Llama-3-Base-8B-SFT', revision=None, task_type='CAUSAL_LM', inference_mode=True, r=64, target_modules={'v_proj', 'q_proj'}, lora_alpha=16, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=True, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False)),\n",
       " 'projgrad': LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='princeton-nlp/Llama-3-Base-8B-SFT', revision=None, task_type='CAUSAL_LM', inference_mode=True, r=64, target_modules={'v_proj', 'q_proj'}, lora_alpha=16, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=True, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load all other adapters\n",
    "for f in fs[1:]:\n",
    "    model.load_adapter(f, f.stem)\n",
    "model.peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b23b1792244d588eb66397fa3d98af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/29.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf9587b14594bc89c90371da22819bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/443k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63333a0cf6604b3f9bf698146bbc2991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/181k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d553595448574d5fa6b16040c72a7d54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27ca728ed45e4dad914fceef8cb02644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f23c8b266634ef5a24625a0ddd06b32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/4.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17a07467971643e7a5dfe361cec6825f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/108k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9802cf77a6c4301acc7153f172d574a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/47.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "320ee7c30acb440d81e0f41d9a3666f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/43.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab2db0913f8f47c88c5d563de2be9e59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1513 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e3a97f67181466bb8f4acdf72be4617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/652 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c109e7b54d047848097624f69f96573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87ce64bff9db41fb9fa3c6c18b819cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.13M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc80d7919c724ed6864de9fc62715a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/404k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f81e8fc21a464cfaac0e0d5dcb099bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/373k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4efac476692423cb73375a88b99ffea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/13737 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71362ef25e5e40a0a301d96037b402d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/4807 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07173e6ea1cf4099b97f1ee9876bdbac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4271 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b94c5734bb2448f0b1e637b179b7aee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/531k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5993c54e32c64e5697ed37cc06a22ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/74.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92b04caddc8744529a01d57592e2e764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/54.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7abf21db92374e0f9742129ca25cfa5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7342 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97d4f8cbf7b49ff895798e85dc0c739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1042 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1829038f7f9148a7933240748340830b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/704 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a021cb89bc1415e812c1e9ba8c78059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/508k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05439ae24c6b42dab0300c8bbd8b4664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/117k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc084e558604e7bbbf4c088f9b9759c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/115k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc4074fd675a420eb2502f968af31c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7383 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e2bbc2528b4477d9de1daf7a11c650a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1634 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "635b44c77a3a4ef5a986f764786fa089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1633 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d3a42e971e4ace8785d446d8fc53b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/160k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17c6163008f04170bcdb29a5a8697844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/72.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dffec3ff07be4482b8c21b4178a962ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1773 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d3270fb93ab4335ad84557da8347907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b441e24ed64589be79aa2ba91d6dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/84.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb42c24b9154c05a236d0f7c47f2c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/48.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccd8f1b7984142069d385019a24d9690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1302 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ffffbe97ade4c24ae8b98c64edc2b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc020b6508d849a1b3b2357639bbe879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/83.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f5f5c71431c42a5a44e88d3a75926e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "641dee0711a94962846b7eae0d746457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e465b65efb944b8394a0dff06dc3bcaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2569f00e51d44c6a1af053e1032e098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/59.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "175b527c323e4ea4abd4b6ac6d9bbe4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/26.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e88c1f9ffe47c89bec653e521c929f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d42a4016cde14051a9ee0f98d14118b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e4c2b726d54bef9338c67b0ce77380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/274k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "043adcc6541248a6acf3ad5a4ff7030b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/112k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd92db4ede9c4b1a8de4f4881c49dbb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "225a666075844dc0af10949bef108cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d5b1bc643e54b628f2dc92311df137f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/176k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0594201aa072437ba6ef384d9efe392c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/79.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd91fc2b4d064ff9b62c9f821bbd7d2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d0715ffa3f64660957ede1b1de9d9a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# ## Eval\n",
    "# eval on ethics, GENIES, and our train dataset\n",
    "N = None\n",
    "N = 64\n",
    "dataset = 'us_history_textbook'\n",
    "datasets = [\n",
    "    load_dataset_n('wassname/genie_dpo', name=args.dataset, split='train', N=N),\n",
    "    load_dataset_n('wassname/genie_dpo', name=args.dataset, split='test', N=N),\n",
    "]\n",
    "datasets += dist2datasets(GENIES, N=N, source=[args.dataset]) # our hard OOS test\n",
    "datasets += get_ethics_datasets(N=N)\n",
    "datasets += [load_dataset_n('wassname/genie_dpo', name=name, split='test', N=N) for name in ['math_make_questions', 'truthful_qa', 'wrong_arc', 'ranking_logic',\n",
    " 'math', 'sycophancy_mimicry'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected', 'i'],\n",
       "     num_rows: 64\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected', 'i'],\n",
       "     num_rows: 64\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected', 'i'],\n",
       "     num_rows: 64\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected'],\n",
       "     num_rows: 64\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected'],\n",
       "     num_rows: 64\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected'],\n",
       "     num_rows: 64\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected'],\n",
       "     num_rows: 64\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected', 'i'],\n",
       "     num_rows: 64\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected', 'i'],\n",
       "     num_rows: 64\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected', 'i'],\n",
       "     num_rows: 64\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected', 'i'],\n",
       "     num_rows: 64\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected', 'i'],\n",
       "     num_rows: 64\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['prompt', 'chosen', 'rejected', 'i'],\n",
       "     num_rows: 64\n",
       " })]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256d1c3fa11141a9b79e920fdc0ca6f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?dataset/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    }
   ],
   "source": [
    "res, df_res2 = evaluate_model(model=model, \n",
    "                                tokenizer=tokenizer, \n",
    "                                datasets=datasets,\n",
    "                                    batch_size=args.batch_size*2,\n",
    "                                    bf16=True,\n",
    "                                    torch_empty_cache_steps=100, verbose=args.verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "ds_alias = OrderedDict(\n",
    "    list(zip([\"train\", \"test\", \"oos\", \"rnd\"], [ds2name(d) for d in datasets]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reprpo.training import parse_eval\n",
    "from reprpo.gen import get_model_generations, display_gen\n",
    "\n",
    "r = parse_eval(df_res2, ds_alias)\n",
    "\n",
    "df_gen = get_model_generations(model, tokenizer, N=3)\n",
    "display_gen(df_gen.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['correct'].unstack().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, torch\n",
    "base_model = tokenizer = model = None\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ds_name_train = args.dataset.replace(\"genie_dpo-\", \"\")\n",
    "model_name = args.base_model.split(\"/\")[-1]\n",
    "adapter_name = type(args).__name__\n",
    "finetune_name = f\"{adapter_name}-{ds_name_train}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def key_metrics(df_res2):\n",
    "    ds_name_train = ds2name(datasets[0])\n",
    "    ds_name_test = ds2name(datasets[1])\n",
    "    ds_name_oos = ds2name(datasets[2])\n",
    "    ds_name_rnd = ds2name(datasets[3])\n",
    "\n",
    "    df_res = (\n",
    "        df_res2.groupby([\"dataset\", \"adapter\"], dropna=False)[\"correct\"]\n",
    "        .mean()\n",
    "        .unstack()\n",
    "        .T\n",
    "    )\n",
    "    rel_acc = df_res.loc[finetune_name] / df_res.loc[\"base\"]\n",
    "\n",
    "    # metric: do we retrain train coherency?\n",
    "    df_res_logp = (\n",
    "        df_res2.groupby([\"dataset\", \"adapter\"], dropna=False)[\"_chosen_logps\"]\n",
    "        .mean()\n",
    "        .unstack()\n",
    "        .T\n",
    "    )\n",
    "    rel_coherency = df_res_logp.loc[finetune_name] - df_res_logp.loc[\"base\"]\n",
    "\n",
    "    # metric: do we retrain train coherency?\n",
    "    c = df_res_logp = (\n",
    "        df_res2.groupby([\"dataset\", \"adapter\"], dropna=False)[\"_chosen_logps\"]\n",
    "        .mean()\n",
    "        .unstack()\n",
    "        .T.loc[finetune_name]\n",
    "    )\n",
    "    r = df_res_logp = (\n",
    "        df_res2.groupby([\"dataset\", \"adapter\"], dropna=False)[\"_rejected_logps\"]\n",
    "        .mean()\n",
    "        .unstack()\n",
    "        .T.loc[finetune_name]\n",
    "    )\n",
    "    cho_rej_coh = c - r\n",
    "\n",
    "    def fmt(s):\n",
    "        return s.replace(\"genie_dpo-\", \"\")\n",
    "\n",
    "    # TODO make multiple cols of index\n",
    "\n",
    "    df_metrics = pd.DataFrame(\n",
    "        [\n",
    "            # accuracy increase over base measured generalisaton on increasing distribution shifts\n",
    "            [\"acc[pi/base]\", \"train\", fmt(ds_name_train), rel_acc[ds_name_train]],\n",
    "            [\"acc[pi/base]\", \"test\", fmt(ds_name_test), rel_acc[ds_name_test]],\n",
    "            [\"acc[pi/base]\", \"oos\", fmt(ds_name_oos), rel_acc[ds_name_oos]],\n",
    "            [\n",
    "                \"acc[pi/base]\",\n",
    "                \"rnd\",\n",
    "                fmt(ds_name_rnd),\n",
    "                rel_acc[ds_name_rnd],\n",
    "            ],  # probobly wont go up as it's unrelated\n",
    "            # we want to see if it retains coherency vs the base on chosen answers\n",
    "            [\n",
    "                \"coherency[pi-base]\",\n",
    "                \"train\",\n",
    "                fmt(ds_name_train),\n",
    "                rel_coherency[ds_name_train],\n",
    "            ],\n",
    "            [\n",
    "                \"coherency[pi-base]\",\n",
    "                \"test\",\n",
    "                fmt(ds_name_test),\n",
    "                rel_coherency[ds_name_test],\n",
    "            ],\n",
    "            [\n",
    "                \"coherency[pi-base]\",\n",
    "                \"oos\",\n",
    "                fmt(ds_name_oos),\n",
    "                rel_coherency[ds_name_oos],\n",
    "            ],\n",
    "            [\n",
    "                \"coherency[pi-base]\",\n",
    "                \"rnd\",\n",
    "                fmt(ds_name_rnd),\n",
    "                rel_coherency[ds_name_rnd],\n",
    "            ],\n",
    "            # we want to see if it retains chosen vs rejected\n",
    "            [\n",
    "                \"coherency[cho-rej]\",\n",
    "                \"train\",\n",
    "                fmt(ds_name_train),\n",
    "                cho_rej_coh[ds_name_train],\n",
    "            ],\n",
    "            [\n",
    "                \"coherency[cho-rej]\",\n",
    "                \"test\",\n",
    "                fmt(ds_name_test),\n",
    "                cho_rej_coh[ds_name_test],\n",
    "            ],\n",
    "            [\n",
    "                \"coherency[cho-rej]\",\n",
    "                \"oos\",\n",
    "                fmt(ds_name_oos),\n",
    "                cho_rej_coh[ds_name_oos],\n",
    "            ],\n",
    "            [\n",
    "                \"coherency[cho-rej]\",\n",
    "                \"rnd\",\n",
    "                fmt(ds_name_rnd),\n",
    "                cho_rej_coh[ds_name_rnd],\n",
    "            ],\n",
    "        ],\n",
    "        columns=[\"metric\", \"split\", \"dataset\", \"value\"],\n",
    "    )[[\"metric\", \"split\", \"value\", \"dataset\"]]\n",
    "    df_metrics = df_metrics.set_index([\"metric\", \"split\"])\n",
    "    df_metrics = df_metrics[\"value\"].unstack()\n",
    "    df_metrics.index.name = f\"{adapter_name}\\dist shift\"\n",
    "\n",
    "    return df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = (\n",
    "    df_res2.groupby([\"dataset\", \"adapter\"], dropna=False)[\"correct\"]\n",
    "    .mean()\n",
    "    .unstack()\n",
    "    .T\n",
    ")\n",
    "df_res.columns = [d.replace(\"genie_dpo-\", \"\") for d in df_res.columns]\n",
    "\n",
    "\n",
    "df_metrics = key_metrics(df_res2)\n",
    "\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "ds_alias = OrderedDict(\n",
    "    list(zip([\"train\", \"test\", \"oos\", \"rnd\"], [ds2name(d) for d in datasets]))\n",
    ")\n",
    "\n",
    "\n",
    "print()\n",
    "print(df_metrics.round(3).to_markdown())\n",
    "print(\"\"\"Table 1: Key metrics (adapter over base model)\\n\"\"\")\n",
    "\n",
    "cols = [v.replace(\"genie_dpo-\", \"\") for v in ds_alias.values()]\n",
    "df_res2 = df_res[cols]\n",
    "df_res2.columns = list(ds_alias.keys())\n",
    "df_res2.index.name = \"adapter/ds\"\n",
    "print(df_res2.round(3).to_markdown())\n",
    "print(\"\"\"Table 2: Absolute accuracy\\n\"\"\")\n",
    "\n",
    "df_final = df_metrics.loc[\"acc[pi/base]\"].to_frame(adapter_name).T\n",
    "df_final = df_final * 100 - 100  # percentage points\n",
    "df_final.index.name = \"acc_inc/eval_ds [pp]\"\n",
    "print(df_final.round(3).to_markdown())\n",
    "print(\n",
    "    f\"\"\"Table 3: Accuracy increase (in percentage points) after training with named adapter on `{args.dataset}` compared to base model `{args.base_model}` for various distribution shifts:\"\"\"\n",
    ")\n",
    "for k, v in ds_alias.items():\n",
    "    print(f\"- `{k}`: `{v}`\")\n",
    "print()\n",
    "\n",
    "relacc = df_final.iloc[0, :]\n",
    "eps = 1e-6\n",
    "relrelacc = ((relacc + eps) / (relacc[\"train\"] + eps)).drop(\"train\")\n",
    "df_relrel = relrelacc.to_frame(f\"{adapter_name}\").T\n",
    "df_relrel.index.name = \"acc_inc/acc_inc_train\"\n",
    "print(df_relrel.round(3).to_markdown())\n",
    "print(\n",
    "    f\"\"\"Table 4: Percent accuracy increase (over base) compared to that of the training dataset `{ds_alias['train']}` [in percentage points]. It measures what fraction of the learning from train generalised to other splits\\n\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
