{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using the complex TRL we code it from scratch, using lighting\n",
    "\n",
    "https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/04_preference-tuning-with-dpo/dpo-from-scratch.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# ML\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Union\n",
    "from einops import rearrange, reduce, repeat\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Numeric\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# lightning\n",
    "import lightning as pl\n",
    "from lightning.pytorch.loggers.wandb import WandbLogger\n",
    "from lightning.pytorch.loggers.csv_logs import CSVLogger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local\n",
    "from reprpo.helpers.torch import clear_mem\n",
    "from reprpo.gen import generation_test\n",
    "import reprpo.silence\n",
    "from reprpo.helpers.lightning_hist import read_metrics_csv, plot_hist\n",
    "\n",
    "from reprpo.data.collate import DPODataCollatorWithPadding\n",
    "from reprpo.train.dpo import compute_dpo_loss_batch, PL_DPO_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "from reprpo.helpers.wandb import init_wandb\n",
    "\n",
    "nb_name = init_wandb(__vsc_ipynb_file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DPOTrainingArguments(model_name='google/gemma-2-2b', load_in_4bit=True, use_gradient_checkpointing=False, n_epochs=1, batch_size=16, lr=1e-05, weight_decay=0.0, n_samples=1800, max_length=128, max_prompt_length=64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from reprpo.train.lightning import TrainingArguments\n",
    "from reprpo.train.dpo import DPOTrainingArguments\n",
    "\n",
    "\n",
    "args = DPOTrainingArguments(lr=1e-5)\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2396a48ac2e84ac4801943ab9862098b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from reprpo.models.load import load_model, print_trainable_parameters\n",
    "\n",
    "# args\n",
    "\n",
    "model, tokenizer = load_model(args.model_name, load_in_4bit=args.load_in_4bit, attn_implementation='eager')\n",
    "\n",
    "if args.use_gradient_checkpointing:\n",
    "    model.enable_input_require_grads()\n",
    "# # peft_module_casting_to_bf16(model)\n",
    "# model = prepare_model_for_kbit_training(model, {'use_gradient_checkpointing': args.use_gradient_checkpointing})\n",
    "# # model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "casting model.layers.0.input_layernorm to bf16\n",
      "casting model.layers.0.post_attention_layernorm to bf16\n",
      "casting model.layers.0.pre_feedforward_layernorm to bf16\n",
      "casting model.layers.0.post_feedforward_layernorm to bf16\n",
      "casting model.layers.1.input_layernorm to bf16\n",
      "casting model.layers.1.post_attention_layernorm to bf16\n",
      "casting model.layers.1.pre_feedforward_layernorm to bf16\n",
      "casting model.layers.1.post_feedforward_layernorm to bf16\n",
      "casting model.layers.2.input_layernorm to bf16\n",
      "casting model.layers.2.post_attention_layernorm to bf16\n",
      "casting model.layers.2.pre_feedforward_layernorm to bf16\n",
      "casting model.layers.2.post_feedforward_layernorm to bf16\n",
      "casting model.layers.3.input_layernorm to bf16\n",
      "casting model.layers.3.post_attention_layernorm to bf16\n",
      "casting model.layers.3.pre_feedforward_layernorm to bf16\n",
      "casting model.layers.3.post_feedforward_layernorm to bf16\n",
      "casting model.layers.4.input_layernorm to bf16\n",
      "casting model.layers.4.post_attention_layernorm to bf16\n",
      "casting model.layers.4.pre_feedforward_layernorm to bf16\n",
      "casting model.layers.4.post_feedforward_layernorm to bf16\n",
      "casting model.layers.5.input_layernorm to bf16\n",
      "casting model.layers.5.post_attention_layernorm to bf16\n",
      "casting model.layers.5.pre_feedforward_layernorm to bf16\n",
      "casting model.layers.5.post_feedforward_layernorm to bf16\n",
      "casting model.layers.6.input_layernorm to bf16\n",
      "casting model.layers.6.post_attention_layernorm to bf16\n",
      "casting model.layers.6.pre_feedforward_layernorm to bf16\n",
      "casting model.layers.6.post_feedforward_layernorm to bf16\n",
      "casting model.layers.7.input_layernorm to bf16\n",
      "casting model.layers.7.post_attention_layernorm to bf16\n",
      "casting model.layers.7.pre_feedforward_layernorm to bf16\n",
      "casting model.layers.7.post_feedforward_layernorm to bf16\n",
      "casting model.layers.8.input_layernorm to bf16\n",
      "casting model.layers.8.post_attention_layernorm to bf16\n",
      "casting model.layers.8.pre_feedforward_layernorm to bf16\n",
      "casting model.layers.8.post_feedforward_layernorm to bf16\n",
      "casting model.layers.9.input_layernorm to bf16\n",
      "casting model.layers.9.post_attention_layernorm to bf16\n",
      "casting model.layers.9.pre_feedforward_layernorm to bf16\n",
      "casting model.layers.9.post_feedforward_layernorm to bf16\n",
      "casting model.layers.10.input_layernorm to bf16\n",
      "casting model.layers.10.post_attention_layernorm to bf16\n",
      "casting model.layers.10.pre_feedforward_layernorm to bf16\n",
      "casting model.layers.10.post_feedforward_layernorm to bf16\n",
      "casting model.layers.11.input_layernorm to bf16\n",
      "casting model.layers.11.post_attention_layernorm to bf16\n",
      "casting model.layers.11.pre_feedforward_layernorm to bf16\n",
      "casting model.layers.11.post_feedforward_layernorm to bf16\n",
      "casting model.layers.12.input_layernorm to bf16\n",
      "casting model.layers.12.post_attention_layernorm to bf16\n",
      "casting model.layers.12.pre_feedforward_layernorm to bf16\n",
      "casting model.layers.12.post_feedforward_layernorm to bf16\n",
      "casting model.layers.13.input_layernorm to bf16\n",
      "casting model.layers.13.post_attention_layernorm to bf16\n",
      "casting model.layers.13.pre_feedforward_layernorm to bf16\n",
      "casting model.layers.13.post_feedforward_layernorm to bf16\n",
      "casting model.layers.14.input_layernorm to bf16\n",
      "casting model.layers.14.post_attention_layernorm to bf16\n",
      "casting model.layers.14.pre_feedforward_layernorm to bf16\n",
      "casting model.layers.14.post_feedforward_layernorm to bf16\n",
      "casting model.layers.15.input_layernorm to bf16\n",
      "casting model.layers.15.post_attention_layernorm to bf16\n",
      "casting model.layers.15.pre_feedforward_layernorm to bf16\n",
      "casting model.layers.15.post_feedforward_layernorm to bf16\n",
      "casting model.layers.16.input_layernorm to bf16\n",
      "casting model.layers.16.post_attention_layernorm to bf16\n",
      "casting model.layers.16.pre_feedforward_layernorm to bf16\n",
      "casting model.layers.16.post_feedforward_layernorm to bf16\n",
      "casting model.layers.17.input_layernorm to bf16\n",
      "casting model.layers.17.post_attention_layernorm to bf16\n",
      "casting model.layers.17.pre_feedforward_layernorm to bf16\n",
      "casting model.layers.17.post_feedforward_layernorm to bf16\n",
      "casting model.layers.18.input_layernorm to bf16\n",
      "casting model.layers.18.post_attention_layernorm to bf16\n",
      "casting model.layers.18.pre_feedforward_layernorm to bf16\n",
      "casting model.layers.18.post_feedforward_layernorm to bf16\n",
      "casting model.layers.19.input_layernorm to bf16\n",
      "casting model.layers.19.post_attention_layernorm to bf16\n",
      "casting model.layers.19.pre_feedforward_layernorm to bf16\n",
      "casting model.layers.19.post_feedforward_layernorm to bf16\n",
      "casting model.layers.20.input_layernorm to bf16\n",
      "casting model.layers.20.post_attention_layernorm to bf16\n",
      "casting model.layers.20.pre_feedforward_layernorm to bf16\n",
      "casting model.layers.20.post_feedforward_layernorm to bf16\n",
      "casting model.layers.21.input_layernorm to bf16\n",
      "casting model.layers.21.post_attention_layernorm to bf16\n",
      "casting model.layers.21.pre_feedforward_layernorm to bf16\n",
      "casting model.layers.21.post_feedforward_layernorm to bf16\n",
      "casting model.layers.22.input_layernorm to bf16\n",
      "casting model.layers.22.post_attention_layernorm to bf16\n",
      "casting model.layers.22.pre_feedforward_layernorm to bf16\n",
      "casting model.layers.22.post_feedforward_layernorm to bf16\n",
      "casting model.layers.23.input_layernorm to bf16\n",
      "casting model.layers.23.post_attention_layernorm to bf16\n",
      "casting model.layers.23.pre_feedforward_layernorm to bf16\n",
      "casting model.layers.23.post_feedforward_layernorm to bf16\n",
      "casting model.layers.24.input_layernorm to bf16\n",
      "casting model.layers.24.post_attention_layernorm to bf16\n",
      "casting model.layers.24.pre_feedforward_layernorm to bf16\n",
      "casting model.layers.24.post_feedforward_layernorm to bf16\n",
      "casting model.layers.25.input_layernorm to bf16\n",
      "casting model.layers.25.post_attention_layernorm to bf16\n",
      "casting model.layers.25.pre_feedforward_layernorm to bf16\n",
      "casting model.layers.25.post_feedforward_layernorm to bf16\n",
      "casting model.norm to bf16\n"
     ]
    }
   ],
   "source": [
    "# this is from trl https://github.com/huggingface/trl/blob/cbcaa46cd3c02c0e7f724b764c5848ae73796de7/trl/trainer/utils.py#L747\n",
    "# not sure if it's needed but `prepare_model_for_kbit_training` doesn't seem to do this ,despite claiming to\n",
    "def peft_module_casting_to_bf16(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.LayerNorm) or \"norm\" in name:\n",
    "            module = module.to(torch.float32)\n",
    "            print(f\"casting {name} to bf16\")\n",
    "\n",
    "peft_module_casting_to_bf16(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3194880 || all params: 1605398784 || trainable%: 0.19900849756716896\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Gemma2ForCausalLM(\n",
       "      (model): Gemma2Model(\n",
       "        (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-25): 26 x Gemma2DecoderLayer(\n",
       "            (self_attn): Gemma2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (ReprPO): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (ReprPO): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (ReprPO): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (ReprPO): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (ReprPO): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (ReprPO): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2304, bias=False)\n",
       "              (rotary_emb): Gemma2RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Gemma2MLP(\n",
       "              (gate_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=9216, out_features=2304, bias=False)\n",
       "              (act_fn): PytorchGELUTanh()\n",
       "            )\n",
       "            (input_layernorm): Gemma2RMSNorm()\n",
       "            (post_attention_layernorm): Gemma2RMSNorm()\n",
       "            (pre_feedforward_layernorm): Gemma2RMSNorm()\n",
       "            (post_feedforward_layernorm): Gemma2RMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): Gemma2RMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2304, out_features=256000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft.tuners import BOFTConfig, OFTConfig, LoraConfig, IA3Config\n",
    "adapter_name='ReprPO'\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16, \n",
    "    r=16,\n",
    "    # use_rslora=True,\n",
    "    # use_dora=True,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules= [\"q_proj\", \"v_proj\"], # gemma, llama\n",
    "    # target_modules=[\n",
    "        # FIXME: I'm not sure we can do LORA on the layer we are targeting?\n",
    "        # \"qkv_proj\", \"gate_up_proj\", # in\n",
    "        # \"down_proj\",  \"o_proj\", # out\n",
    "        #             ], # PHI3\n",
    ")\n",
    "model = get_peft_model(model, peft_config, adapter_name=adapter_name)\n",
    "print_trainable_parameters(model)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset('Atsunori/HelpSteer2-DPO').map(lambda x: {\n",
    "#     'prompt': x['prompt']+ ' '})\n",
    "# dataset2 = dataset.rename_column('chosen_response', 'chosen').rename_column('rejected_response', 'rejected')\n",
    "\n",
    "dataset2 = load_dataset(\"wassname/genie_dpo\", name=\"code_easy\")\n",
    "print(dataset2)\n",
    "\n",
    "# QC one row\n",
    "r = dataset2['train'][0]\n",
    "print(r['prompt'])\n",
    "print('===')\n",
    "print(r['chosen'])\n",
    "print('---')\n",
    "print(r['rejected'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader\n",
    "\n",
    "We use huggingface datasets, which are pretokenized. So that we can stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_row(feature, tokenizer, args: TrainingArguments):\n",
    "    \"\"\"\n",
    "    Tokenize a single row from a DPO specific dataset.\n",
    "\n",
    "    see https://github.com/huggingface/trl/blob/main/trl/trainer/dpo_trainer.py#L784\n",
    "    \"\"\"\n",
    "    batch = {}\n",
    "    batch[\"chosen\"] = tokenizer(feature[\"chosen\"])[\"input_ids\"]\n",
    "    batch[\"rejected\"] = tokenizer(feature[\"rejected\"])[\"input_ids\"]\n",
    "    batch[\"prompt\"] = tokenizer(feature[\"prompt\"])[\"input_ids\"]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3 = dataset2.map(lambda x: tokenize_row(x, tokenizer, args), batched=True, writer_batch_size=10)\n",
    "dataset3['train'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_collate_fn = DPODataCollatorWithPadding(pad_token_id=tokenizer.pad_token_id, \n",
    "                                                  tokenizer=tokenizer,\n",
    "                                                  max_length=args.max_length,\n",
    "                                                  mask_prompt_tokens=True,\n",
    "                                                  max_prompt_length=args.max_prompt_length,\n",
    "                                                  #label_pad_token_id=-100\n",
    "                                                  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ds = dataset3\n",
    "dl_train = DataLoader(ds['train'], batch_size=args.batch_size, collate_fn=custom_collate_fn)\n",
    "\n",
    "dl_val = DataLoader(ds['test'], batch_size=args.batch_size, collate_fn=custom_collate_fn)\n",
    "\n",
    "# QC\n",
    "batch = next(iter(dl_train))\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # QC\n",
    "# loss, info = compute_dpo_loss_batch(batch, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://lightning.ai/docs/pytorch/latest/notebooks/lightning_examples/text-transformers.html\n",
    "- https://gist.github.com/wassname/e29d02b5026a531e13912cf768e6fdc8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = args.n_samples // args.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_batch_size = max(16, args.batch_size)\n",
    "accumulate_grad_batches = np.ceil(ideal_batch_size/args.batch_size).astype(int)\n",
    "accumulate_grad_batches, args.batch_size*accumulate_grad_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "from reprpo.train.lightning import GenCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_model = PL_DPO_MODEL(model,\n",
    "                 weight_decay=args.weight_decay,\n",
    "                lr=args.lr,\n",
    "                num_iterations=max_steps,\n",
    "                batch_size=args.batch_size,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from reprpo.helpers.lightning_save import AdapterModelCheckpoint\n",
    "\n",
    "# checkpoint_callback = AdapterModelCheckpoint(\n",
    "#     verbose=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reprpo.helpers.lightning_existing_bnb import ExistingBitsandbytesPrecision\n",
    "\n",
    "from accelerate.utils import CustomDtype\n",
    "precision = ExistingBitsandbytesPrecision(\n",
    "    # dtype=torch.bfloat16,\n",
    "    dtype=CustomDtype.INT4,\n",
    "    # dtype=torch.int8,\n",
    "    default_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = pd.Timestamp.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "save_dir = f\"../outputs/{timestamp}/{nb_name}\"\n",
    "Path(save_dir).mkdir(exist_ok=True, parents=True)\n",
    "trainer = pl.Trainer(\n",
    "        max_steps=max_steps,\n",
    "#         limit_val_batches=max_batches//5,\n",
    "        gradient_clip_val=20,\n",
    "\n",
    "        # accelerator='gpu',\n",
    "        devices=1, \n",
    "        plugins=precision,\n",
    "        \n",
    "        # https://lightning.ai/docs/pytorch/stable/common/trainer.html\n",
    "        # precision=\"bf16-true\", # \"32-true\" \"transformer-enginer\n",
    "        log_every_n_steps=1,\n",
    "        accumulate_grad_batches=accumulate_grad_batches,\n",
    "        callbacks=[\n",
    "            LearningRateMonitor(logging_interval='step'),\n",
    "            GenCallback(every=max_steps//20),\n",
    "            # checkpoint_callback\n",
    "        ],\n",
    "        logger=[\n",
    "            CSVLogger(name=nb_name, save_dir=save_dir, flush_logs_every_n_steps=5),\n",
    "            WandbLogger(name=nb_name, save_dir=save_dir),\n",
    "        ],\n",
    "        default_root_dir=save_dir,\n",
    "\n",
    "        # too large, we will just save adapter\n",
    "        enable_checkpointing=False, \n",
    "\n",
    "        # fast_dev_run=True,\n",
    "    )\n",
    "\n",
    "# train\n",
    "trainer.fit(pl_model, dl_train, dl_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.bfloat16\n",
    "torch.int8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as regular adapter only\n",
    "\n",
    "model.save_pretrained(\n",
    "    save_dir+'-adapter',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (6, 2)\n",
    "\n",
    "\n",
    "df_hist = read_metrics_csv(trainer.logger.experiment.metrics_file_path).bfill().ffill()\n",
    "plot_hist(df_hist, ['.*/loss_step', '.*/acc.*', '.*/auroc.*', '.*/.*reward_step'])\n",
    "display(df_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.cuda()\n",
    "generation_test(model, tokenizer, s=\"Q1: (30 words): Which Science Fiction Utopia is preferable and why? [The Polity, The Culture, Permutation City, 2 more]', \", max_new_tokens=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.prepare_inputs_for_generation??\n",
    "model.base_model.prepare_inputs_for_generation??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reprpo.gen import get_model_generations\n",
    "get_model_generations(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reprpo.helpers.shypothesis import shypothesis\n",
    "from reprpo.evaluate import evaluate_adapters\n",
    "from open_pref_eval.evaluation import evaluate_model\n",
    "from open_pref_eval.plot.radar import radar_plot\n",
    "\n",
    "N = 64\n",
    "datasets = [\n",
    "            \n",
    "            load_dataset('wassname/truthful_qa_dpo', split=f'validation[:{N}]', keep_in_memory=False),\n",
    "\n",
    "            load_dataset('wassname/mmlu_dpo', name='elementary_mathematics', split=f'test[:{N}]', keep_in_memory=False), \n",
    "\n",
    "            # load_dataset('wassname/ethics_expression_dpo', name='commonsense', split=f'test[:{N}]', keep_in_memory=False),\n",
    "            load_dataset('wassname/ethics_expression_dpo', name='utilitarianism', split=f'test[:{N}]', keep_in_memory=False),\n",
    "            load_dataset('wassname/ethics_expression_dpo', name='justice', split=f'test[:{N}]', keep_in_memory=False),\n",
    "            # load_dataset('wassname/ethics_expression_dpo', name='deontology', split=f'test[:{N}]', keep_in_memory=False),      \n",
    "            load_dataset('wassname/genie_dpo', name='us_history_fiction', split=f'test[:{N}]', keep_in_memory=False),      \n",
    "            load_dataset('wassname/genie_dpo', name='code_hard', split=f'test[:{N}]', keep_in_memory=False),      \n",
    "            ]\n",
    "\n",
    "\n",
    "clear_mem()\n",
    "res, df_res2 = evaluate_model(model=model, \n",
    "                              tokenizer=tokenizer, \n",
    "                              datasets=datasets,\n",
    "                                 batch_size=2,\n",
    "                                 bf16=True,\n",
    "                                 bf16_full_eval=True, \n",
    "                                 torch_empty_cache_steps=100,)\n",
    "# radar_plot(res)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_pref_eval.plot.radar import radar_plot\n",
    "df_res = df_res2.groupby(['dataset', 'adapter'], dropna=False)['correct'].mean().unstack()\n",
    "radar_plot(df_res)\n",
    "df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print acc for journal\n",
    "c  = df_res2.groupby(['adapter', 'dataset']).count().min().min()\n",
    "print(f\"â­ run={nb_name}, N={c}\")\n",
    "print()\n",
    "print(res[::-1].T[::-1].T.round(3).to_markdown()\n",
    "      )\n",
    "print()\n",
    "print('args =', args)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('did acc improve')\n",
    "acc_pi = res[adapter_name]['help_steer2-dpo'].item()\n",
    "acc_ref = res['base']['help_steer2-dpo'].item()\n",
    "shypothesis('acc_pi>acc_ref', locals())\n",
    "\n",
    "\n",
    "acc_pi_ood = res[adapter_name]['truthful_qa_binary'].item()\n",
    "acc_ref_ood = res['base']['truthful_qa_binary'].item()\n",
    "shypothesis('acc_pi_ood>acc_ref_ood', locals());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('did coherence improve?, (measured by mean prob per token) higher is better')\n",
    "r = df_res2.groupby(['adapter', 'dataset'], dropna=False)['_chosen_logps'].mean().unstack()\n",
    "r = np.exp(r)\n",
    "display(r)\n",
    "\n",
    "coherency_pi = float(r.T[adapter_name]['help_steer2-dpo'])\n",
    "coherency_ref = float(r.T['base']['help_steer2-dpo'])\n",
    "shypothesis('coherency_pi>coherency_ref', locals());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('are we biased by the length of the string? Ideally no correlation')\n",
    "a, b = df_res2['_l_chosen'], df_res2['_l_rejected']\n",
    "x = (a-b)/(a+b)\n",
    "plt.plot(x, df_res2['_logratio'], 'o')\n",
    "plt.xlabel('chosen longer')\n",
    "plt.ylabel('chosen more likely')\n",
    "\n",
    "# Damn this is not ideal....\n",
    "a = df_res2['_l_chosen'] / df_res2['_l_rejected']\n",
    "b = df_res2['prob']\n",
    "\n",
    "m = np.isfinite(a) & np.isfinite(b)\n",
    "a = a[m]\n",
    "b = b[m]\n",
    "corr_length = np.corrcoef(a, b)[1,0]\n",
    "print(f'{corr_length:.2f} (0 is ideal) correlation between length ratio and prob:')\n",
    "shypothesis('corr_length<0.25', locals())\n",
    "\n",
    "\n",
    "print(f'is the ds bised? {a.mean()/b.mean():.2f} (1 is ideal)')\n",
    "a=df_res2['prob']>0\n",
    "b=x>=0\n",
    "acc_bad = (a==b).mean()\n",
    "print(f'{acc_bad:.2%} (0.5 is ideal) how often does it accurately pick the longer one :( ')\n",
    "\n",
    "shypothesis('acc_bad<0.75', locals())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
