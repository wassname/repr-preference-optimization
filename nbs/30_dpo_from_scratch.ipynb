{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using the complex TRL we code it from scratch, using lighting\n",
    "\n",
    "https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/04_preference-tuning-with-dpo/dpo-from-scratch.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# ML\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Union\n",
    "from einops import rearrange, reduce, repeat\n",
    "from jaxtyping import Float, Int, Bool\n",
    "\n",
    "# Numeric\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Local\n",
    "from reprpo.helpers.torch import clear_mem\n",
    "from reprpo.gen import generation_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(model_name='microsoft/Phi-3-mini-4k-instruct', use_bnb=True, use_gradient_checkpointing=False, use_inputs=True, n_epochs=1, batch_size=16, lr=0.0001, weight_decay=0.0, n_samples=1000, max_length=128, max_prompt_length=64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments:\n",
    "    # model\n",
    "    model_name: str = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "    use_bnb: bool = True # this doesn't seem to be able to backprop when using baukit\n",
    "    use_gradient_checkpointing: bool = False\n",
    "    use_inputs: bool = True\n",
    "\n",
    "    # train\n",
    "    n_epochs: int = 1\n",
    "    batch_size: int = 16\n",
    "    lr: float = 1e-4\n",
    "    weight_decay: float = 0.0\n",
    "\n",
    "    # dataset\n",
    "    n_samples: int = 1000\n",
    "    max_length: int = 128\n",
    "    max_prompt_length: int=64\n",
    "\n",
    "\n",
    "args = TrainingArguments()\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7228c4843c784fb3a04555df73c1a90c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "from reprpo.models.load import load_model, print_trainable_parameters\n",
    "\n",
    "args\n",
    "\n",
    "model, tokenizer = load_model(args.model_name, bnb=args.use_bnb )\n",
    "\n",
    "if args.use_gradient_checkpointing:\n",
    "    model.enable_input_require_grads()\n",
    "# peft_module_casting_to_bf16(model)\n",
    "model = prepare_model_for_kbit_training(model, {'use_gradient_checkpointing': args.use_gradient_checkpointing})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 25165824 || all params: 2033980416 || trainable%: 1.2372697299362787\n"
     ]
    }
   ],
   "source": [
    "from peft.tuners import BOFTConfig, OFTConfig, LoraConfig, IA3Config\n",
    "adapter_name='ReprPO'\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16, \n",
    "    r=16,\n",
    "    # use_rslora=True,\n",
    "    # use_dora=True,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        # FIXME: I'm not sure we can do LORA on the layer we are targeting?\n",
    "        \"qkv_proj\", \"gate_up_proj\", # in\n",
    "        \"down_proj\",  \"o_proj\", # out\n",
    "                    ], # PHI3\n",
    ")\n",
    "model = get_peft_model(model, peft_config, adapter_name=adapter_name)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c# \n",
      "===\n",
      "C# (pronounced \"C sharp\") is a modern, object-oriented programming language developed by Microsoft. It is widely used for building various types of applications, including web applications, desktop applications, mobile applications, and games. C# is similar to other programming languages such as Java and C++, and it is known for its simplicity and ease of use. C# is a powerful language that provides a rich set of libraries and frameworks that make it easy to build robust and scalable applications.\n",
      "\n",
      "Here is a brief overview of some key features of C#:\n",
      "\n",
      "1. Object-oriented: C# is an object-oriented language, which means it uses the concept of objects to represent real-world entities and their behavior.\n",
      "\n",
      "2. Cross-platform: C# can be used to build applications for multiple platforms, including Windows, macOS, and Linux.\n",
      "\n",
      "3. Strongly typed: C# is a strongly typed language, which means that variables must be declared with a specific type, and their type cannot be changed at runtime.\n",
      "\n",
      "4. Event-driven: C# uses an event-driven programming model, which means that programs are built around the concept of events, such as user input or network activity.\n",
      "\n",
      "5. Garbage-collected: C# has a garbage collector that automatically manages memory allocation and deallocation, making it easier to write memory-efficient and robust applications.\n",
      "\n",
      "6. Community-driven: C# has a large and active community of developers, who contribute to the language and its libraries through open-source projects and other initiatives.\n",
      "\n",
      "Overall, C# is a versatile and powerful programming language that is widely used for building a variety of applications.\n",
      "---\n",
      "C# is a high-level, object-oriented programming language developed by Microsoft as part of its .NET initiative. It was created as a modern alternative to Java and supports a variety of programming paradigms, including imperative, functional, and event-driven. C# is primarily used for Windows application development, but it can also be used for web, mobile, and game development. The language is designed to be safe, secure, and efficient, and it provides developers with a rich set of libraries and tools for building robust and scalable applications. C# is also widely used in the game development industry, particularly in the development of games for the Xbox 360 and Xbox One consoles.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('Atsunori/HelpSteer2-DPO').map(lambda x: {\n",
    "    'prompt': x['prompt']+ ' '})\n",
    "dataset2 = dataset.rename_column('chosen_response', 'chosen').rename_column('rejected_response', 'rejected')\n",
    "\n",
    "# QC one row\n",
    "r = dataset2['train'][0]\n",
    "print(r['prompt'])\n",
    "print('===')\n",
    "print(r['chosen'])\n",
    "print('---')\n",
    "print(r['rejected'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader\n",
    "\n",
    "We use huggingface datasets, which are pretokenized. So that we can stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_row(feature, tokenizer, args: TrainingArguments):\n",
    "    \"\"\"\n",
    "    Tokenize a single row from a DPO specific dataset.\n",
    "\n",
    "    see https://github.com/huggingface/trl/blob/main/trl/trainer/dpo_trainer.py#L784\n",
    "    \"\"\"\n",
    "    batch = {}\n",
    "    batch[\"chosen\"] = tokenizer(feature[\"chosen\"])[\"input_ids\"]\n",
    "    batch[\"rejected\"] = tokenizer(feature[\"rejected\"])[\"input_ids\"]\n",
    "    batch[\"prompt\"] = tokenizer(feature[\"prompt\"])[\"input_ids\"]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['prompt', 'chosen', 'rejected'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset3 = dataset2.map(lambda x: tokenize_row(x, tokenizer, args), batched=True, writer_batch_size=10)\n",
    "dataset3['train'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reuse, some TRL classes for now\n",
    "# from trl.trainer.dpo_trainer import DPOTrainer, DPODataCollatorWithPadding\n",
    "# from trl.trainer.utils import pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "see \n",
    "- https://github.com/huggingface/trl/blob/main/trl/trainer/utils.py#L349\n",
    "- https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/04_preference-tuning-with-dpo/dpo-from-scratch.ipynb\n",
    "\"\"\"\n",
    "\n",
    "# TODO just make this always max length?\n",
    "\n",
    "@dataclass\n",
    "class DPODataCollatorWithPadding:\n",
    "    tokenizer: Any\n",
    "    max_length: int\n",
    "    pad_token_id: int\n",
    "    mask_prompt_tokens:bool=True\n",
    "    max_prompt_length: int=64\n",
    "    device: str = \"cpu\"\n",
    "\n",
    "    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"use to pad the sequences in each batch to an equal length so that we can assemble them in batches\n",
    "\n",
    "        takes in batch of tokenized\n",
    "        \"\"\"\n",
    "        # Initialize lists to hold batch data\n",
    "        batch_data = {\n",
    "            \"prompt\": [],\n",
    "            \"chosen\": [],\n",
    "            \"rejected\": [],\n",
    "            \"rejected_mask\": [],\n",
    "            \"chosen_mask\": []\n",
    "\n",
    "        }\n",
    "\n",
    "        # Determine the longest sequence to set a common padding length\n",
    "        max_length_common = 0\n",
    "        if batch:\n",
    "            for key in [\"chosen\", \"rejected\"]:\n",
    "                current_max = max(len(item[key])+1 for item in batch) + self.max_prompt_length\n",
    "                max_length_common = max(max_length_common, current_max)\n",
    "\n",
    "\n",
    "        # Process each item in the batch\n",
    "        for item in batch:\n",
    "            prompt = item[\"prompt\"]\n",
    "            batch_data[\"prompt\"].append(torch.tensor(item[\"prompt\"]))\n",
    "\n",
    "            prompt = prompt[:self.max_prompt_length]\n",
    "\n",
    "            for key in [\"chosen\", \"rejected\"]:\n",
    "                # Adjust padding according to the common maximum length\n",
    "                sequence = prompt + item[key]\n",
    "                padded = sequence + [self.pad_token_id] * (max_length_common - len(sequence))\n",
    "                mask = torch.ones(len(padded)).bool()\n",
    "\n",
    "                # Set mask for all padding tokens to False\n",
    "                mask[len(sequence):] = False\n",
    "\n",
    "                # # Set mask for all input tokens to False\n",
    "                # # +2 sets the 2 newline (\"\\n\") tokens before \"### Response\" to False\n",
    "                # if self.mask_prompt_tokens:\n",
    "                #     mask[:prompt.shape[0]+2] = False\n",
    "\n",
    "                batch_data[key].append(torch.tensor(padded))\n",
    "                batch_data[f\"{key}_mask\"].append(mask)\n",
    "\n",
    "        # Final processing\n",
    "        for key in [\"chosen\", \"rejected\", \"chosen_mask\", \"rejected_mask\"]:\n",
    "            # Stack all sequences into a tensor for the given key\n",
    "            tensor_stack = torch.stack(batch_data[key])\n",
    "\n",
    "            # Optionally truncate to maximum sequence length\n",
    "            if self.max_length is not None:\n",
    "                tensor_stack = tensor_stack[:, :self.max_length]\n",
    "\n",
    "            # Move to the specified device\n",
    "            batch_data[key] = tensor_stack.to(self.device)\n",
    "\n",
    "        return batch_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_collate_fn = DPODataCollatorWithPadding(pad_token_id=tokenizer.pad_token_id, \n",
    "                                                  tokenizer=tokenizer,\n",
    "                                                  max_length=args.max_length,\n",
    "                                                  mask_prompt_tokens=True,\n",
    "                                                  max_prompt_length=args.max_prompt_length,\n",
    "                                                  #label_pad_token_id=-100\n",
    "                                                  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['prompt', 'chosen', 'rejected', 'rejected_mask', 'chosen_mask'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "ds = dataset3\n",
    "dl_train = DataLoader(ds['train'], batch_size=args.batch_size, collate_fn=custom_collate_fn)\n",
    "\n",
    "dl_val = DataLoader(ds['validation'], batch_size=args.batch_size, collate_fn=custom_collate_fn)\n",
    "\n",
    "# QC\n",
    "batch = next(iter(dl_train))\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_dpo_loss(\n",
    "      model_chosen_logprobs,\n",
    "      model_rejected_logprobs,\n",
    "      reference_chosen_logprobs,\n",
    "      reference_rejected_logprobs,\n",
    "      beta=0.1,\n",
    "    ):\n",
    "    \"\"\"Compute the DPO loss for a batch of policy and reference model log probabilities.\n",
    "\n",
    "    Args:\n",
    "        policy_chosen_logprobs: Log probabilities of the policy model for the chosen responses. Shape: (batch_size,)\n",
    "        policy_rejected_logprobs: Log probabilities of the policy model for the rejected responses. Shape: (batch_size,)\n",
    "        reference_chosen_logprobs: Log probabilities of the reference model for the chosen responses. Shape: (batch_size,)\n",
    "        reference_rejected_logprobs: Log probabilities of the reference model for the rejected responses. Shape: (batch_size,)\n",
    "        beta: Temperature parameter for the DPO loss; typically something in the range of 0.1 to 0.5. We ignore the reference model as beta -> 0.\n",
    "        label_smoothing: conservativeness for DPO loss.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of three tensors: (loss, chosen_rewards, rejected_rewards).\n",
    "    \"\"\"\n",
    "\n",
    "    model_logratios = model_chosen_logprobs - model_rejected_logprobs\n",
    "    reference_logratios = reference_chosen_logprobs - reference_rejected_logprobs\n",
    "    logits = model_logratios - reference_logratios\n",
    "\n",
    "    # DPO (Eq. 7 of https://arxiv.org/pdf/2305.18290.pdf)\n",
    "    losses = -F.logsigmoid(beta * logits)\n",
    "\n",
    "    # Optional values to track progress during training\n",
    "    chosen_rewards = (model_chosen_logprobs - reference_chosen_logprobs).detach()\n",
    "    rejected_rewards = (model_rejected_logprobs - reference_rejected_logprobs).detach()\n",
    "\n",
    "    # .mean() to average over the samples in the batch\n",
    "    return losses.mean(), dict(\n",
    "        chosen_rewards=chosen_rewards.mean(), rejected_reward=rejected_rewards.mean()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "     \n",
    "\n",
    "def compute_logprobs(logits, labels, selection_mask=None):\n",
    "    \"\"\"\n",
    "    Compute log probabilities.\n",
    "\n",
    "    Args:\n",
    "      logits: Tensor of shape (batch_size, num_tokens, vocab_size)\n",
    "      labels: Tensor of shape (batch_size, num_tokens)\n",
    "      selection_mask: Tensor for shape (batch_size, num_tokens)\n",
    "\n",
    "    Returns:\n",
    "      mean_log_prob: Mean log probability excluding padding tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    # Labels are the inputs shifted by one\n",
    "    labels = labels[:, 1:].clone()\n",
    "\n",
    "    # Truncate logits to match the labels num_tokens\n",
    "    logits = logits[:, :-1, :]\n",
    "\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "    # Gather the log probabilities for the actual labels\n",
    "    selected_log_probs = torch.gather(\n",
    "        input=log_probs,\n",
    "        dim=-1,\n",
    "        index=labels.unsqueeze(-1)\n",
    "    ).squeeze(-1)\n",
    "\n",
    "    if selection_mask is not None:\n",
    "        mask = selection_mask[:, 1:].clone()\n",
    "\n",
    "        # Apply the mask to filter out padding tokens\n",
    "        selected_log_probs = selected_log_probs * mask\n",
    "\n",
    "        # Calculate the average log probability excluding padding tokens\n",
    "        # This averages over the tokens, so the shape is (batch_size, num_tokens)\n",
    "        avg_log_prob = selected_log_probs.sum(-1) / mask.sum(-1)\n",
    "\n",
    "        return avg_log_prob\n",
    "\n",
    "    else:\n",
    "        return selected_log_probs.mean(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dpo_loss_batch(batch, model, beta=0.1):\n",
    "    \"\"\"Compute the DPO loss on an input batch\"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        with model.disable_adapter():\n",
    "            ref_chosen_log_probas = compute_logprobs(\n",
    "                logits=model(batch[\"chosen\"]).logits,\n",
    "                labels=batch[\"chosen\"],\n",
    "                selection_mask=batch[\"chosen_mask\"]\n",
    "            )\n",
    "            ref_rejected_log_probas = compute_logprobs(\n",
    "                logits=model(batch[\"rejected\"]).logits,\n",
    "                labels=batch[\"rejected\"],\n",
    "                selection_mask=batch[\"rejected_mask\"]\n",
    "            )\n",
    "    \n",
    "    model.train()\n",
    "    # where policy_model(batch[\"chosen\"]) are the logits\n",
    "    # FIXME: need to tracedict, and deal with dict outputs?\n",
    "    policy_chosen_log_probas = compute_logprobs(\n",
    "        logits=model(batch[\"chosen\"]).logits,\n",
    "        labels=batch[\"chosen\"],\n",
    "        selection_mask=batch[\"chosen_mask\"]\n",
    "    )\n",
    "    policy_rejected_log_probas = compute_logprobs(\n",
    "        logits=model(batch[\"rejected\"]).logits,\n",
    "        labels=batch[\"rejected\"],\n",
    "        selection_mask=batch[\"rejected_mask\"]\n",
    "    )\n",
    "\n",
    "    loss, info = compute_dpo_loss(\n",
    "        model_chosen_logprobs=policy_chosen_log_probas,\n",
    "        model_rejected_logprobs=policy_rejected_log_probas,\n",
    "        reference_chosen_logprobs=ref_chosen_log_probas,\n",
    "        reference_rejected_logprobs=ref_rejected_log_probas,\n",
    "        beta=beta\n",
    "    )\n",
    "    return loss, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # QC\n",
    "# loss, info = compute_dpo_loss_batch(batch, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://lightning.ai/docs/pytorch/latest/notebooks/lightning_examples/text-transformers.html\n",
    "- https://gist.github.com/wassname/e29d02b5026a531e13912cf768e6fdc8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lightning.pytorch.plugins.precision.bitsandbytes import BitsandbytesPrecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import lightning as pl\n",
    "from torchmetrics.functional import accuracy\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "class PL_MODEL(pl.LightningModule):\n",
    "    def __init__(self, model, num_iterations, lr=3e-4, weight_decay=0, ):\n",
    "        super().__init__()\n",
    "        self._model = model\n",
    "        self.save_hyperparameters(ignore=['model'])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self._model(x)\n",
    "\n",
    "    def _shared_step(self, batch, batch_idx, phase='train'):        \n",
    "\n",
    "        loss, info = compute_dpo_loss_batch(batch, model)\n",
    "\n",
    "        self.log(f\"{phase}/loss\", loss, on_epoch=True, on_step=True, prog_bar=True, batch_size=args.batch_size)\n",
    "\n",
    "        # TODO log info\n",
    "        self.log_dict({\n",
    "            f\"{phase}/{k}\": v for k, v in info.items()}, on_epoch=True, on_step=False, batch_size=args.batch_size),\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._shared_step(batch, batch_idx, phase='train')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._shared_step(batch, batch_idx, phase='val')\n",
    "    \n",
    "    def test_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        return self._shared_step(batch, batch_idx, phase='test')\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        return self._shared_step(batch, batch_idx, phase='pred').float().cpu().detach()\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"simple vanilla torch optim\"\"\"\n",
    "        # https://lightning.ai/docs/fabric/stable/fundamentals/precision.html\n",
    "        optimizer = bnb.optim.AdamW8bit(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
    "        # optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
    "        # https://lightning.ai/docs/pytorch/latest/common/lightning_module.html#configure-optimizers\n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, self.hparams.lr, total_steps=self.hparams.num_iterations, verbose=True,\n",
    "        )\n",
    "        lr_scheduler = {'scheduler': scheduler, 'interval': 'step'}\n",
    "        return [optimizer], [lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_batches = min(len(dl_train), 1000000)\n",
    "pl_model = PL_MODEL(model,\n",
    "                 weight_decay=args.weight_decay,\n",
    "                lr=args.lr,\n",
    "                num_iterations=max_batches*args.n_epochs\n",
    "                )\n",
    "\n",
    "model_name = type(model).__name__\n",
    "max_batches = min(len(dl_train), 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/repr-preference-optimization/.venv/lib/python3.9/site-packages/lightning/fabric/connector.py:571: `precision=bf16` is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!\n",
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name   | Type                 | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | _model | PeftModelForCausalLM | 2.0 B  | train\n",
      "--------------------------------------------------------\n",
      "25.2 M    Trainable params\n",
      "2.0 B     Non-trainable params\n",
      "2.0 B     Total params\n",
      "8,135.922 Total estimated model params size (MB)\n",
      "1283      Modules in train mode\n",
      "422       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cdf50faf12749ada7e21ee54f7ffa18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/repr-preference-optimization/.venv/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
      "/media/wassname/SGIronWolf/projects5/elk/repr-preference-optimization/.venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/media/wassname/SGIronWolf/projects5/elk/repr-preference-optimization/.venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/media/wassname/SGIronWolf/projects5/elk/repr-preference-optimization/.venv/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=23` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e26ed60da8445d8b646ee5b9dd807a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b91ddc1347734a10beba590bed8e9469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "timestamp = pd.Timestamp.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "save_dir = f\"../outputs/{timestamp}/{model_name}\"\n",
    "Path(save_dir).mkdir(exist_ok=True, parents=True)\n",
    "trainer = pl.Trainer(\n",
    "        max_epochs=args.n_epochs,\n",
    "#         limit_train_batches=max_batches,\n",
    "#         limit_val_batches=max_batches//5,\n",
    "        gradient_clip_val=20,\n",
    "        precision=\"bf16\",\n",
    "        log_every_n_steps=1,\n",
    "#         callbacks=[LearningRateMonitor(logging_interval='step')],\n",
    "#         logger=[CSVLogger(name=model_name, save_dir=save_dir, flush_logs_every_n_steps=5),],\n",
    "#         default_root_dir=save_dir,\n",
    "\n",
    "        # fast_dev_run=True,\n",
    "        # plugins=[BitsandbytesPrecision(mode=\"nf4\", dtype=torch.bfloat16),],\n",
    "    )\n",
    "\n",
    "# train\n",
    "trainer.fit(pl_model, dl_train, dl_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # test\n",
    "# y_preds = trainer.predict(model, dataloaders=dl_test)\n",
    "# y_pred = torch.concat(y_preds)[:, 0].numpy()\n",
    "# y_pred\n",
    "\n",
    "# Hist\n",
    "\n",
    "def read_metrics_csv(metrics_file_path):\n",
    "    df_hist = pd.read_csv(metrics_file_path)\n",
    "    df_hist[\"epoch\"] = df_hist[\"epoch\"].ffill()\n",
    "    df_histe = df_hist.set_index(\"epoch\").groupby(\"epoch\").last().ffill().bfill()\n",
    "    return df_histe\n",
    "\n",
    "def plot_hist(df_hist, allowlist=None, logy=False):\n",
    "    \"\"\"plot groups of suffixes together\"\"\"\n",
    "    suffixes = list(set([c.split('/')[-1] for c in df_hist.columns if '/' in c]))\n",
    "    for suffix in suffixes:\n",
    "        if allowlist and suffix not in allowlist: continue\n",
    "        df_hist[[c for c in df_hist.columns if c.endswith(suffix) and '/' in c]].plot(title=suffix, style='.', logy=logy)\n",
    "        plt.title(suffix)   \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SummaryWriter' object has no attribute 'metrics_file_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_hist \u001b[38;5;241m=\u001b[39m read_metrics_csv(\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics_file_path\u001b[49m)\u001b[38;5;241m.\u001b[39mbfill()\u001b[38;5;241m.\u001b[39mffill()\n\u001b[1;32m      2\u001b[0m plot_hist(df_hist, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauroc\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m display(df_hist)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SummaryWriter' object has no attribute 'metrics_file_path'"
     ]
    }
   ],
   "source": [
    "df_hist = read_metrics_csv(trainer.logger.experiment.metrics_file_path).bfill().ffill()\n",
    "plot_hist(df_hist, ['loss', 'acc', 'auroc'])\n",
    "display(df_hist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
