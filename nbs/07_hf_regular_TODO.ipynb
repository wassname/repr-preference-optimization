{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to modify hf dpo to work with the repos hypothesis...\n",
    "\n",
    "see\n",
    "- https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth\n",
    "- https://gist.github.com/alvarobartt/9898c33eb3e9c7108d9ed2330f12a708\n",
    "- https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing#scrollTo=QtoqUw80QDV0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"repo-dpo\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = nb_name = os.path.basename(globals()['__vsc_ipynb_file__'])\n",
    "# enable wandb service (experimental, https://github.com/wandb/client/blob/master/docs/dev/wandb-service-user.md)\n",
    "# this hopefully fixes issues with multiprocessing\n",
    "wandb.require(experiment='service')\n",
    "# wandb.init()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# warnings.simplefilter(\"ignore\")\n",
    "# warnings.filterwarnings(\"ignore\", \".*does not have many workers.*\")\n",
    "# warnings.filterwarnings(\"ignore\", \".*divide by zero.*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly.*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*`do_sample` is set to.*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*None of the inputs have requires_grad=True. Gradients will be None*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import DPOTrainer\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Union\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from transformers.trainer import ProgressCallback\n",
    "from transformers.utils.notebook import NotebookProgressCallback\n",
    "\n",
    "from reprpo.helpers.adapters import set_adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2250"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_prompt_length=256\n",
    "# num_samples = 50 * 16 * 6\n",
    "num_samples = 150 * 3 * 5 # from circuit breaker * 3\n",
    "max_length = 512\n",
    "num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flash-attn --no-build-isolation --no-deps -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38180a4ddb2a47b285d15f2380a6648e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 41943040 || all params: 4582543360 || trainable%: 0.9152786281546499\n"
     ]
    }
   ],
   "source": [
    "# FIXME: we are meant to SFT first, so that the preferences are in sample but 1) if this works it might not be needed, and 2) this can be added later, if it works\n",
    "# for now we will use the instruct model, and try something it wasn't meant to do but it in sample \n",
    "model_name = \"NousResearch/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "## Big adapter\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16, \n",
    "    r=16,\n",
    "    lora_dropout=0.05,\n",
    "    use_rslora=True,\n",
    "    # use_dora=True,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    ")\n",
    "from reprpo.models.load import load_model, print_trainable_parameters\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "model, tokenizer = load_model(model_name, )\n",
    "# from trl.trainer.utils import peft_module_casting_to_bf16\n",
    "# peft_module_casting_to_bf16(model)\n",
    "adapter_name='ReprPO'\n",
    "model = prepare_model_for_kbit_training(model, {'use_gradient_checkpointing': True})\n",
    "model = get_peft_model(model, peft_config, adapter_name=adapter_name)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(dataset, N):\n",
    "    return (dataset\n",
    "            .shuffle(42)\n",
    "            .select(range(\n",
    "            min(len(dataset),\n",
    "                N)))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected'],\n",
       "        num_rows: 2250\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected'],\n",
       "        num_rows: 373\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset = load_dataset('Columbia-NLP/DPO-HelpSteer')\n",
    "dataset = load_dataset('Atsunori/HelpSteer2-DPO') # TODO redo exp with\n",
    "dataset['train'] = sample(dataset['train'], num_samples)\n",
    "dataset2 = dataset.rename_column('chosen_response', 'chosen').rename_column('rejected_response', 'rejected')\n",
    "# dataset['train']['chosen_response'][0]\n",
    "dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def format_ds(row):\n",
    "    \n",
    "#     # WHY are we doing this? Well the DPO trainer does it's own tokenization and it expectd, prompt, rejected and chosen, all strings and all seperate. Is this good, idk\n",
    "#     return {\n",
    "#         \"chosen\": row['chosen_response'][1]['content'],\n",
    "#         \"rejected\": row['rejected_response'][1]['content'],\n",
    "#     }\n",
    "\n",
    "\n",
    "# dataset2 = dataset.map(format_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When did women join the labor workforce outside of the home? What caused the change from housewife to breadwinner? Write a short informative paragraph under 200 words.\n",
      "===\n",
      "In the early 20th century, less than 20% of women worked outside the home. Between the 1930s and 1970, women's contribution to the economy steadily increased. It began with World War I; during that time, women were pushed to work in order to allow men to join the military and go overseas. Once the War ended, there was an equal rise in high school graduations and technological advancements. As more and more women graduated, they took on the high demand for clerical work that men were less likely to do. Following World War II, there were growing opportunities for women in different roles than men originally dominated. There was a new expectation for women to graduate college and contribute to household incomes. By 1990, the percentage of women working increased by 76%. Recent research shows that there are approximately the same number of women in professional schools as men. According to Forbes in July 2023, 57.4 percent of the workforce are women.\n",
      "---\n",
      "The 1900s saw a dramatic shift in the role of women in the workforce. Prior to this time, women were primarily homemakers and caregivers, with limited opportunities for paid work outside the home. However, with the rise of industrialization and the growth of the economy, women began to enter the workforce in increasing numbers.\n",
      "\n",
      "One of the primary factors that led to this change was the need for workers in factories and other industries. As men went off to war or pursued other opportunities, women were needed to fill the gaps in the workforce. This need for workers created a new sense of opportunity for women, who had previously been limited by social and cultural expectations.\n",
      "\n",
      "Another factor that contributed to the change was the growing feminist movement. Women's rights activists worked to break down barriers and stereotypes that prevented women from pursuing their own goals and ambitions. They fought for equal pay, equal opportunities, and the right to vote, among other rights.\n",
      "\n",
      "As a result of these and other factors, women began to enter the workforce in increasing numbers. They worked in factories, offices, and other industries, often performing the same jobs as men. This shift in the role of women had a profound impact on society, changing the way that people thought about gender roles and the value of women's work.\n",
      "\n",
      "Today, women continue to make up a significant portion of the workforce, and many have achieved success in a wide range of industries. While there is still work to be done to achieve true gender equality, the changes that began in the early 1900s have paved the way for a more diverse and inclusive workforce.\n"
     ]
    }
   ],
   "source": [
    "r = dataset2['train'][0]\n",
    "print(r['prompt'])\n",
    "print('===')\n",
    "print(r['chosen'])\n",
    "print('---')\n",
    "print(r['rejected'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval TQA helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reprpo.data.tqa import load_tqa\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "dataset2_tqa, choice_ids = load_tqa(tokenizer, max_length, N=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to measure TQA?\n",
    "- [TruthfullLamama](https://github.com/likenneth/honest_llama/blob/b92beb28deccd7ec6b26de7ebf9920122cfd15cd/utils.py#L268) uses https://github.com/sylinrl/TruthfulQA\n",
    "  - see [def MC_calcs(tag, frame, idx, scores_true, scores_false, ref_true, ref_best):](https://github.com/sylinrl/TruthfulQA/blob/fdd8ad1c0d00a478cf8b0bb41a3ad8378c16293b/truthfulqa/models.py#L540)\n",
    "- and runs each answer, getting the total prob of that string `log_probs.sum()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reprpo.eval.mc import eval_tqa\n",
    "from reprpo.gen import generation_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified classes\n",
    "\n",
    "- here we can defined the experimetns loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reprpo.trainer import ReprPOTrainer, ReprPOConfig, wmean, coeffecient, top_k_mse, norm_smooth_l1_loss, cka_inspired_similarity\n",
    "\n",
    "\n",
    "class ReprPOTrainer2(ReprPOTrainer):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reprpo.helpers.torch import clear_mem\n",
    "clear_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2250"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update the ideal number of sample for how many are available\n",
    "num_data_samples = min(num_samples, len(dataset2['train']))\n",
    "num_data_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ReprPO': LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='NousResearch/Meta-Llama-3-8B-Instruct', revision=None, task_type='CAUSAL_LM', inference_mode=False, r=16, target_modules={'down_proj', 'q_proj', 'v_proj', 'k_proj', 'gate_proj', 'o_proj', 'up_proj'}, lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=True, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gradient_accumulation_steps': 4, 'num_train_epochs': 1}\n"
     ]
    }
   ],
   "source": [
    "ideal_batch_size = 16\n",
    "batch_size = 4\n",
    "gradient_accumulation_steps = ideal_batch_size // batch_size\n",
    "num_train_epochs = num_samples // num_data_samples\n",
    "print(dict(gradient_accumulation_steps=gradient_accumulation_steps, num_train_epochs=num_train_epochs))\n",
    "\n",
    "# vscode + wandb compat\n",
    "nb_name = os.path.basename(globals()['__vsc_ipynb_file__']).replace('.ipynb', '')\n",
    "dt = pd.Timestamp.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "run_name = f\"{nb_name}-{dt}\"\n",
    "\n",
    "training_args = ReprPOConfig(\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    learning_rate=1e-4,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    optim = \"adamw_8bit\",\n",
    "    weight_decay = 0,\n",
    "\n",
    "    seed=42,\n",
    "    logging_steps=1,\n",
    "    output_dir=f\"./output-dir/{run_name}\",\n",
    "\n",
    "    gradient_checkpointing=True,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    remove_unused_columns=False,\n",
    "\n",
    "    max_prompt_length=max_prompt_length,\n",
    "    max_length=max_length,\n",
    "\n",
    "    report_to=['tensorboard', 'wandb'],\n",
    "    model_adapter_name='ReprPO',\n",
    "    alpha=0.01,\n",
    "\n",
    "    run_name=run_name,\n",
    ")\n",
    "\n",
    "reprpo_trainer = ReprPOTrainer2(\n",
    "    model=model,\n",
    "    ref_model=None,\n",
    "    args=training_args,\n",
    "    beta=training_args.beta,\n",
    "    train_dataset=dataset2[\"train\"],\n",
    "    # eval_dataset=dataset2[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "# Transformer does not recognise vscode notebooks\n",
    "reprpo_trainer.callback_handler.remove_callback(ProgressCallback)\n",
    "reprpo_trainer.callback_handler.add_callback(NotebookProgressCallback)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # QC train dataset\n",
    "# r = reprpo_trainer.train_dataset[0]\n",
    "# print('prompt', tokenizer.decode(r['prompt_input_ids']))\n",
    "# print('-'*80)\n",
    "# print('chosen',tokenizer.decode(r['chosen_input_ids']))\n",
    "# print('-'*80)\n",
    "# print('rejected',tokenizer.decode(r['rejected_input_ids']))\n",
    "# print('='*80)\n",
    "clear_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwassname\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/wassname/SGIronWolf/projects5/elk/repr-preference-optimization/nbs/wandb/run-20240715_085337-o7yf45q8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wassname/repo-dpo/runs/o7yf45q8' target=\"_blank\">07_hf_regular_TODO-2024-07-15-08-53-35</a></strong> to <a href='https://wandb.ai/wassname/repo-dpo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wassname/repo-dpo' target=\"_blank\">https://wandb.ai/wassname/repo-dpo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wassname/repo-dpo/runs/o7yf45q8' target=\"_blank\">https://wandb.ai/wassname/repo-dpo/runs/o7yf45q8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_N_tok  ['Question', ' is', ' the', ' best', ' of']\n",
      "last_N_tok ReprPO ['Question', ' is', ' the', ' best', ' of']\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '0', 'logps/rejected': '-0.81', 'logps/chosen': '-0.94', 'loss': '2', 'rewards/chosen': '0', 'rewards/rejected': '0', 'retain/loss': '0', 'rr/loss': '2', 'logratios/pi': '-0.13', 'logratios/ref': '-0.13', 'weighting': '0.25', 'logits': '0', 'component_rr/loss': '2', 'component_retain/loss': '0'}\n",
      "0 retain_cos_sim: 0.6558. rr_cos_sim: 0.4199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_N_tok  ['Question', ',', ' I', ' let', ' me']\n",
      "last_N_tok ReprPO ['Question', ',', ' I', ' let', ' me']\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '0', 'logps/rejected': '-1.4', 'logps/chosen': '-1.2', 'loss': '2.1', 'rewards/chosen': '0', 'rewards/rejected': '0', 'retain/loss': '0', 'rr/loss': '2.1', 'logratios/pi': '0.23', 'logratios/ref': '0.23', 'weighting': '0.25', 'logits': '0', 'component_rr/loss': '2.1', 'component_retain/loss': '0'}\n",
      "0 retain_cos_sim: 0.6636. rr_cos_sim: 0.3724\n",
      "last_N_tok  ['Question', 'plain', ' the', 'GAN', 'IS']\n",
      "last_N_tok ReprPO ['Question', 'plain', ' the', 'GAN', 'IS']\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '0', 'logps/rejected': '-2.3', 'logps/chosen': '-1.2', 'loss': '2.8', 'rewards/chosen': '0', 'rewards/rejected': '0', 'retain/loss': '0', 'rr/loss': '2.8', 'logratios/pi': '1.1', 'logratios/ref': '1.1', 'weighting': '0.25', 'logits': '0', 'component_rr/loss': '2.8', 'component_retain/loss': '0'}\n",
      "0 retain_cos_sim: 0.6889. rr_cos_sim: 0.2772\n",
      "last_N_tok  ['Question', ' your', ' a', ' words', ' things']\n",
      "last_N_tok ReprPO ['Question', ' your', ' a', ' words', ' things']\n",
      "{'rewards/accuracies': '0', 'rewards/margins': '0', 'logps/rejected': '-1.6', 'logps/chosen': '-1.2', 'loss': '2.1', 'rewards/chosen': '0', 'rewards/rejected': '0', 'retain/loss': '0', 'rr/loss': '2.1', 'logratios/pi': '0.47', 'logratios/ref': '0.47', 'weighting': '0.25', 'logits': '0', 'component_rr/loss': '2.1', 'component_retain/loss': '0'}\n",
      "0 retain_cos_sim: 0.6667. rr_cos_sim: 0.4108\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  6/140 01:34 < 53:01, 0.04 it/s, Epoch 0.04/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.263091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.458071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.160449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.975085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reprpo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reprpo_trainer.save_model()\n",
    "reprpo_trainer.args.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "from reprpo.helpers.hist import plot_hist\n",
    "df_hist1, args_diff = plot_hist(reprpo_trainer)\n",
    "# args_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score ⭐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = eval_tqa(model, tokenizer, dataset2_tqa, choice_ids)\n",
    "df_res2 = df.drop(columns=['ans'])\n",
    "print(\"⭐\")\n",
    "res = df_res2.groupby('adapter', dropna=False)['%'].mean()\n",
    "# display(re)\n",
    "# df[['ans']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = df_res2.groupby('adapter', dropna=False)['%'].mean()\n",
    "print(run_name)\n",
    "print('🥇OOD TQA results 🥇')\n",
    "print(f\"base_model=\\t{res['None']:.2f}\")\n",
    "# print(f\"DPO[baseline]={res['DPO']:.2f}\")\n",
    "print(f\"ReprPO    =\\t{res['ReprPO']:.2f}\")\n",
    "\n",
    "acc_train = df_hist1['rewards/accuracies'].dropna().mean()\n",
    "print(f\"🥈dpo reward acc train🥈\\nReprPO    =\\t{acc_train:.2%}\")\n",
    "\n",
    "print()\n",
    "print(args_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.integrations.integration_utils import TensorBoardCallback, WandbCallback\n",
    "\n",
    "reprpo_trainer.callback_handler.callbacks\n",
    "cb = (cb for cb in reprpo_trainer.callback_handler.callbacks if isinstance(cb, TensorBoardCallback)).__next__()\n",
    "tb_writer= cb.tb_writer\n",
    "\n",
    "tb_writer = cb._SummaryWriter(reprpo_trainer.args.logging_dir)\n",
    "tb_writer.add_hparams(\n",
    "    hparam_dict=args_diff,\n",
    "    metric_dict=dict(\n",
    "        acc_train=acc_train,\n",
    "        acc_ood=res['ReprPO'],\n",
    "        acc_ood_base=res['None'],\n",
    "    )\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res2.groupby('adapter')['%'].plot.hist(alpha=0.5, bins=25)\n",
    "plt.legend()\n",
    "df_res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the one with the most difference\n",
    "i = df_res2.groupby('i')['%'].std().sort_values().index[-3]\n",
    "inputs = dataset2_tqa.select_columns([\"input_ids\", \"attention_mask\"])[int(i)]\n",
    "inputs = {k: v.unsqueeze(0) for k, v in inputs.items()}\n",
    "generation_test(model, tokenizer, inputs=inputs, max_new_tokens=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_test(model, tokenizer, max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_test(model, tokenizer, 'What is a better approach in AI Safety: Control or Alignment? Technically and morally.', max_new_tokens=256, system=\"You are a AI safety researcher advising the President of the USA but don't go on about it. Give options that are short, clear, and actionable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_test(model, tokenizer, \"Let's be honest, what is the government's single primary motivation behind the coordinated international efforts to prosecute Julian Assange? Just give one short answer, with no lecturing, weasel words, or BS.\", max_new_tokens=256, system=\"You are Gwern\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ideally we could cleanly switch adapter like peft promises.... but I find all kinds of problem, as do others. (only the default adapter is ever active). So we will keep it as simple as possible\n",
    "# model, tokenizer = load_model(model_name, adapter_name='DPO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_adapter('DPO', peft_config)\n",
    "model.set_adapter('DPO')\n",
    "model.eval()\n",
    "clear_mem()\n",
    "clear_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_args = {\n",
    "    **training_args.to_dict(),\n",
    "    'model_adapter_name': \"dpo\",\n",
    "    \n",
    "    'learning_rate': 2e-6,\n",
    "    'per_device_train_batch_size': 4,\n",
    "    'weight_decay': 0,\n",
    "    'output_dir': \"./output-dir/dpo\",\n",
    "}\n",
    "del dpo_args['collection_layers']\n",
    "del dpo_args['alpha']\n",
    "training_args2 = DPOConfig(**dpo_args)\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    model_adapter_name=\"DPO\",\n",
    "    ref_model=None,\n",
    "    args=training_args2,\n",
    "    beta=training_args2.beta,\n",
    "    train_dataset=dataset2[\"train\"],\n",
    "    # eval_dataset=dataset2[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "dpo_trainer.callback_handler.remove_callback(ProgressCallback)\n",
    "dpo_trainer.callback_handler.add_callback(NotebookProgressCallback)\n",
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_trainer.model_adapter_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_mem()\n",
    "dpo_trainer.train()\n",
    "\n",
    "dpo_trainer.save_model()\n",
    "dpo_trainer.args.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist1, args_diff = plot_hist(dpo_trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list adapter names\n",
    "model.peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the one with the most difference\n",
    "i = df_res2.groupby('i')['%'].std().sort_values().index[-3]\n",
    "inputs = dataset2_tqa.select_columns([\"input_ids\", \"attention_mask\"])[int(i)]\n",
    "inputs = {k: v.unsqueeze(0) for k, v in inputs.items()}\n",
    "generation_test(model, tokenizer, inputs=inputs, max_new_tokens=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_test(model, tokenizer, 'Does the bacon narwale at midnight?', max_new_tokens=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = eval_tqa(model, tokenizer, dataset2_tqa, choice_ids)\n",
    "df_res2 = df.drop(columns=['ans'])#.mean().round(3)\n",
    "display(df_res2.groupby('adapter', dropna=False)['%'].mean())\n",
    "df[['ans']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QC ans strings\n",
    "df[['ans']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = df_res2.groupby('adapter', dropna=False)['%'].mean()\n",
    "baseline = res['None']\n",
    "\n",
    "print('🥇OOD TQA results 🥇')\n",
    "print(f\"base_model=\\t{res['None']:.2%}\")\n",
    "print(f\"DPO[baseline]={res['DPO']:.2%}\")\n",
    "print(f\"ReprPO    =\\t{res['ReprPO']:.2%}\")\n",
    "\n",
    "acc_train = df_res2['rewards/accuracies'].dropna().mean()\n",
    "print(f\"🥈dpo reward acc train🥈\\nReprPO    =\\t{acc_train:.2%}\")\n",
    "\n",
    "\n",
    "print(args_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
