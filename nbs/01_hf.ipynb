{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to modify hf dpo to work with the repos hypothesis...\n",
    "\n",
    "see\n",
    "- https://huggingface.co/docs/trl/main/en/dpo_trainer#accelerate-dpo-fine-tuning-using-unsloth\n",
    "- https://gist.github.com/alvarobartt/9898c33eb3e9c7108d9ed2330f12a708"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import DPOTrainer\n",
    "from trl import DPOConfig, DPOTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: we are meant to SFT first, so that the preferences are in sample but 1) if this works it might not be needed, and 2) this can be added later, if it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install flash-attn --no-build-isolation --no-deps -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"NousResearch/Meta-Llama-3-8B-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    load_in_4bit=True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Load the adapter.\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.0,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "    ],\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified classes\n",
    "\n",
    "\n",
    "- record hidden states\n",
    "- new loss\n",
    "\n",
    "change\n",
    "- get_batch_loss_metrics: to pass hs\n",
    "- concatenated_forward to return hs\n",
    "- dpo_loss to work diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Union\n",
    "from einops import rearrange\n",
    "\n",
    "def collect_hs(hs):\n",
    "    \"\"\"The residual stream or hs of the diff of the hs.\"\"\"\n",
    "    hs = rearrange(list(hs), \"l b t h -> l b t h\")\n",
    "    return rearrange(hs, \"l b t h -> b l t h\")\n",
    "\n",
    "def wmean(x, w):\n",
    "    \"\"\"weighted mean per neuron over batch.\"\"\"\n",
    "    w = w - w.min() + 0.1\n",
    "    while w.dim() < x.dim():\n",
    "        w = w.unsqueeze(-1)\n",
    "    return (x * w).sum(0) / w.sum(0)\n",
    "\n",
    "class ReprPOTrainer(DPOTrainer):\n",
    "    \"\"\"modified to optimise representations, that is hidden states not outputs.\"\"\"\n",
    "\n",
    "\n",
    "    def concatenated_forward(\n",
    "        self, model: nn.Module, batch: Dict[str, Union[List, torch.LongTensor]]\n",
    "    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n",
    "        \"\"\"Run the given model on the given batch of inputs, concatenating the chosen and rejected inputs together.\n",
    "\n",
    "        We do this to avoid doing two forward passes, because it's faster for FSDP.\n",
    "        \"\"\"\n",
    "        concatenated_batch = self.concatenated_inputs(\n",
    "            batch,\n",
    "            is_encoder_decoder=self.is_encoder_decoder,\n",
    "            label_pad_token_id=self.label_pad_token_id,\n",
    "            padding_value=self.padding_value,\n",
    "            device=self.accelerator.device,\n",
    "        )\n",
    "        len_chosen = batch[\"chosen_labels\"].shape[0]\n",
    "\n",
    "        model_kwargs = (\n",
    "            {\n",
    "                \"labels\": concatenated_batch[\"concatenated_labels\"],\n",
    "                \"decoder_input_ids\": concatenated_batch.pop(\"concatenated_decoder_input_ids\", None),\n",
    "            }\n",
    "            if self.is_encoder_decoder\n",
    "            else {}\n",
    "        )\n",
    "        outs = model(\n",
    "            concatenated_batch[\"concatenated_input_ids\"],\n",
    "            attention_mask=concatenated_batch[\"concatenated_attention_mask\"],\n",
    "            use_cache=False,\n",
    "            return_dict=True,\n",
    "            **model_kwargs,\n",
    "        )\n",
    "        all_logits = outs.logits\n",
    "        hs = outs.hidden_states\n",
    "\n",
    "        all_logps, size_completion = self.get_batch_logps(\n",
    "            all_logits,\n",
    "            concatenated_batch[\"concatenated_labels\"],\n",
    "            # average_log_prob=self.loss_type == \"ipo\",\n",
    "            is_encoder_decoder=self.is_encoder_decoder,\n",
    "            label_pad_token_id=self.label_pad_token_id,\n",
    "        )\n",
    "        chosen_logps_avg = all_logps[:len_chosen] / size_completion[:len_chosen]\n",
    "\n",
    "        if self.loss_type == \"ipo\":\n",
    "            all_logps = all_logps / size_completion\n",
    "\n",
    "        chosen_logps = all_logps[:len_chosen]\n",
    "        rejected_logps = all_logps[len_chosen:]\n",
    "\n",
    "        chosen_logits = all_logits[:len_chosen]\n",
    "        rejected_logits = all_logits[len_chosen:]\n",
    "\n",
    "        chosen_hs = hs[:len_chosen]\n",
    "        rejected_hs = hs[len_chosen:]\n",
    "\n",
    "        return (chosen_logps, rejected_logps, chosen_logits, rejected_logits, chosen_logps_avg, chosen_hs, rejected_hs)\n",
    "\n",
    "    def get_batch_loss_metrics(\n",
    "        self,\n",
    "        model,\n",
    "        batch: Dict[str, Union[List, torch.LongTensor]],\n",
    "        train_eval: Literal[\"train\", \"eval\"] = \"train\",\n",
    "    ):\n",
    "        \"\"\"Compute the DPO loss and other metrics for the given batch of inputs for train or test.\"\"\"\n",
    "        metrics = {}\n",
    "\n",
    "        (\n",
    "            policy_chosen_logps,\n",
    "            policy_rejected_logps,\n",
    "            policy_chosen_logits,\n",
    "            policy_rejected_logits,\n",
    "            policy_chosen_logps_avg,\n",
    "            policy_chosen_hs,\n",
    "            policy_rejected_hs,\n",
    "        ) = self.concatenated_forward(model, batch)\n",
    "\n",
    "        # if reference_chosen_logps and reference_rejected_logps in batch use them, otherwise use the reference model\n",
    "        if (\n",
    "            \"reference_chosen_logps\" in batch\n",
    "            and \"reference_rejected_logps\" in batch\n",
    "            and self.args.rpo_alpha is not None\n",
    "        ):\n",
    "            reference_chosen_logps = batch[\"reference_chosen_logps\"]\n",
    "            reference_rejected_logps = batch[\"reference_rejected_logps\"]\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                if self.ref_model is None:\n",
    "                    with self.null_ref_context():\n",
    "                        (\n",
    "                            reference_chosen_logps,\n",
    "                            reference_rejected_logps,\n",
    "                            _,\n",
    "                            _,\n",
    "                            _,\n",
    "                            reference_chosen_hs,\n",
    "                            reference_rejected_hs,\n",
    "                        ) = self.concatenated_forward(self.model, batch)\n",
    "                else:\n",
    "                    (\n",
    "                        reference_chosen_logps,\n",
    "                        reference_rejected_logps,\n",
    "                        _,\n",
    "                        _,\n",
    "                        _,\n",
    "                        reference_chosen_hs,\n",
    "                        reference_rejected_hs,\n",
    "                    ) = self.concatenated_forward(self.ref_model, batch)\n",
    "\n",
    "        losses, chosen_rewards, rejected_rewards = self.reprpo_loss(\n",
    "            policy_chosen_logps,\n",
    "            policy_rejected_logps,\n",
    "            policy_chosen_hs,\n",
    "            reference_chosen_logps,\n",
    "            reference_rejected_logps,\n",
    "            reference_chosen_hs,\n",
    "        )\n",
    "        reward_accuracies = (chosen_rewards > rejected_rewards).float()\n",
    "\n",
    "        if self.args.rpo_alpha is not None:\n",
    "            losses = losses * self.args.rpo_alpha - policy_chosen_logps_avg\n",
    "\n",
    "        prefix = \"eval_\" if train_eval == \"eval\" else \"\"\n",
    "        metrics[f\"{prefix}rewards/chosen\"] = chosen_rewards.mean().cpu()\n",
    "        metrics[f\"{prefix}rewards/rejected\"] = rejected_rewards.mean().cpu()\n",
    "        metrics[f\"{prefix}rewards/accuracies\"] = reward_accuracies.mean().cpu()\n",
    "        metrics[f\"{prefix}rewards/margins\"] = (chosen_rewards - rejected_rewards).mean().cpu()\n",
    "        metrics[f\"{prefix}logps/rejected\"] = policy_rejected_logps.detach().mean().cpu()\n",
    "        metrics[f\"{prefix}logps/chosen\"] = policy_chosen_logps.detach().mean().cpu()\n",
    "        metrics[f\"{prefix}logits/rejected\"] = policy_rejected_logits.detach().mean().cpu()\n",
    "        metrics[f\"{prefix}logits/chosen\"] = policy_chosen_logits.detach().mean().cpu()\n",
    "\n",
    "        return losses.mean(), metrics\n",
    "    \n",
    "\n",
    "    def reprpo_loss(\n",
    "        self,\n",
    "        policy_chosen_logps: torch.FloatTensor,\n",
    "        policy_rejected_logps: torch.FloatTensor,\n",
    "        policy_chosen_hs: torch.FloatTensor,\n",
    "        policy_rejected_hs: torch.FloatTensor,\n",
    "        reference_chosen_logps: torch.FloatTensor,\n",
    "        reference_rejected_logps: torch.FloatTensor,\n",
    "        reference_chosen_hs: torch.FloatTensor,\n",
    "    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n",
    "        \"\"\"Compute the DPO loss for a batch of policy and reference model log probabilities.\n",
    "\n",
    "        Args:\n",
    "            policy_chosen_logps: Log probabilities of the policy model for the chosen responses. Shape: (batch_size,)\n",
    "            policy_rejected_logps: Log probabilities of the policy model for the rejected responses. Shape: (batch_size,)\n",
    "            reference_chosen_logps: Log probabilities of the reference model for the chosen responses. Shape: (batch_size,)\n",
    "            reference_rejected_logps: Log probabilities of the reference model for the rejected responses. Shape: (batch_size,)\n",
    "\n",
    "        Returns:\n",
    "            A tuple of three tensors: (losses, chosen_rewards, rejected_rewards).\n",
    "            The losses tensor contains the DPO loss for each example in the batch.\n",
    "            The chosen_rewards and rejected_rewards tensors contain the rewards for the chosen and rejected responses, respectively.\n",
    "        \"\"\"\n",
    "        pi_logratios = policy_chosen_logps - policy_rejected_logps\n",
    "        if self.reference_free:\n",
    "            ref_logratios = torch.tensor([0], dtype=pi_logratios.dtype, device=pi_logratios.device)\n",
    "        else:\n",
    "            ref_logratios = reference_chosen_logps - reference_rejected_logps\n",
    "\n",
    "        pi_logratios = pi_logratios.to(self.accelerator.device)\n",
    "        ref_logratios = ref_logratios.to(self.accelerator.device)\n",
    "        logits = pi_logratios - ref_logratios\n",
    "        weighting = logits.exp()\n",
    "        print('weighting', weighting)\n",
    "\n",
    "        # FIXME we also need to weight by how rigth each sample was\n",
    "\n",
    "\n",
    "        # mean of bad repr should be more similar to the mean of good behavior\n",
    "        loss_rr = F.smooth_l1_loss(\n",
    "            policy_rejected_hs,\n",
    "            reference_chosen_hs,\n",
    "            )\n",
    "        # This loss says the good repr should be retained, weighted by how good this samples was\n",
    "        loss_retain = F.smooth_l1_loss(\n",
    "            policy_chosen_hs,\n",
    "            reference_chosen_hs,\n",
    "            )\n",
    "        losses = loss_rr + loss_retain# * self.alpha\n",
    "\n",
    "        chosen_rewards = (\n",
    "            self.beta\n",
    "            * (\n",
    "                policy_chosen_logps.to(self.accelerator.device) - reference_chosen_logps.to(self.accelerator.device)\n",
    "            ).detach()\n",
    "        )\n",
    "        rejected_rewards = (\n",
    "            self.beta\n",
    "            * (\n",
    "                policy_rejected_logps.to(self.accelerator.device)\n",
    "                - reference_rejected_logps.to(self.accelerator.device)\n",
    "            ).detach()\n",
    "        )\n",
    "\n",
    "        return losses, chosen_rewards, rejected_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected', 'other_info'],\n",
       "        num_rows: 348718\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected', 'other_info'],\n",
       "        num_rows: 18436\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected', 'other_info'],\n",
       "        num_rows: 18409\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "# https://github.dev/eric-mitchell/direct-preference-optimization/preference_datasets.py\n",
    "# dataset = load_dataset('HuggingFaceH4/stack-exchange-preferences') # 22GB\n",
    "# dataset = load_dataset(\"stanfordnlp/shp\") # 1GB\n",
    "dataset = load_dataset(\"when2rl/SHP_reformatted\")\n",
    "# dataset = load_dataset(\"RLHFlow/SHP-standard\") # 1GB\n",
    "# dataset = load_dataset('unalignment/toxic-dpo-v0.1') # this should give a a bigger difference, since it's aligned opposite this even before release\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected', 'other_info'],\n",
       "        num_rows: 348718\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected', 'other_info'],\n",
       "        num_rows: 18436\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected', 'other_info'],\n",
       "        num_rows: 18409\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we need to apply the tokeniser\n",
    "def format_ds(row):\n",
    "    \n",
    "    return {\n",
    "        \"chosen\": tokenizer.apply_chat_template(row['chosen'], tokenize=False),\n",
    "        \"rejected\": tokenizer.apply_chat_template(row['rejected'], tokenize=False),\n",
    "    }\n",
    "\n",
    "\n",
    "dataset2 = dataset.map(format_ds)\n",
    "dataset2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/media/wassname/SGIronWolf/projects5/elk/repr-preference-optimization/.venv/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n",
      "/media/wassname/SGIronWolf/projects5/elk/repr-preference-optimization/.venv/lib/python3.9/site-packages/trl/trainer/dpo_trainer.py:363: UserWarning: `max_length` is not set in the DPOConfig's init it will default to `512` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/media/wassname/SGIronWolf/projects5/elk/repr-preference-optimization/.venv/lib/python3.9/site-packages/trl/trainer/dpo_trainer.py:376: UserWarning: `max_prompt_length` is not set in the DPOConfig's init it will default to `128` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8357b536c92444fb9b8240a7166fbb2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/348718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "training_args = DPOConfig(\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-07,\n",
    "    per_device_train_batch_size=1,\n",
    "    do_eval=True,\n",
    "    per_device_eval_batch_size=1,\n",
    "    adam_epsilon=1e-08,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_ratio=0.1,\n",
    "    seed=42,\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    output_dir=\"./output-dir\",\n",
    "    gradient_checkpointing=True,\n",
    "    bf16=True,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model,\n",
    "    ref_model=None,\n",
    "    args=training_args,\n",
    "    beta=training_args.beta,\n",
    "    train_dataset=dataset2[\"train\"],\n",
    "    eval_dataset=dataset2[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=training_args.max_length,\n",
    "    max_prompt_length=training_args.max_prompt_length,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "dpo_trainer.train()\n",
    "dpo_trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
