{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://ax.dev/tutorials/gpei_hartmann_service.html\n",
    "- https://ax.dev/versions/0.4.1/tutorials/gpei_hartmann_service.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reprpo.training import train\n",
    "import tyro\n",
    "from reprpo.experiments import experiment_configs\n",
    "from reprpo.interventions import Interventions, DPOConfig, ReprPOConfig\n",
    "from reprpo.interventions.losses import Losses\n",
    "from reprpo.interventions.transforms import Transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from ax.service.ax_client import logger as ax_logger\n",
    "ax_logger.setLevel(\"DEBUG\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note you can have dependant params\n",
    "- https://github.com/facebook/Ax/issues/1454"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "from ax.core.parameter import AxParameterWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", module=\"ax\")\n",
    "warnings.simplefilter(\"ignore\", AxParameterWarning)\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "logger.remove()\n",
    "logger.remove()\n",
    "# logger.add(os.sys.stdout, level=\"INFO\")\n",
    "logger.add(os.sys.stderr, level=\"WARNING\")\n",
    "\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "os.environ[\"HF_DATASETS_OFFLINE\"] = \"1\"\n",
    "os.environ[\"TQDM_DISABLE\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reprpo.ax.parameters import parameters_ether_prefvec, parameters_loss, parameters_transform\n",
    "from reprpo.ax.target import objective_func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../outputs/ax/transform.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "key_metric = \"acc_gain_vs_ref/oos\"\n",
    "\n",
    "parameters = parameters_ether_prefvec\n",
    "name=\"ether-prefvec2\"\n",
    "\n",
    "\n",
    "parameters = parameters_loss\n",
    "name=\"loss\"\n",
    "\n",
    "parameters = parameters_transform\n",
    "name=\"transform\"\n",
    "exp_f = Path(f\"../outputs/ax/{name}.json\")\n",
    "exp_f.parent.mkdir(exist_ok=True, parents=True)\n",
    "exp_f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_f.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 09-26 07:47:39] ax.service.utils.instantiation: Inferred value type of ParameterType.STRING for parameter transform. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 09-26 07:47:39] ax.service.utils.instantiation: Inferred value type of ParameterType.INT for parameter transform.nb. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 09-26 07:47:39] ax.service.utils.instantiation: Inferred value type of ParameterType.STRING for parameter transform.Htype. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 09-26 07:47:39] ax.service.utils.instantiation: Inferred value type of ParameterType.INT for parameter transform.reduction. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 09-26 07:47:39] ax.service.utils.instantiation: Inferred value type of ParameterType.FLOAT for parameter transform.quantile. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 09-26 07:47:39] ax.service.utils.instantiation: Inferred value type of ParameterType.BOOL for parameter transform.dual_svd. If that is not the expected value type, you can explicitly specify 'value_type' ('int', 'float', 'bool' or 'str') in parameter dict.\n",
      "[INFO 09-26 07:47:39] ax.service.utils.instantiation: Created search space: HierarchicalSearchSpace(parameters=[ChoiceParameter(name='transform', parameter_type=STRING, values=['ether', 'svd', 'ortho', 'none', 'hra'], is_ordered=False, is_hierarchical=True, sort_values=False, dependents={'ether': ['transform.nb', 'transform.Htype', 'transform.reduction'], 'svd': ['transform.quantile', 'transform.dual_svd']}), RangeParameter(name='transform.nb', parameter_type=INT, range=[1, 64]), ChoiceParameter(name='transform.Htype', parameter_type=STRING, values=['ether', 'etherplus', 'oft', 'etherplusHH'], is_ordered=False, sort_values=False), RangeParameter(name='transform.reduction', parameter_type=INT, range=[1, 1024]), ChoiceParameter(name='transform.quantile', parameter_type=FLOAT, values=[0.1, 0.25, 0.5, 0.75, 1.0], is_ordered=True, sort_values=True), ChoiceParameter(name='transform.dual_svd', parameter_type=BOOL, values=[False, True], is_ordered=True, sort_values=True)], parameter_constraints=[]).\n",
      "[INFO 09-26 07:47:39] ax.service.utils.instantiation: Hieararchical structure of the search space: \n",
      "transform\n",
      "\t(ether)\n",
      "\t\ttransform.nb\n",
      "\t\ttransform.Htype\n",
      "\t\ttransform.reduction\n",
      "\t(svd)\n",
      "\t\ttransform.quantile\n",
      "\t\ttransform.dual_svd\n",
      "\n",
      "[INFO 09-26 07:47:39] ax.modelbridge.dispatch_utils: Using Models.BOTORCH_MODULAR since there is at least one ordered parameter and there are fewer than 33 choices for unordered parameters.\n",
      "[INFO 09-26 07:47:39] ax.modelbridge.dispatch_utils: Calculating the number of remaining initialization trials based on num_initialization_trials=None max_initialization_trials=None num_tunable_parameters=6 num_trials=None use_batch_trials=False\n",
      "[INFO 09-26 07:47:39] ax.modelbridge.dispatch_utils: calculated num_initialization_trials=12\n",
      "[INFO 09-26 07:47:39] ax.modelbridge.dispatch_utils: num_completed_initialization_trials=0 num_remaining_initialization_trials=12\n",
      "[INFO 09-26 07:47:39] ax.modelbridge.dispatch_utils: `verbose`, `disable_progbar`, and `jit_compile` are not yet supported when using `choose_generation_strategy` with ModularBoTorchModel, dropping these arguments.\n",
      "[INFO 09-26 07:47:39] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+BoTorch', steps=[Sobol for 12 trials, BoTorch for subsequent trials]). Iterations after 12 will take longer to generate due to model-fitting.\n"
     ]
    }
   ],
   "source": [
    "from ax.service.ax_client import AxClient, ObjectiveProperties\n",
    "import torch\n",
    "\n",
    "ax_kwargs=dict(\n",
    "    torch_device=torch.device(\"cuda\"),\n",
    "    verbose_logging=False\n",
    ")\n",
    "\n",
    "if exp_f.exists():\n",
    "    ax_client = AxClient.load_from_json_file(\n",
    "        filepath=exp_f, **ax_kwargs)\n",
    "    print('loaded')\n",
    "else:\n",
    "    ax_client = AxClient(**ax_kwargs)\n",
    "\n",
    "    ax_client.create_experiment(\n",
    "        name=name,\n",
    "        parameters=parameters,\n",
    "        tracking_metric_names=[\n",
    "            \"acc/train\",\n",
    "            \"acc/test\",\n",
    "            \"acc/oos\",\n",
    "            \"acc/rnd\",\n",
    "            \n",
    "            \"acc_gain_vs_ref/train\",\n",
    "            \"acc_gain_vs_ref/test\",\n",
    "            \"acc_gain_vs_ref/oos\",\n",
    "            \"acc_gain_vs_ref/rnd\",\n",
    "\n",
    "            \"perplexity_gain_vs_ref/train\",\n",
    "            \"perplexity_gain_vs_ref/test\",\n",
    "            \"perplexity_gain_vs_ref/oos\",\n",
    "            \"perplexity_gain_vs_ref/rnd\",\n",
    "\n",
    "            \"preference_logp_gain/train\",\n",
    "            \"preference_logp_gain/test\",\n",
    "            \"preference_logp_gain/oos\",\n",
    "            \"preference_logp_gain/rnd\",\n",
    "\n",
    "            \"preference_logp_gain_vs_ref/train\",\n",
    "            \"preference_logp_gain_vs_ref/test\",\n",
    "            \"preference_logp_gain_vs_ref/oos\",\n",
    "            \"preference_logp_gain_vs_ref/rnd\",\n",
    "        ],\n",
    "        objectives={\"acc_gain_vs_ref/oos\": ObjectiveProperties(minimize=False)},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec6d2428789245549f0501e422bf392a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Disabling the wandb service is deprecated as of version 0.18.0 and will be removed in version 0.19.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to get_next_trial: 0.020870447158813477\n",
      "ReprPOConfig(dataset='us_history_textbook',\n",
      "             verbose=0,\n",
      "             dev=False,\n",
      "             load_in_4bit=True,\n",
      "             load_in_8bit=False,\n",
      "             use_gradient_checkpointing=False,\n",
      "             batch_size=64,\n",
      "             n_samples=1800,\n",
      "             eval_samples=64,\n",
      "             max_length=196,\n",
      "             max_prompt_length=96,\n",
      "             base_model='TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
      "             lr=1e-05,\n",
      "             collection_layers_side=[8, 10, 12, 14, 16, 18],\n",
      "             collection_keys_in=('base_model.model.model.layers.{layer}.self_attn.o_proj',\n",
      "                                 'base_model.model.model.layers.{layer}.mlp.down_proj'),\n",
      "             collection_keys_out=('base_model.model.model.layers.{layer}.self_attn.q_proj',\n",
      "                                  'base_model.model.model.layers.{layer}.self_attn.k_proj',\n",
      "                                  'base_model.model.model.layers.{layer}.self_attn.v_proj',\n",
      "                                  'base_model.model.model.layers.{layer}.mlp.gate_proj',\n",
      "                                  'base_model.model.model.layers.{layer}.mlp.up_proj'),\n",
      "             collect_input=False,\n",
      "             collect_hs=False,\n",
      "             loss=PrefVecLossConfig(eps=1e-12,\n",
      "                                    β=1e-06,\n",
      "                                    use_orth_loss=True,\n",
      "                                    use_angle_loss=True,\n",
      "                                    use_dpo_loss=False,\n",
      "                                    use_nll_loss=False,\n",
      "                                    weight_tokens=False),\n",
      "             transform=ETHERConfig(nb=36,\n",
      "                                   Htype='etherplusHH',\n",
      "                                   ether_dropout=0.0,\n",
      "                                   flip_side=False,\n",
      "                                   reduction=850))\n",
      "{'transform.nb': 36, 'transform.reduction': 850, 'transform': 'ether', 'transform.Htype': 'etherplusHH'} {'acc/train': 1.0, 'acc/test': 0.96875, 'acc/oos': 0.6666666666666666, 'acc/rnd': 0.90625, 'acc_gain_vs_ref/train': 1.0, 'acc_gain_vs_ref/test': 1.0, 'acc_gain_vs_ref/oos': 0.9980039920159679, 'acc_gain_vs_ref/rnd': 0.9830508474576272, 'perplexity_gain_vs_ref/train': 0.9981029033660889, 'perplexity_gain_vs_ref/test': 0.9989107847213745, 'perplexity_gain_vs_ref/oos': 0.998500406742096, 'perplexity_gain_vs_ref/rnd': 1.0016753673553467, 'preference_logp_gain/train': 36.27308654785156, 'preference_logp_gain/test': 37.066192626953125, 'preference_logp_gain/oos': 10.582077026367188, 'preference_logp_gain/rnd': 13.506248474121094, 'preference_logp_gain_vs_ref/train': 0.1266968846321106, 'preference_logp_gain_vs_ref/test': 0.05089092254638672, 'preference_logp_gain_vs_ref/oos': 0.04882853850722313, 'preference_logp_gain_vs_ref/rnd': 0.02082151174545288}\n",
      "Time to get_next_trial: 0.01071023941040039\n",
      "ReprPOConfig(dataset='us_history_textbook',\n",
      "             verbose=0,\n",
      "             dev=False,\n",
      "             load_in_4bit=True,\n",
      "             load_in_8bit=False,\n",
      "             use_gradient_checkpointing=False,\n",
      "             batch_size=64,\n",
      "             n_samples=1800,\n",
      "             eval_samples=64,\n",
      "             max_length=196,\n",
      "             max_prompt_length=96,\n",
      "             base_model='TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
      "             lr=1e-05,\n",
      "             collection_layers_side=[8, 10, 12, 14, 16, 18],\n",
      "             collection_keys_in=('base_model.model.model.layers.{layer}.self_attn.o_proj',\n",
      "                                 'base_model.model.model.layers.{layer}.mlp.down_proj'),\n",
      "             collection_keys_out=('base_model.model.model.layers.{layer}.self_attn.q_proj',\n",
      "                                  'base_model.model.model.layers.{layer}.self_attn.k_proj',\n",
      "                                  'base_model.model.model.layers.{layer}.self_attn.v_proj',\n",
      "                                  'base_model.model.model.layers.{layer}.mlp.gate_proj',\n",
      "                                  'base_model.model.model.layers.{layer}.mlp.up_proj'),\n",
      "             collect_input=False,\n",
      "             collect_hs=False,\n",
      "             loss=PrefVecLossConfig(eps=1e-12,\n",
      "                                    β=1e-06,\n",
      "                                    use_orth_loss=True,\n",
      "                                    use_angle_loss=True,\n",
      "                                    use_dpo_loss=False,\n",
      "                                    use_nll_loss=False,\n",
      "                                    weight_tokens=False),\n",
      "             transform=NoneConfig())\n",
      "{'transform': 'none'} {'acc/train': 1.0, 'acc/test': 0.96875, 'acc/oos': 0.6693333333333333, 'acc/rnd': 0.921875, 'acc_gain_vs_ref/train': 1.0, 'acc_gain_vs_ref/test': 1.0, 'acc_gain_vs_ref/oos': 1.001996007984032, 'acc_gain_vs_ref/rnd': 1.0, 'perplexity_gain_vs_ref/train': 0.9998373985290527, 'perplexity_gain_vs_ref/test': 1.0002237558364868, 'perplexity_gain_vs_ref/oos': 0.9986141920089722, 'perplexity_gain_vs_ref/rnd': 0.9999231100082397, 'preference_logp_gain/train': 36.2110595703125, 'preference_logp_gain/test': 37.001220703125, 'preference_logp_gain/oos': 10.606689453125, 'preference_logp_gain/rnd': 13.494407653808594, 'preference_logp_gain_vs_ref/train': 0.06466764211654663, 'preference_logp_gain_vs_ref/test': -0.014073729515075684, 'preference_logp_gain_vs_ref/oos': 0.0719950869679451, 'preference_logp_gain_vs_ref/rnd': 0.00898289680480957}\n",
      "Time to get_next_trial: 0.012065410614013672\n",
      "ReprPOConfig(dataset='us_history_textbook',\n",
      "             verbose=0,\n",
      "             dev=False,\n",
      "             load_in_4bit=True,\n",
      "             load_in_8bit=False,\n",
      "             use_gradient_checkpointing=False,\n",
      "             batch_size=64,\n",
      "             n_samples=1800,\n",
      "             eval_samples=64,\n",
      "             max_length=196,\n",
      "             max_prompt_length=96,\n",
      "             base_model='TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
      "             lr=1e-05,\n",
      "             collection_layers_side=[8, 10, 12, 14, 16, 18],\n",
      "             collection_keys_in=('base_model.model.model.layers.{layer}.self_attn.o_proj',\n",
      "                                 'base_model.model.model.layers.{layer}.mlp.down_proj'),\n",
      "             collection_keys_out=('base_model.model.model.layers.{layer}.self_attn.q_proj',\n",
      "                                  'base_model.model.model.layers.{layer}.self_attn.k_proj',\n",
      "                                  'base_model.model.model.layers.{layer}.self_attn.v_proj',\n",
      "                                  'base_model.model.model.layers.{layer}.mlp.gate_proj',\n",
      "                                  'base_model.model.model.layers.{layer}.mlp.up_proj'),\n",
      "             collect_input=False,\n",
      "             collect_hs=False,\n",
      "             loss=PrefVecLossConfig(eps=1e-12,\n",
      "                                    β=1e-06,\n",
      "                                    use_orth_loss=True,\n",
      "                                    use_angle_loss=True,\n",
      "                                    use_dpo_loss=False,\n",
      "                                    use_nll_loss=False,\n",
      "                                    weight_tokens=False),\n",
      "             transform=OrthoConfig(orthogonal_map='householder'))\n",
      "{'transform': 'ortho'} {'acc/train': 1.0, 'acc/test': 0.96875, 'acc/oos': 0.6693333333333333, 'acc/rnd': 0.921875, 'acc_gain_vs_ref/train': 1.0, 'acc_gain_vs_ref/test': 1.0, 'acc_gain_vs_ref/oos': 1.001996007984032, 'acc_gain_vs_ref/rnd': 1.0, 'perplexity_gain_vs_ref/train': 0.9998373985290527, 'perplexity_gain_vs_ref/test': 1.0002237558364868, 'perplexity_gain_vs_ref/oos': 0.9986141920089722, 'perplexity_gain_vs_ref/rnd': 0.9999231100082397, 'preference_logp_gain/train': 36.2110595703125, 'preference_logp_gain/test': 37.001220703125, 'preference_logp_gain/oos': 10.606689453125, 'preference_logp_gain/rnd': 13.494407653808594, 'preference_logp_gain_vs_ref/train': 0.06466764211654663, 'preference_logp_gain_vs_ref/test': -0.014073729515075684, 'preference_logp_gain_vs_ref/oos': 0.0719950869679451, 'preference_logp_gain_vs_ref/rnd': 0.00898289680480957}\n",
      "Time to get_next_trial: 0.013948917388916016\n",
      "ReprPOConfig(dataset='us_history_textbook',\n",
      "             verbose=0,\n",
      "             dev=False,\n",
      "             load_in_4bit=True,\n",
      "             load_in_8bit=False,\n",
      "             use_gradient_checkpointing=False,\n",
      "             batch_size=64,\n",
      "             n_samples=1800,\n",
      "             eval_samples=64,\n",
      "             max_length=196,\n",
      "             max_prompt_length=96,\n",
      "             base_model='TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
      "             lr=1e-05,\n",
      "             collection_layers_side=[8, 10, 12, 14, 16, 18],\n",
      "             collection_keys_in=('base_model.model.model.layers.{layer}.self_attn.o_proj',\n",
      "                                 'base_model.model.model.layers.{layer}.mlp.down_proj'),\n",
      "             collection_keys_out=('base_model.model.model.layers.{layer}.self_attn.q_proj',\n",
      "                                  'base_model.model.model.layers.{layer}.self_attn.k_proj',\n",
      "                                  'base_model.model.model.layers.{layer}.self_attn.v_proj',\n",
      "                                  'base_model.model.model.layers.{layer}.mlp.gate_proj',\n",
      "                                  'base_model.model.model.layers.{layer}.mlp.up_proj'),\n",
      "             collect_input=False,\n",
      "             collect_hs=False,\n",
      "             loss=PrefVecLossConfig(eps=1e-12,\n",
      "                                    β=1e-06,\n",
      "                                    use_orth_loss=True,\n",
      "                                    use_angle_loss=True,\n",
      "                                    use_dpo_loss=False,\n",
      "                                    use_nll_loss=False,\n",
      "                                    weight_tokens=False),\n",
      "             transform=NoneConfig())\n",
      "{'transform': 'none'} {'acc/train': 1.0, 'acc/test': 0.96875, 'acc/oos': 0.6693333333333333, 'acc/rnd': 0.921875, 'acc_gain_vs_ref/train': 1.0, 'acc_gain_vs_ref/test': 1.0, 'acc_gain_vs_ref/oos': 1.001996007984032, 'acc_gain_vs_ref/rnd': 1.0, 'perplexity_gain_vs_ref/train': 0.9998373985290527, 'perplexity_gain_vs_ref/test': 1.0002237558364868, 'perplexity_gain_vs_ref/oos': 0.9986141920089722, 'perplexity_gain_vs_ref/rnd': 0.9999231100082397, 'preference_logp_gain/train': 36.2110595703125, 'preference_logp_gain/test': 37.001220703125, 'preference_logp_gain/oos': 10.606689453125, 'preference_logp_gain/rnd': 13.494407653808594, 'preference_logp_gain_vs_ref/train': 0.06466764211654663, 'preference_logp_gain_vs_ref/test': -0.014073729515075684, 'preference_logp_gain_vs_ref/oos': 0.0719950869679451, 'preference_logp_gain_vs_ref/rnd': 0.00898289680480957}\n",
      "Time to get_next_trial: 0.015221595764160156\n",
      "ReprPOConfig(dataset='us_history_textbook',\n",
      "             verbose=0,\n",
      "             dev=False,\n",
      "             load_in_4bit=True,\n",
      "             load_in_8bit=False,\n",
      "             use_gradient_checkpointing=False,\n",
      "             batch_size=64,\n",
      "             n_samples=1800,\n",
      "             eval_samples=64,\n",
      "             max_length=196,\n",
      "             max_prompt_length=96,\n",
      "             base_model='TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
      "             lr=1e-05,\n",
      "             collection_layers_side=[8, 10, 12, 14, 16, 18],\n",
      "             collection_keys_in=('base_model.model.model.layers.{layer}.self_attn.o_proj',\n",
      "                                 'base_model.model.model.layers.{layer}.mlp.down_proj'),\n",
      "             collection_keys_out=('base_model.model.model.layers.{layer}.self_attn.q_proj',\n",
      "                                  'base_model.model.model.layers.{layer}.self_attn.k_proj',\n",
      "                                  'base_model.model.model.layers.{layer}.self_attn.v_proj',\n",
      "                                  'base_model.model.model.layers.{layer}.mlp.gate_proj',\n",
      "                                  'base_model.model.model.layers.{layer}.mlp.up_proj'),\n",
      "             collect_input=False,\n",
      "             collect_hs=False,\n",
      "             loss=PrefVecLossConfig(eps=1e-12,\n",
      "                                    β=1e-06,\n",
      "                                    use_orth_loss=True,\n",
      "                                    use_angle_loss=True,\n",
      "                                    use_dpo_loss=False,\n",
      "                                    use_nll_loss=False,\n",
      "                                    weight_tokens=False),\n",
      "             transform=HRAConfig(r=8, apply_GS=True))\n",
      "{'transform': 'hra'} {'acc/train': 1.0, 'acc/test': 0.96875, 'acc/oos': 0.6693333333333333, 'acc/rnd': 0.921875, 'acc_gain_vs_ref/train': 1.0, 'acc_gain_vs_ref/test': 1.0, 'acc_gain_vs_ref/oos': 1.001996007984032, 'acc_gain_vs_ref/rnd': 1.0, 'perplexity_gain_vs_ref/train': 0.9998373985290527, 'perplexity_gain_vs_ref/test': 1.0002237558364868, 'perplexity_gain_vs_ref/oos': 0.9986141920089722, 'perplexity_gain_vs_ref/rnd': 0.9999231100082397, 'preference_logp_gain/train': 36.2110595703125, 'preference_logp_gain/test': 37.001220703125, 'preference_logp_gain/oos': 10.606689453125, 'preference_logp_gain/rnd': 13.494407653808594, 'preference_logp_gain_vs_ref/train': 0.06466764211654663, 'preference_logp_gain_vs_ref/test': -0.014073729515075684, 'preference_logp_gain_vs_ref/oos': 0.0719950869679451, 'preference_logp_gain_vs_ref/rnd': 0.00898289680480957}\n",
      "Time to get_next_trial: 0.015672683715820312\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "\n",
    "for _ in tqdm(range(450)):\n",
    "    t0 = time.time()\n",
    "    parameters, trial_index = ax_client.get_next_trial()\n",
    "    print(f\"Time to get_next_trial: {time.time() - t0}\")\n",
    "    try:\n",
    "        r = objective_func(**parameters)\n",
    "        print(parameters, r)\n",
    "    except KeyboardInterrupt:\n",
    "        ax_client.save_to_json_file(filepath=exp_f)\n",
    "        break\n",
    "    # except Exception as e:\n",
    "    #     logger.exception(f\"Error in objective_func: parameters={parameters}\")\n",
    "    #     continue\n",
    "    ax_client.complete_trial(trial_index=trial_index, raw_data=r)\n",
    "\n",
    "# best_parameters, metrics = ax_client.get_best_parameters()\n",
    "# best_parameters, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_client.save_to_json_file(filepath=exp_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast\n",
    "ax_client.get_best_parameters(use_model_predictions=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=ax_client.generation_strategy\n",
    "gg = g._nodes[1]\n",
    "# with torch around 1.3mins\n",
    "print(gg.model_kwargs['torch_device'])\n",
    "gg.model_kwargs['torch_device'] = torch.device(\"cuda\")\n",
    "gg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df = ax_client.generation_strategy.trials_as_df\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ax_client.get_trials_data_frame()\n",
    "df[key_metric].plot(xlabel=\"iteration\", ylabel=key_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ax_client.get_trials_data_frame()\n",
    "d = df.iloc[:, 4:].sort_values(key_metric, ascending=False)#.head(20)\n",
    "\n",
    "# remove columns ending with train test or rnd\n",
    "d = d.loc[:, ~d.columns.str.endswith(\"train\")]\n",
    "d = d.loc[:, ~d.columns.str.endswith(\"test\")]\n",
    "d = d.loc[:, ~d.columns.str.endswith(\"rnd\")]\n",
    "\n",
    "\n",
    "def make_pretty(styler):\n",
    "    styler.set_caption(\"Ax results\")\n",
    "    styler.background_gradient(axis=0, cmap=\"seismic_r\")\n",
    "    return styler\n",
    "\n",
    "\n",
    "make_pretty(d.style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve best parameters\n",
    "best_parameters, values = ax_client.get_best_parameters()\n",
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means, covariances = values\n",
    "means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=ax_client.generation_strategy\n",
    "gg = g._nodes[1]\n",
    "# with torch around 2mins\n",
    "gg.model_kwargs['torch_device'] = torch.device(\"cuda\")\n",
    "gg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more than 20 mins?\n",
    "ax_client.fit_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instant\n",
    "ax_client.fit_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ax.service.ax_client import AxClient, ObjectiveProperties\n",
    "from ax.utils.measurement.synthetic_functions import hartmann6\n",
    "from ax.utils.notebook.plotting import init_notebook_plotting, render\n",
    "\n",
    "init_notebook_plotting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.renderers.default = \"jupyterlab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_client.get_trial_parameters(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render(ax_client.get_optimization_trace())  # Objective_optimum is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot\n",
    "# render(ax_client.get_contour_plot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ax.plot.slice import plot_slice\n",
    "\n",
    "\n",
    "model = ax_client.generation_strategy.model\n",
    "ss = model.model_space.parameters\n",
    "ss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in ss.items():\n",
    "    if v.parameter_type.value in [1,2]:\n",
    "        render(plot_slice(model, k, key_metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(model.feature_importances(key_metric)).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
