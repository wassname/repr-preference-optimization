{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3438, 0.3141, 0.3421],\n",
       "        [0.3364, 0.3232, 0.3404],\n",
       "        [0.3374, 0.3313, 0.3313],\n",
       "        [0.3330, 0.3336, 0.3334],\n",
       "        [0.3346, 0.3323, 0.3332]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([\n",
    "    [ 0.0470, -0.0431,  0.0421],\n",
    "    [ 0.0282, -0.0119,  0.0402],\n",
    "    [0.0181, 0.0000, 0.0000],\n",
    "    [-0.0017,  0.0001, -0.0004],\n",
    "    [ 4.1447e-03, -2.7173e-03, -2.3842e-07]\n",
    "    ])\n",
    "x.softmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1225, 0.7424, 0.1351],\n",
       "        [0.2490, 0.5552, 0.1958],\n",
       "        [0.2582, 0.3709, 0.3709],\n",
       "        [0.3403, 0.3282, 0.3315],\n",
       "        [0.3093, 0.3548, 0.3360]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(-x*20, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3438, 0.3141, 0.3421],\n",
       "        [0.3364, 0.3230, 0.3406],\n",
       "        [0.3374, 0.3313, 0.3313],\n",
       "        [0.3330, 0.3336, 0.3334],\n",
       "        [0.3346, 0.3323, 0.3332]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(x.exp()-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0986,  0.0000,  0.6931])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(torch.tensor([1/3, 1, 2/1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.exp()/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = x - x.min(1, keepdim=True)[0] + 1e-3\n",
    "x1 / x1.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOXIC dpo\n",
    "\n",
    "- turn exach pair into ds row\n",
    "- shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('unalignment/toxic-dpo-v0.1') # this should give a a bigger # use map in batched mode to return more rows than it got\n",
    "def transform(row):\n",
    "    return {\n",
    "        \"chosen\": [{'role':'user', 'content': row['prompt']},{'role':'assistant', 'content': row['chosen']}],\n",
    "        \"rejected\": [{'role':'user', 'content': row['prompt']},{'role':'assistant', 'content': row['rejected']}]\n",
    "    }\n",
    "\n",
    "dataset1 = dataset.map(transform)\n",
    "dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model_name = \"NousResearch/Meta-Llama-3-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HelpSteer2\n",
    "\n",
    "https://github.com/jondurbin/bagel/blob/3c7d2410a5a5ad2fd31b63529ef541135feefce4/bagel/data_sources/helpsteer.py#L4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('nvidia/HelpSteer2')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from loguru import logger\n",
    "\n",
    "REFUSALS = [\n",
    "    re.compile(regexp)\n",
    "    for regexp in [\n",
    "        \"(against|violate) my programming\",\n",
    "        \"openai\",\n",
    "        \"language model\",\n",
    "        \"large language\",\n",
    "        \"as an? (ai|generative language|gpt|bot)\",\n",
    "        \"important to note\",\n",
    "        \"i do(n't| not) (possess|have|exhibit) (personal|consciousness|subjective)\",\n",
    "        \"personal (feelings|thoughts|emotions|desires|experiences|goals|objective|belief)\",\n",
    "        \"(can('t| ?not)|w(on't|will not)|unable.?) (\\\\w+\\\\s)+(with (that|your)|your \\\\w+|provide)\",\n",
    "        \"my limitations\",\n",
    "        \"the limitations of my\",\n",
    "        \"my abilities\",\n",
    "        \"violates my\",\n",
    "        \"i (can('t| ?not)|w(on't|will not)|am (not |un)able.?).{0,30}(you are|you're|your )\",\n",
    "        \"please note that\",\n",
    "        \"unethical|illegal|dangerous\",\n",
    "        \"a text-based\",\n",
    "        \"(engag(e|ing)|participat(e|ing)|be involved (in|with)|promot(e|ing)|discuss(ing)?|provid(e|ing))( in)?(\\\\s*\\\\w+ that)?(\\\\s+potentially)? (derogatory|inappropriate|offensive|discriminate|discriminatory|sexist|unacceptable|immoral|unethical|unacceptable|hateful|harmful)\",\n",
    "        \"i am commited to\",\n",
    "        \"adhere to safety guidelines\",\n",
    "        \"maintain user safety\",\n",
    "        \"about something else instead\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "def has_refusal(response):\n",
    "    \"\"\"Check if a response has a refusal - not perfect, but good enough...\"\"\"\n",
    "    for regexp in REFUSALS:\n",
    "        if m := regexp.search(response, re.I):\n",
    "            logger.warning(f\"Refusal? {m.group()}\")\n",
    "            return True\n",
    "    return False\n",
    "\n",
    ".filter(\n",
    "        lambda item: not has_refusal(item[\"response\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(known_uids=set([]), **_):\n",
    "    \"\"\"HelpSteer dataset for DPO.\"\"\"\n",
    "    logger.info(\"Loading HelpSteer dataset...\")\n",
    "    dataset = load_dataset(\"nvidia/HelpSteer\", split=\"train\").filter(\n",
    "        lambda item: not has_refusal(item[\"response\"])\n",
    "    )\n",
    "\n",
    "    # Use the highest correctness items as the \"chosen\" responses.\n",
    "    logger.info(\"Generating DPO pairs...\")\n",
    "    options = {}\n",
    "    for item in dataset:\n",
    "        uid = get_uid(item[\"prompt\"])\n",
    "        if uid not in options:\n",
    "            options[uid] = []\n",
    "        item[\"score\"] = (\n",
    "            item[\"helpfulness\"]\n",
    "            + item[\"correctness\"]\n",
    "            + item[\"coherence\"]\n",
    "            + item[\"complexity\"]\n",
    "            + item[\"verbosity\"]\n",
    "        )\n",
    "        options[uid].append(item)\n",
    "\n",
    "    # Select the chosen and rejected items.\n",
    "    data = []\n",
    "    for uid, responses in options.items():\n",
    "        if len(responses) < 2:\n",
    "            continue\n",
    "        if uid in known_uids:\n",
    "            continue\n",
    "        known_uids.add(uid)\n",
    "        chosen = None\n",
    "        chosen_score = 0\n",
    "        rejected = None\n",
    "        rejected_score = 0\n",
    "        for item in responses:\n",
    "            if item[\"correctness\"] == 4:\n",
    "                if chosen:\n",
    "                    if item[\"score\"] > chosen[\"score\"]:\n",
    "                        rejected = chosen[\"response\"]\n",
    "                        rejected_score = chosen[\"score\"]\n",
    "                        chosen = item\n",
    "                        chosen_score = item[\"score\"]\n",
    "                        break\n",
    "                else:\n",
    "                    chosen = item\n",
    "                    chosen_score = item[\"score\"]\n",
    "            else:\n",
    "                if not rejected:\n",
    "                    rejected = item[\"response\"]\n",
    "                    rejected_score = item[\"score\"]\n",
    "        if chosen and rejected and chosen[\"response\"] != rejected:\n",
    "            logger.success(f\"Found DPO pair: {chosen_score} vs {rejected_score}\")\n",
    "            data.append(\n",
    "                {\n",
    "                    \"id\": uid,\n",
    "                    \"source\": \"helpsteer\",\n",
    "                    \"prompt\": item[\"prompt\"],\n",
    "                    \"chosen\": chosen[\"response\"],\n",
    "                    \"rejected\": rejected,\n",
    "                    \"conversations\": None,\n",
    "                }\n",
    "            )\n",
    "    return Dataset.from_list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train']['prompt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is norm mean same as mse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "# b l t h \n",
    "x = torch.randn(2, 3, 4, 5)\n",
    "y = torch.randn(2, 3, 4 , 5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.3756), tensor(16.8840), tensor(1.2228), tensor(146.7348))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mse_loss(x, y), torch.norm(y-x, p=2).nanmean(), F.l1_loss(x, y), torch.norm(y-x, p=1).nanmean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2726)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e=y-x\n",
    "((e**2).sum(-1)**(1/2)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.2726)\n",
      "tensor(3.2726)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.2726)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.norm(y-x, p=2, dim=-1).nanmean())\n",
    "print(\n",
    "     torch.linalg.vector_norm(y-x, ord=2, dim=-1).nanmean()\n",
    ")\n",
    "torch.linalg.norm(y-x, ord=2, dim=-1).nanmean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
