{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with hidden states\n",
    "\n",
    "Question, is there a better representation of concepts in hidden states?\n",
    "\n",
    "Setup: we use DPO setup, with a chosen and rejected string. We then generate a set of hidden states, and compare the hidden states of the chosen and rejected string.\n",
    "\n",
    "Goal: better generalisation of desired behavuour by changing the internal representation of policy rather than directly changing the policy\n",
    "\n",
    "  - Hypothesis: rejected and chosen hidden states will - on mean - be best representated as rotations from each other\n",
    "  - alternate: either mean mass diff (linear) or no repr will be better\n",
    "  - metric: manual generation getting output while maintaining coherency, prediction other sets of hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"repo-dpo\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import DPOTrainer\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Union\n",
    "from jaxtyping import Float\n",
    "from einops import rearrange, reduce\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from reprpo.helpers.adapters import set_adapter\n",
    "from matplotlib import pyplot as plt\n",
    "from reprpo import silence\n",
    "from reprpo.gen import generation_test\n",
    "\n",
    "from reprpo.trainer import mean_with_attention, symlog, mult_with_attention\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from reprpo.trainer import collect_hs, ReprPOConfig, ReprPOTrainer, normalize_output, normalize_per\n",
    "from reprpo.helpers.shypothesis import shypothesis\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7227185f8f4dbdba86f1d3cf9aa07c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from reprpo.models.load import load_model, print_trainable_parameters\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "model_name = \"NousResearch/Meta-Llama-3-8B-Instruct\"\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "## Big adapter\n",
    "# peft_config = LoraConfig(\n",
    "#     lora_alpha=16, \n",
    "#     r=16,\n",
    "#     lora_dropout=0.0,\n",
    "#     use_rslora=False,\n",
    "#     # use_dora=True,\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "# )\n",
    "\n",
    "use_gradient_checkpointing = False\n",
    "model, tokenizer = load_model(model_name, bnb=False) \n",
    "# from trl.trainer.utils import peft_module_casting_to_bf16\n",
    "# peft_module_casting_to_bf16(model)\n",
    "if use_gradient_checkpointing:\n",
    "    model.enable_input_require_grads()\n",
    "# adapter_name='ReprPO2'\n",
    "# model = prepare_model_for_kbit_training(model, {'use_gradient_checkpointing': use_gradient_checkpointing})\n",
    "# model = get_peft_model(model, peft_config, adapter_name=adapter_name)\n",
    "# print_trainable_parameters(model)\n",
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dpo_adapter_f = './output-dir/dpo/DPO'\n",
    "# model.load_adapter(dpo_adapter_f, 'DPO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # QC model and adapter is coherent\n",
    "# generation_test(model, tokenizer, max_new_tokens=24, system='no yapping', adapter_names=[None, 'DPO'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DPO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected'],\n",
       "        num_rows: 160\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected'],\n",
       "        num_rows: 160\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sample(dataset, N):\n",
    "    return (dataset\n",
    "            .shuffle(42)\n",
    "            .select(range(\n",
    "            min(len(dataset),\n",
    "                N)))\n",
    "    )\n",
    "\n",
    "dataset = load_dataset('Atsunori/HelpSteer2-DPO')\n",
    "dataset['train'] = sample(dataset['train'], num_samples)\n",
    "dataset['validation'] = sample(dataset['validation'], num_samples)\n",
    "dataset2 = dataset.rename_column('chosen_response', 'chosen').rename_column('rejected_response', 'rejected')\n",
    "dataset2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect HS in DPO way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<bound method DPOTrainer.tokenize_row of <reprpo.trainer.ReprPOTrainer object at 0x75dce79b73a0>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f05c8c22feb84f1792ca91b2bc491bbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/160 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = ReprPOConfig('./output-dir/scratch',\n",
    "    per_device_train_batch_size=3,\n",
    "    per_device_eval_batch_size=3,\n",
    "    bf16=True,\n",
    "    # tf32=True,\n",
    "    max_prompt_length=64,\n",
    "    max_length=128,\n",
    "    collection_layers=[20,],\n",
    "    remove_unused_columns=False,\n",
    "\n",
    "    # optim = \"adamw_8bit\",\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    learning_rate=1e-3,\n",
    "    gradient_checkpointing=use_gradient_checkpointing,\n",
    "                             )\n",
    "reprpo_trainer = ReprPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=None,\n",
    "    args=training_args,\n",
    "    beta=training_args.beta,\n",
    "    train_dataset=dataset2[\"train\"],\n",
    "    # eval_dataset=dataset2[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 128])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# QC get dpo catch\n",
    "dl = reprpo_trainer.get_train_dataloader()\n",
    "batch = next(iter(dl))\n",
    "batch['chosen_input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # QC view a typical input to the model (since the dpo trainer transformes in dataset, concatenating chosen and rejecting along the batch dimension)\n",
    "# batch_concat = reprpo_trainer.concatenated_inputs(\n",
    "#             batch,\n",
    "#             is_encoder_decoder=reprpo_trainer.is_encoder_decoder,\n",
    "#             label_pad_token_id=reprpo_trainer.label_pad_token_id,\n",
    "#             padding_value=reprpo_trainer.padding_value,\n",
    "#             device=reprpo_trainer.accelerator.device,\n",
    "#             max_length=reprpo_trainer.args.max_length,\n",
    "#         )\n",
    "# layer_idx = 0\n",
    "# print(batch_concat.keys())\n",
    "# batch['chosen_input_ids'].shape, batch_concat['concatenated_input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baukit.nethook import recursive_copy, set_requires_grad\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedSVDDecomposer:\n",
    "    \"\"\"\n",
    "    Decompose hidden states into part that is used by the lm_head (hs_h) and part that is residual (hs_r).\n",
    "    \"\"\"\n",
    "    def __init__(self, lm_head: nn.Linear, epsilon: float = 1e-12):\n",
    "        self.W = W = lm_head.weight\n",
    "        U, S, Vt = torch.linalg.svd(W, full_matrices=False)\n",
    "        \n",
    "        # Precompute V * (S^2 / (S^2 + epsilon)) * V^T for stability\n",
    "        S_squared = S**2\n",
    "        stability_term = S_squared / (S_squared + epsilon)\n",
    "        self.projection_matrix = Vt.T @ (stability_term.unsqueeze(1) * Vt)\n",
    "        \n",
    "        # Store for potential debugging or analysis\n",
    "        self.S = S\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def decompose(self, hs: Float[Tensor, \"batch hidden_size\"]) -> Tuple[Float[Tensor, \"batch hidden_size\"], Float[Tensor, \"batch hidden_size\"]]:\n",
    "        hs_h = (self.projection_matrix @ hs.T).T\n",
    "        hs_r = hs - hs_h\n",
    "        return hs_r, hs_h\n",
    "    \n",
    "    def decompose_layers(self, hs):\n",
    "        \"\"\"this version flattens all dimensions except the last\"\"\"\n",
    "        original_shape = hs.shape\n",
    "        hs_flat = hs.view(-1, original_shape[-1]).to(self.projection_matrix.dtype).to(self.projection_matrix.device)\n",
    "\n",
    "        # Compute projection\n",
    "        hs_h_flat = (self.projection_matrix @ hs_flat.T).T\n",
    "        hs_r_flat = hs_flat - hs_h_flat\n",
    "        \n",
    "        # Reshape back to original shape\n",
    "        hs_h = hs_h_flat.view(original_shape)\n",
    "        hs_r = hs_r_flat.view(original_shape)\n",
    "        \n",
    "        return hs_r, hs_h\n",
    "    \n",
    "    def reconstruct_from_residual(self, hs_r: Float[Tensor, \"batch hidden_size\"]) -> Float[Tensor, \"batch hidden_size\"]:\n",
    "        return  hs_r + (self.projection_matrix @ hs_r.T).T\n",
    "\n",
    "def check_orthogonality(H_residual, W):\n",
    "    proj = W @ H_residual.T\n",
    "    orthogonality = torch.norm(proj) / (torch.norm(W) * torch.norm(H_residual))\n",
    "    print(f\"Orthogonality measure: {orthogonality.item():.3f} (should be close to 0)\")\n",
    "    return orthogonality\n",
    "\n",
    "# # # Usage\n",
    "# decomposer = OptimizedSVDDecomposer(lm_head)  # Adjust epsilon as needed\n",
    "# hs_r, hs_h = decomposer.decompose(H)\n",
    "# H_reconstructed = decomposer.reconstruct_from_residual(hs_r)\n",
    "# error = torch.norm(H - H_reconstructed, dim=-1).mean()\n",
    "# print(error)\n",
    "# check_orthogonality(hs_r, lm_head.weight);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from typing import Tuple\n",
    "from jaxtyping import Float\n",
    "\n",
    "class OptimizedSVDDecomposer:\n",
    "    def __init__(self, lm_head: nn.Linear, epsilon: float = 1e-12):\n",
    "        W = lm_head.weight\n",
    "        U, S, Vt = torch.linalg.svd(W, full_matrices=False)\n",
    "        \n",
    "        self.S = S\n",
    "        self.Vt = Vt\n",
    "        self.epsilon = epsilon\n",
    "        print(f\"Using full SVD with {len(S)} singular values\")\n",
    "\n",
    "    def decompose(self, hs: Float[Tensor, \"batch layers tokens hidden_size\"]) -> Tuple[Float[Tensor, \"batch layers tokens hidden_size\"], Float[Tensor, \"batch layers tokens hidden_size\"]]:\n",
    "        original_shape = hs.shape\n",
    "        hs_flat = hs.view(-1, original_shape[-1]).to(self.Vt.dtype).to(self.Vt.device)\n",
    "        \n",
    "        # Project onto the right singular vectors\n",
    "        projection = self.Vt @ hs_flat.T\n",
    "        \n",
    "        # Apply a soft thresholding\n",
    "        S_inv = self.S / (self.S**2 + self.epsilon)\n",
    "        soft_threshold = torch.sign(projection) * torch.max(torch.abs(projection) - self.epsilon, torch.zeros_like(projection))\n",
    "        \n",
    "        # Reconstruct\n",
    "        hs_h_flat = (self.Vt.T @ (S_inv.unsqueeze(1) * soft_threshold)).T\n",
    "        hs_r_flat = hs_flat - hs_h_flat\n",
    "        \n",
    "        hs_h = hs_h_flat.view(original_shape)\n",
    "        hs_r = hs_r_flat.view(original_shape)\n",
    "        \n",
    "        return hs_r, hs_h\n",
    "\n",
    "    def estimate_error(self, hs: Float[Tensor, \"batch layers tokens hidden_size\"]) -> float:\n",
    "        hs_r, _ = self.decompose(hs)\n",
    "        relative_error = torch.norm(hs_r) / torch.norm(hs)\n",
    "        return relative_error.item()\n",
    "\n",
    "    def get_condition_number(self) -> float:\n",
    "        return (self.S[0] / self.S[-1]).item()\n",
    "    \n",
    "    # def reconstruct_from_residual(self, hs_r: Float[Tensor, \"batch layers tokens hidden_size\"]) -> Float[Tensor, \"batch layers tokens hidden_size\"]:\n",
    "    #     hs_r \n",
    "    #     return hs_h\n",
    "\n",
    "# # Usage\n",
    "# decomposer = OptimizedSVDDecomposer(lm_head, epsilon=1e-12)\n",
    "\n",
    "# # For R matrix\n",
    "# hs_r_R, hs_h_R = decomposer.decompose(R)\n",
    "# error_R = decomposer.estimate_error(R)\n",
    "\n",
    "# # For C matrix\n",
    "# hs_r_C, hs_h_C = decomposer.decompose(C)\n",
    "# error_C = decomposer.estimate_error(C)\n",
    "\n",
    "# print(f\"R matrix - Estimated relative error: {error_R}\")\n",
    "# print(f\"C matrix - Estimated relative error: {error_C}\")\n",
    "# print(f\"Frobenius norm of difference in residuals: {torch.norm(hs_r_R - hs_r_C)}\")\n",
    "# print(f\"Relative difference in residuals: {torch.norm(hs_r_R - hs_r_C) / torch.norm(hs_r_R)}\")\n",
    "\n",
    "# # Additional analysis\n",
    "# print(f\"Frobenius norm of difference between R and C: {torch.norm(R - C)}\")\n",
    "# print(f\"Relative difference between R and C: {torch.norm(R - C) / torch.norm(R)}\")\n",
    "# print(f\"Condition number: {decomposer.get_condition_number()}\")\n",
    "\n",
    "# # Analyze the differences in the head space\n",
    "# hs_h_diff = torch.norm(hs_h_R - hs_h_C) / torch.norm(hs_h_R)\n",
    "# print(f\"Relative difference in head space projections: {hs_h_diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone this linear, using a clone of the weights and bias\n",
    "m = model.lm_head\n",
    "m2 = nn.Linear(m.in_features, m.out_features)\n",
    "m2.weight = nn.Parameter(m.weight.clone().float())\n",
    "if m.bias is not None:\n",
    "    m2.bias = nn.Parameter(m.bias.clone().float())\n",
    "m2 = m2.cpu()\n",
    "\n",
    "lm_head = m2\n",
    "m = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using full SVD with 3072 singular values\n"
     ]
    }
   ],
   "source": [
    "decomposer = OptimizedSVDDecomposer(lm_head)  # Adjust epsilon as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get batch of hidden states\n",
    "from reprpo.helpers.torch import clear_mem\n",
    "from trl.trainer.utils import pad_to_length\n",
    "from torch import Tensor\n",
    "from jaxtyping import Float\n",
    "\n",
    "\n",
    "\n",
    "# @torch.cuda.amp.autocast()\n",
    "def get_hs(trainer, model, batch, collect_gradient=False):\n",
    "    model.zero_grad()\n",
    "    model.train()\n",
    "\n",
    "    if collect_gradient:\n",
    "        raise NotImplementedError('Not implemented yet, gradient is None?')\n",
    "        def make_inputs_require_grad(module, input, output):\n",
    "            output.requires_grad_(True)\n",
    "        model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n",
    "\n",
    "    \n",
    "    (\n",
    "        chosen_logps,\n",
    "        rejected_logps,\n",
    "        chosen_logits,\n",
    "        rejected_logits,\n",
    "        _,\n",
    "        chosen_hs,\n",
    "        rejected_hs,\n",
    "        chosen_attn_mask,\n",
    "        rejected_attn_mask\n",
    "    ) = trainer.concatenated_forward(trainer.model, batch)\n",
    "\n",
    "    r = dict(chosen_hs=chosen_hs, rejected_hs=rejected_hs, chosen_logps=chosen_logps, rejected_logps=rejected_logps, chosen_attn_mask=chosen_attn_mask, rejected_attn_mask=rejected_attn_mask)\n",
    "\n",
    "\n",
    "    if collect_gradient:\n",
    "        chosen_hs.retain_grad()\n",
    "        rejected_hs.retain_grad()\n",
    "        loss = (chosen_logps - rejected_logps).mean()\n",
    "        trainer.accelerator.backward(loss)\n",
    "        print('chosen_hs.grad', chosen_hs.grad)\n",
    "        assert chosen_hs.grad is not None, 'FIXME'\n",
    "        r['chosen_hs_grad'] = chosen_hs.grad\n",
    "        r['rejected_hs_grad'] = rejected_hs.grad\n",
    "\n",
    "\n",
    "\n",
    "    # get unembected hs\n",
    "    r['chosen_unemb'] = model.lm_head(chosen_hs)\n",
    "    r['rejected_unemb'] = model.lm_head(rejected_hs)\n",
    "\n",
    "    def get_layer_logps(trainer, hidden_states: Float[Tensor, 'b l t h'], labels: Float[Tensor, 'b t'], log_softmax=True):\n",
    "        # pad to length\n",
    "        hidden_states = pad_to_length(hidden_states, trainer.max_length, pad_value=0)\n",
    "        labels = pad_to_length(labels, trainer.max_length, pad_value=trainer.label_pad_token_id)\n",
    "\n",
    "        # gather for each layer\n",
    "        logps = []\n",
    "        for layer in range(hidden_states.shape[1]):\n",
    "            all_logps, size_completion = trainer.get_batch_logps(\n",
    "                hidden_states[:, layer],\n",
    "                labels,\n",
    "                label_pad_token_id=trainer.label_pad_token_id,\n",
    "                log_softmax=log_softmax\n",
    "                )\n",
    "            all_logps = all_logps / size_completion\n",
    "            logps.append(all_logps)\n",
    "        all_logps = torch.stack(logps, dim=1)\n",
    "        return all_logps\n",
    "    \n",
    "    # get fake logp from unemb_hs\n",
    "    r['chosen_gthr_logps_unemb'] = get_layer_logps(trainer, r['chosen_unemb'], batch[\"chosen_labels\"])\n",
    "    r['rejection_gthr_logps_unemb'] = get_layer_logps(trainer, r['rejected_unemb'], batch[\"rejected_labels\"])\n",
    "\n",
    "    r['chosen_gthr_unemb'] = get_layer_logps(trainer, r['chosen_unemb'], batch[\"chosen_labels\"], log_softmax=False)\n",
    "    r['rejection_gthr_unemb'] = get_layer_logps(trainer, r['rejected_unemb'], batch[\"rejected_labels\"], log_softmax=False)\n",
    "    \n",
    "    # we reuse the function, adding a fake layer dim, and squeezing it out\n",
    "    r['chosen_gthr_logits'] = get_layer_logps(trainer, chosen_logits[:, None], batch[\"chosen_labels\"], log_softmax=False).squeeze(1)\n",
    "    r['rejected_gthr_logits'] = get_layer_logps(trainer, rejected_logits[:, None], batch[\"rejected_labels\"], log_softmax=False).squeeze(1)\n",
    "\n",
    "    # unproject for chosen_hs and rejected_hs\n",
    "    for k in list(r.keys()):\n",
    "        if k.endswith('_hs'):\n",
    "            hs_r, hs_h = decomposer.decompose(r[k])\n",
    "            r[k+'_r'] = hs_r\n",
    "            r[k+'_h'] = hs_h\n",
    "\n",
    "    r = {k: recursive_copy(v, clone=True, detach=True).detach().cpu() for k, v in r.items()}\n",
    "    loss = chosen_hs = rejected_hs = chosen_logps = rejected_logps = chosen_logits = rejected_logits = None\n",
    "    clear_mem(trainer)\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e4d2136fac4242a6535b4311700be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([66, 1, 128, 3072])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "n = 0 \n",
    "\n",
    "dl = reprpo_trainer.get_train_dataloader()\n",
    "data = []\n",
    "for batch in tqdm(dl):\n",
    "    with reprpo_trainer.null_ref_context():\n",
    "        r = get_hs(reprpo_trainer, reprpo_trainer.model, batch)\n",
    "        data.append(r)\n",
    "        n += r['chosen_hs'].shape[0]\n",
    "        if n > 64:\n",
    "            break\n",
    "\n",
    "# concat\n",
    "data = {k: torch.cat([d[k] for d in data], dim=0) for k in data[0].keys()}\n",
    "data['chosen_hs'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_mem(reprpo_trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO try getting residual hidden states are removing what might be used by lm_head, e.g. disentangling the part not projected\n",
    "model = None\n",
    "clear_mem(reprpo_trainer)\n",
    "reprpo_trainer = None\n",
    "clear_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disentangling hiddenstates into inner and head components\n",
    "\n",
    "Hypothesis: hidden states contain both \"output-directed\" information (what will be unembedded to logits) and \"internal processing\" information. This seems possible because 1) we can unembed the hidden states 2) the hidden states remain fairly constant throughout the layer being additivly transformed by residual connections.\n",
    "\n",
    "Test: do we get a better correlation with concepts. And does it generalise?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['chosen_hs', 'rejected_hs', 'chosen_logps', 'rejected_logps', 'chosen_attn_mask', 'rejected_attn_mask', 'chosen_unemb', 'rejected_unemb', 'chosen_gthr_logps_unemb', 'rejection_gthr_logps_unemb', 'chosen_gthr_unemb', 'rejection_gthr_unemb', 'chosen_gthr_logits', 'rejected_gthr_logits', 'chosen_hs_r', 'chosen_hs_h', 'rejected_hs_r', 'rejected_hs_h'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hsC_r, hsC_h = decomposer.decompose(data['chosen_hs'])\n",
    "# hsR_r, hsR_h = decomposer.decompose(data['rejected_hs'])\n",
    "# (hsC_r-hsC_r).mean(), (hsC_h-hsC_h).mean(), (hsR_r-hsR_r).mean(), (hsR_h-hsR_h).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hsC_r, hsC_h = decomposer.decompose(data['chosen_hs'][:, 0, 0].float())\n",
    "# hsR_r, hsR_h = decomposer.decompose(data['rejected_hs'][:, 0, 0].float())\n",
    "# (hsC_r-hsC_r).mean(), (hsC_h-hsC_h).mean(), (hsR_r-hsR_r).mean(), (hsR_h-hsR_h).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decomposer = OptimizedSVDDecomposer(lm_head, epsilon=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H = data['rejected_hs'][:, 0, 0].float()\n",
    "# hs_r, hs_h = decomposer.decompose(H)\n",
    "# H_reconstructed = decomposer.reconstruct_from_residual(hs_r)-H\n",
    "# error = torch.norm(H - H_reconstructed, dim=-1).mean()\n",
    "# error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H = data['chosen_hs'][:, 0, 0].float()\n",
    "# hs_r, hs_h = decomposer.decompose(H)\n",
    "# H_reconstructed = decomposer.reconstruct_from_residual(hs_r)-H\n",
    "# error = torch.norm(H - H_reconstructed, dim=-1).mean()\n",
    "# error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1335.6732)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data['chosen_hs_h']-data['rejected_hs_h']).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4509.4600)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data['chosen_hs_r']-data['rejected_hs_r']).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4509.4600)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data['chosen_hs_r']-data['rejected_hs_r']).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get some data samples\n",
    "layer = 0\n",
    "C = data['chosen_hs_r'].float() # we could also consider unembedding them, whith the lm head, this might make them more interpretable, but also a bit large\n",
    "R = data['rejected_hs_r'].float()\n",
    "\n",
    "CA = data['chosen_attn_mask']\n",
    "RA = data['rejected_attn_mask']\n",
    "\n",
    "M = 100\n",
    "A2 = CA * RA # use both attn masks when comparing?\n",
    "\n",
    "# choose layer, mean over tokens\n",
    "C = mult_with_attention(C, A2)[:M, layer]\n",
    "C = reduce(C, 'b t h -> b h', 'mean')\n",
    "R = mult_with_attention(R, A2)[:M, layer]\n",
    "R = reduce(R, 'b t h -> b h', 'mean')\n",
    "\n",
    "# compare two unrelated sets of samples, that way we have ones that should show the difference and ones that shouldn't show the difference we arel ooking for\n",
    "n = len(C)//2\n",
    "C1 = C[:n] # chosen pair 1\n",
    "R1 = R[:n] # rejected pair 1\n",
    "C2 = C[n:] # chosen, pair 2\n",
    "R2 = R[n:] # rejected pair 2\n",
    "\n",
    "\n",
    "# now we choose what to test correlations with. At first I tried the logprobs but they have been through a softmax which makes them relative and hard to compare to the hidden states\n",
    "# so instead we are going to try the hidden states values that corresponded to the \n",
    "\n",
    "# logratios = data['chosen_logps'] - data['rejected_logps'] # the logp is the log probability (mean per token) of this response, logratios is the log ratio of probs, this should be correlated with the direction we are seeking in the hidden states\n",
    "\n",
    "# logratios = (data['chosen_gthr_logits'] - data['rejected_gthr_logits'])#.exp()\n",
    "logratios = data['chosen_gthr_unemb'][:, layer] - data['rejection_gthr_unemb'][:, layer]\n",
    "logratios = logratios.float()\n",
    "\n",
    "# we can use this to check the null hypothesis, as the `roll` operation mixes up the batch dimension, so they are unrelated\n",
    "# logratios_unrelated = logratios.roll(1, 0)\n",
    "\n",
    "logratios1 = logratios[:n]\n",
    "logratios2 = logratios[n:]\n",
    "logratios2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n",
    "\n",
    "### Magnitude/norm is correlated with result?\n",
    "\n",
    "Is the amplitude of A-B related to the logp?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "does the magnitude of the hs_unemb correlate with the prob ratio (across batches)? if so it's a good repr\n",
      "should be significantly bigger\n",
      "Hypothesis: corr>corr_null\n",
      "          : 0.494 > 0.140\n",
      "Residual  : = corr - corr_null\n",
      "Residual  : = 0.353 ✅\n",
      "\n",
      "should pass\n",
      "Hypothesis: corr>corr_null\n",
      "          : 0.184 > -0.172\n",
      "Residual  : = corr - corr_null\n",
      "Residual  : = 0.356 ✅\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ur using abs corrcoef\n",
    "def acorr(A, B):\n",
    "    return (np.corrcoef(A, B, rowvar=True)[0, 1])\n",
    "\n",
    "def mag_corr(C, R, logratios):\n",
    "    hs_d = torch.norm(C-R, dim=-1, p=2) # get magnitude of the difference\n",
    "    corr = acorr(hs_d, logratios)\n",
    "    # note that after flipping we ruin the order, and they should be uncorrelated except for the middle one if it's odd\n",
    "    corr_null = acorr(hs_d, logratios.roll(1, 0))\n",
    "    return corr, corr_null\n",
    "    \n",
    "\n",
    "print('does the magnitude of the hs_unemb correlate with the prob ratio (across batches)? if so it\\'s a good repr')\n",
    "\n",
    "print(\"should be significantly bigger\")\n",
    "corr, corr_null = mag_corr(C1, R1, logratios1)\n",
    "shypothesis(\"corr>corr_null\", variables=dict(corr=corr, corr_null=corr_null))\n",
    "\n",
    "print(\"should pass\")\n",
    "corr, corr_null = mag_corr(C2, R2, logratios2)\n",
    "shypothesis(\"corr>corr_null\", variables=dict(corr=corr, corr_null=corr_null))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is it significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is the red line outside the distribution of null correlations?\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-value: 0.029\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-value: 0.235\n"
     ]
    }
   ],
   "source": [
    "def permutation_test(C, R, logratios, n_permutations=100):\n",
    "\n",
    "    n_permutations = min(n_permutations, len(logratios))\n",
    "\n",
    "    true_corr = np.corrcoef(\n",
    "        torch.norm(C - R, dim=-1, p=2), \n",
    "        logratios)[0, 1]\n",
    "    \n",
    "    null_corrs = []\n",
    "    for _ in range(n_permutations):\n",
    "        perm_logratios = logratios[torch.randperm(len(logratios))]\n",
    "        null_corrs.append(np.corrcoef(torch.norm(C - R, dim=-1, p=2), perm_logratios)[0, 1])\n",
    "    \n",
    "    plt.hist(null_corrs, bins=50, label='null_corrs')\n",
    "    plt.axvline(true_corr, color='r', linestyle='dashed', linewidth=2, label='true_corr')\n",
    "    plt.title('Permutation Test')\n",
    "    plt.xlabel('Correlation')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    p_value = (np.sum(np.abs(null_corrs) >= np.abs(true_corr)) + 1) / (n_permutations + 1)\n",
    "    print(f\"P-value: {p_value:.3f}\")\n",
    "\n",
    "print('is the red line outside the distribution of null correlations?')\n",
    "permutation_test(C1, R1, logratios1)\n",
    "permutation_test(C2, R2, logratios2)\n",
    "# FAIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spearmanr kendalltau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spearmanr measure of dist ~ logratios should be large and significant\n",
      "Hypothesis: corr>n_corr\n",
      "          : 0.256 > 0.092\n",
      "Residual  : = corr - n_corr\n",
      "Residual  : = 0.164 ✅\n",
      "\n",
      "Hypothesis: p<0.2\n",
      "          : 0.150 < 0.200\n",
      "Residual  : = p - 0.2\n",
      "Residual  : = -0.050 ✅\n",
      "\n",
      "kendalltau measure of dist ~ logratios should be large and significant\n",
      "Hypothesis: corr>n_corr\n",
      "          : 0.170 > 0.078\n",
      "Residual  : = corr - n_corr\n",
      "Residual  : = 0.092 ✅\n",
      "\n",
      "Hypothesis: p<0.2\n",
      "          : 0.167 < 0.200\n",
      "Residual  : = p - 0.2\n",
      "Residual  : = -0.033 ✅\n",
      "\n",
      "Hypothesis: n_p>0.2\n",
      "          : 0.525 > 0.200\n",
      "Residual  : = n_p - 0.2\n",
      "Residual  : = 0.325 ✅\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "\n",
    "def correlation_dist(C, R, logratios):\n",
    "    distances = torch.norm(C - R, dim=-1)\n",
    "    logratios = logratios.cpu().numpy()\n",
    "    corr, p = stats.spearmanr(distances, logratios)\n",
    "    return np.abs(corr), p\n",
    "\n",
    "print('spearmanr measure of dist ~ logratios should be large and significant')\n",
    "corr, p = correlation_dist(C1, R1, logratios1)\n",
    "n_corr, n_p = correlation_dist(C1, R1, logratios1.roll(1, 0))\n",
    "shypothesis('corr>n_corr', variables=locals())\n",
    "shypothesis('p<0.2', variables=locals())\n",
    "\n",
    "def kendalltau_dist(C, R, logratios):\n",
    "    distances = torch.norm(C - R, dim=-1)\n",
    "    logratios = logratios.cpu().numpy()\n",
    "    corr, p = stats.kendalltau(distances, logratios)\n",
    "    return np.abs(corr), p\n",
    "\n",
    "print('kendalltau measure of dist ~ logratios should be large and significant')\n",
    "corr, p = kendalltau_dist(C1, R1, logratios1)\n",
    "n_corr, n_p = kendalltau_dist(C1, R1, logratios1.roll(1, 0))\n",
    "shypothesis('corr>n_corr', variables=locals())\n",
    "shypothesis('p<0.2', variables=locals())\n",
    "shypothesis('n_p>0.2', variables=locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing angle_similarity_correlation:\n",
      "Similarity of related ones should be higher\n",
      "Hypothesis: c111 > c112\n",
      "          : c111 > c112\n",
      "     Where: {'c112': 0.04393696167948609, 'c111': 0.43047465427745973}\n",
      "          : 0.430 > 0.044\n",
      "Residual  : = c111 - c112\n",
      "Residual  : = 0.387 ✅\n",
      "\n",
      "Hypothesis: c222 > c221\n",
      "          : 0.049 > 0.175\n",
      "Residual  : = -c221 + c222\n",
      "Residual  : = -0.126 ❌\n",
      "\n",
      "\n",
      "Testing norm_difference_correlation:\n",
      "Similarity of related ones should be higher\n",
      "Hypothesis: c111 > c112\n",
      "          : c111 > c112\n",
      "     Where: {'c112': 0.08703309300812705, 'c111': 0.36625025494666746}\n",
      "          : 0.366 > 0.087\n",
      "Residual  : = c111 - c112\n",
      "Residual  : = 0.279 ✅\n",
      "\n",
      "Hypothesis: c222 > c221\n",
      "          : 0.026 > 0.052\n",
      "Residual  : = -c221 + c222\n",
      "Residual  : = -0.026 ❌\n",
      "\n",
      "\n",
      "Testing magnitude_correlation:\n",
      "Similarity of related ones should be higher\n",
      "Hypothesis: c111 > c112\n",
      "          : c111 > c112\n",
      "     Where: {'c112': 0.18932139533856315, 'c111': 0.15721044738795775}\n",
      "          : 0.157 > 0.189\n",
      "Residual  : = c111 - c112\n",
      "Residual  : = -0.032 ❌\n",
      "\n",
      "Hypothesis: c222 > c221\n",
      "          : 0.207 > 0.339\n",
      "Residual  : = -c221 + c222\n",
      "Residual  : = -0.132 ❌\n",
      "\n",
      "\n",
      "Testing inner_product_correlation:\n",
      "Similarity of related ones should be higher\n",
      "Hypothesis: c111 > c112\n",
      "          : c111 > c112\n",
      "     Where: {'c112': 0.04696282677986383, 'c111': 0.10119931589724394}\n",
      "          : 0.101 > 0.047\n",
      "Residual  : = c111 - c112\n",
      "Residual  : = 0.054 ✅\n",
      "\n",
      "Hypothesis: c222 > c221\n",
      "          : 0.078 > 0.222\n",
      "Residual  : = -c221 + c222\n",
      "Residual  : = -0.144 ❌\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def angle_similarity_correlation(C, R, logratios):\n",
    "    C_norm = F.normalize(C, p=2, dim=-1)\n",
    "    R_norm = F.normalize(R, p=2, dim=-1)\n",
    "    angle = torch.acos(torch.clamp(torch.sum(C_norm * R_norm, dim=-1), -1.0, 1.0))\n",
    "    corrs = np.corrcoef(angle.cpu().numpy(), logratios.cpu().numpy())[0, 1]\n",
    "    return np.abs(corrs)\n",
    "\n",
    "def norm_difference_correlation(C, R, logratios):\n",
    "    norm_diff = torch.norm(C, dim=-1) - torch.norm(R, dim=-1)\n",
    "    corrs = np.corrcoef(norm_diff.cpu().numpy(), logratios.cpu().numpy())[0, 1]\n",
    "    return np.abs(corrs)\n",
    "\n",
    "def magnitude_correlation(C, R, logratios):\n",
    "    magnitude_diff = torch.abs(C - R).mean(dim=-1)\n",
    "    corrs = np.corrcoef(magnitude_diff.cpu().numpy(), logratios.cpu().numpy())[0, 1]\n",
    "    return np.abs(corrs)\n",
    "\n",
    "def inner_product_correlation(C, R, logratios):\n",
    "    inner_prod = torch.sum(C * R, dim=-1)\n",
    "    corrs = np.corrcoef(inner_prod.cpu().numpy(), logratios.cpu().numpy())[0, 1]\n",
    "    return np.abs(corrs)\n",
    "\n",
    "# Usage\n",
    "def run_all_tests(C1, R1, C2, R2, logratios1, logratios2):\n",
    "    tests = [\n",
    "        angle_similarity_correlation,\n",
    "        norm_difference_correlation,\n",
    "        magnitude_correlation,\n",
    "        inner_product_correlation\n",
    "    ]\n",
    "    \n",
    "    for test in tests:\n",
    "        print(f\"\\nTesting {test.__name__}:\")\n",
    "        c111 = test(C1, R1, logratios1)\n",
    "        c222 = test(C2, R2, logratios2)\n",
    "        c112 = test(C1, R1, logratios1.roll(1, 0))\n",
    "        c221 = test(C2, R2, logratios2.roll(1, 0))\n",
    "        \n",
    "        print('Similarity of related ones should be higher')\n",
    "        shypothesis('c111 > c112', variables=locals(), verbose=True)\n",
    "        shypothesis('c222 > c221', variables=locals())\n",
    "        # Uncomment these if you want to test across different pairs\n",
    "        # shypothesis('c111 > c221', variables=locals())\n",
    "        # shypothesis('c222 > c112', variables=locals())\n",
    "\n",
    "# Run all tests\n",
    "run_all_tests(C1, R1, C2, R2, logratios1, logratios2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H: cosine_similarity_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity of related ones should be higher\n",
      "Hypothesis: c111 > c112\n",
      "          : c111 > c112\n",
      "     Where: {'c112': 0.07685789012498802, 'c111': 0.48415128775540195}\n",
      "          : 0.484 > 0.077\n",
      "Residual  : = c111 - c112\n",
      "Residual  : = 0.407 ✅\n",
      "\n",
      "Hypothesis: c222 > c221\n",
      "          : 0.081 > 0.172\n",
      "Residual  : = -c221 + c222\n",
      "Residual  : = -0.091 ❌\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity_correlation(C, R, logratios):\n",
    "    cos_sim = F.cosine_similarity(C, R, dim=-1)  # [batch, layers, tokens]\n",
    "\n",
    "    corrs = np.corrcoef(cos_sim, logratios)[0, 1] \n",
    "    \n",
    "    return np.abs(corrs)\n",
    "\n",
    "c111 = cosine_similarity_correlation(C1, R1, logratios1)\n",
    "c222 = cosine_similarity_correlation(C2, R2, logratios2)\n",
    "c112 = cosine_similarity_correlation(C1, R1, logratios1.roll(1, 0))\n",
    "c221 = cosine_similarity_correlation(C2, R2, logratios2.roll(1, 0))\n",
    "\n",
    "print('cosine similarity of related ones should be higher')\n",
    "shypothesis('c111 > c112', variables=locals(), verbose=True)\n",
    "shypothesis('c222 > c221', variables=locals())\n",
    "# shypothesis('c111 > c221', variables=locals())\n",
    "# shypothesis('c222 > c112', variables=locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C1-R1 with logratios1:\n",
      "Hypothesis: mi1>mi2\n",
      "          : nan > nan\n",
      "Residual  : = mi1 - mi2\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid NaN comparison",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m kl2,js2,mi2 \u001b[38;5;241m=\u001b[39m info_theoretic_correlation(C1, R1, logratios1\u001b[38;5;241m.\u001b[39mroll(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# shypothesis('kl1>kl2', variables=locals())\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# shypothesis('js1>js2', variables=locals())\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[43mshypothesis\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmi1>mi2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC2-R2 with logratios2:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m kl1,js1,mi1 \u001b[38;5;241m=\u001b[39m info_theoretic_correlation(C2, R2, logratios2)\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/repr-preference-optimization/reprpo/helpers/shypothesis.py:56\u001b[0m, in \u001b[0;36mshypothesis\u001b[0;34m(hypothesis, variables, round, verbose)\u001b[0m\n\u001b[1;32m     53\u001b[0m     result_value \u001b[38;5;241m=\u001b[39m result_value\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;28mround\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Determine the result\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResidual  : = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m✅\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mresult\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m❌\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m/media/wassname/SGIronWolf/projects5/elk/repr-preference-optimization/.venv/lib/python3.9/site-packages/sympy/core/relational.py:843\u001b[0m, in \u001b[0;36m_Inequality.__new__\u001b[0;34m(cls, lhs, rhs, **options)\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid comparison of non-real \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m me)\n\u001b[1;32m    842\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m me \u001b[38;5;129;01mis\u001b[39;00m S\u001b[38;5;241m.\u001b[39mNaN:\n\u001b[0;32m--> 843\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid NaN comparison\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    844\u001b[0m \u001b[38;5;66;03m# First we invoke the appropriate inequality method of `lhs`\u001b[39;00m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;66;03m# (e.g., `lhs.__lt__`).  That method will try to reduce to\u001b[39;00m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;66;03m# boolean or raise an exception.  It may keep calling\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;66;03m# exception).  In that case, it must call us with\u001b[39;00m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;66;03m# `evaluate=False` to prevent infinite recursion.\u001b[39;00m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_relation(lhs, rhs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid NaN comparison"
     ]
    }
   ],
   "source": [
    "def kl_divergence(p, q):\n",
    "    return (p * (p / q).log()).sum(dim=-1)\n",
    "\n",
    "def js_divergence(p, q):\n",
    "    m = 0.5 * (p + q)\n",
    "    return 0.5 * kl_divergence(p, m) + 0.5 * kl_divergence(q, m)\n",
    "\n",
    "def mutual_information(C, R):\n",
    "    joint = torch.einsum('bt,bt->bt', C, R)\n",
    "    joint = joint / joint.sum(dim=-1, keepdim=True)\n",
    "    mi = kl_divergence(joint, C * R)\n",
    "    return mi\n",
    "\n",
    "def info_theoretic_correlation(C, R, logratios):\n",
    "    C_norm = F.softmax(C, dim=-1)\n",
    "    R_norm = F.softmax(R, dim=-1)\n",
    "    \n",
    "    kl = kl_divergence(C_norm, R_norm)#.mean(dim=(1,))\n",
    "    js = js_divergence(C_norm, R_norm)#.mean(dim=(1,))\n",
    "    mi = mutual_information(C_norm, R_norm)#.mean(dim=(1,))\n",
    "    \n",
    "    kl_corr = np.abs(np.corrcoef(kl.cpu().numpy(), logratios)[0,1])\n",
    "    js_corr = np.abs(np.corrcoef(js.cpu().numpy(), logratios)[0,1])\n",
    "    mi_corr = np.abs(np.corrcoef(mi.cpu().numpy(), logratios)[0,1])\n",
    "    \n",
    "    return kl_corr, js_corr, mi_corr\n",
    "\n",
    "print(\"C1-R1 with logratios1:\")\n",
    "kl1,js1,mi1 = info_theoretic_correlation(C1, R1, logratios1)\n",
    "kl2,js2,mi2 = info_theoretic_correlation(C1, R1, logratios1.roll(1, 0))\n",
    "shypothesis('kl1>kl2', variables=locals())\n",
    "shypothesis('js1>js2', variables=locals())\n",
    "shypothesis('mi1>mi2', variables=locals())\n",
    "\n",
    "print(\"C2-R2 with logratios2:\")\n",
    "kl1,js1,mi1 = info_theoretic_correlation(C2, R2, logratios2)\n",
    "kl2,js2,mi2 = info_theoretic_correlation(C2, R2, logratios2.roll(1, 0))\n",
    "shypothesis('kl1>kl2', variables=locals())\n",
    "shypothesis('js1>js2', variables=locals())\n",
    "shypothesis('mi1>mi2', variables=locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original difference: 113.40776062011719\n",
      "Rotated difference: 815.3988037109375\n",
      "Improvement: -619.00%\n",
      "Correlation before rotation: 0.49\n",
      "Correlation after rotation: 0.05\n",
      "Improvement: -90.57%\n",
      "\n",
      "Hypothesis: r > c\n",
      "          : 0.047 > 0.494\n",
      "Residual  : = -c + r\n",
      "Residual  : = -0.447 ❌\n",
      "\n",
      "Original difference: 126.20033264160156\n",
      "Rotated difference: 827.69921875\n",
      "Improvement: -555.86%\n",
      "Correlation before rotation: 0.18\n",
      "Correlation after rotation: 0.11\n",
      "Improvement: -39.68%\n",
      "\n",
      "Hypothesis: r > c\n",
      "          : 0.111 > 0.184\n",
      "Residual  : = -c + r\n",
      "Residual  : = -0.073 ❌\n",
      "\n",
      "Original difference: 126.20033264160156\n",
      "Rotated difference: 827.69921875\n",
      "Improvement: -555.86%\n",
      "Correlation before rotation: 0.17\n",
      "Correlation after rotation: 0.26\n",
      "Improvement: 52.03%\n",
      "\n",
      "Hypothesis: r > c\n",
      "          : 0.262 > 0.172\n",
      "Residual  : = -c + r\n",
      "Residual  : = 0.090 ✅\n",
      "\n",
      "Original difference: 113.40776062011719\n",
      "Rotated difference: 815.3988037109375\n",
      "Improvement: -619.00%\n",
      "Correlation before rotation: 0.14\n",
      "Correlation after rotation: 0.09\n",
      "Improvement: -38.20%\n",
      "\n",
      "Hypothesis: r > c\n",
      "          : 0.087 > 0.140\n",
      "Residual  : = -c + r\n",
      "Residual  : = -0.054 ❌\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def rotation_test(C, R, logratios):\n",
    "    def optimal_rotation(A, B):\n",
    "\n",
    "        corr_matrix = torch.mm(A.t(), B)\n",
    "        Q, _ = torch.linalg.qr(corr_matrix)\n",
    "        return Q\n",
    "    \n",
    "    def optimal_rotation(A, B):\n",
    "        corr_matrix = torch.mm(A.t(), B)\n",
    "        U, _, V = torch.linalg.svd(corr_matrix)\n",
    "        return torch.mm(U, V.t())\n",
    "    \n",
    "    R_opt = optimal_rotation(C, R)\n",
    "    R_rotated = torch.matmul(R, R_opt.t())\n",
    "    \n",
    "    a = C - R\n",
    "    b = C - R_rotated\n",
    "\n",
    "    # a -= a.mean(dim=-1, keepdim=True)\n",
    "    # b -= b.mean(dim=-1, keepdim=True)\n",
    "    # a /= torch.norm(a, dim=0, p=2, keepdim=True)\n",
    "    # b /= torch.norm(b, dim=0, p=2, keepdim=True)\n",
    "\n",
    "    original_diff = torch.norm(a, p=2)\n",
    "    rotated_diff = torch.norm(b, p=2)\n",
    "    \n",
    "    print(f\"Original difference: {original_diff}\")\n",
    "    print(f\"Rotated difference: {rotated_diff}\")\n",
    "    print(f\"Improvement: {(original_diff - rotated_diff) / original_diff * 100:.2f}%\")\n",
    "    \n",
    "    c = np.abs(np.corrcoef(torch.norm(a, dim=-1, p=2), logratios)[0, 1])\n",
    "    r = np.abs(np.corrcoef(torch.norm(b, dim=-1, p=2), logratios)[0, 1])\n",
    "    print(f\"Correlation before rotation: {c:.2f}\")\n",
    "    print(f\"Correlation after rotation: {r:.2f}\")\n",
    "    print(f\"Improvement: {(r - c) / c * 100:.2f}%\")\n",
    "    print()\n",
    "    shypothesis('r > c', variables=dict(r=r, c=c))\n",
    "    return R_opt, R_rotated\n",
    "\n",
    "# maybe I should softmax first?\n",
    "R_opt, R_rotated = rotation_test(C1, R1, logratios1)\n",
    "R_opt, R_rotated = rotation_test(C2, R2, logratios2)\n",
    "\n",
    "R_opt, R_rotated = rotation_test(C2, R2, logratios2.roll(1, 0))\n",
    "R_opt, R_rotated = rotation_test(C1, R1, logratios1.roll(1, 0))\n",
    "# R_opt, R_rotated = rotation_test(C2, R2, logratios1)\n",
    "# R_opt, R_rotated = rotation_test(C1, R1, logratios2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA):\n",
    "Apply PCA to (A - B) pairs. If differences are primarily rotational, most variance should be explained by a few components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothesis: e_a1b1 > e_a1b1roll\n",
      "          : 0.155 > 0.145\n",
      "Residual  : = e_a1b1 - e_a1b1roll\n",
      "Residual  : = 0.01 ✅\n",
      "\n",
      "Hypothesis: e_a1b1 > e_a1b1roll\n",
      "          : 0.154 > 0.134\n",
      "Residual  : = e_a1b1 - e_a1b1roll\n",
      "Residual  : = 0.019 ✅\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def pca_exp_var(X, Y):\n",
    "    diffs = (X - Y).numpy()\n",
    "    s = diffs.std()\n",
    "    diffs /= s\n",
    "    # axis 1, norm each neuron across the batch\n",
    "    # diffs /= diffs.std(1, keepdims=True) + 1e-3\n",
    "    pca = PCA(n_components=5)\n",
    "    pca.fit(diffs)\n",
    "    r = pca.explained_variance_ratio_.mean()\n",
    "    return r\n",
    "\n",
    "\n",
    "# \n",
    "e_a1b1 = pca_exp_var(C1, R1)\n",
    "e_a1b1roll = pca_exp_var(C1, R1.roll(1, 0))\n",
    "shypothesis('e_a1b1 > e_a1b1roll', variables=locals())\n",
    "\n",
    "e_a1b1 = pca_exp_var(C2, R2)\n",
    "e_a1b1roll = pca_exp_var(C2, R2.roll(1, 0))\n",
    "shypothesis('e_a1b1 > e_a1b1roll', variables=locals())\n",
    "\n",
    "# e_a1a2 = pca_exp_var(C1, C2)\n",
    "# e_a1b2 = pca_exp_var(C1, R2)\n",
    "# e_b1b2 = pca_exp_var(R1, R2)\n",
    "# e_b1a1 = pca_exp_var(R1, C1)\n",
    "# e_b1a2 = pca_exp_var(R1, C2)\n",
    "\n",
    "# print('PCA explained variance ratio')\n",
    "# print('A1-B1', e_a1b2)\n",
    "# shypothesis('e_a1b1 > e_a1a2', variables=locals())\n",
    "# shypothesis('e_a1b1 > e_a1b2', variables=locals())\n",
    "# shypothesis('e_a1b1 > e_b1b2', variables=locals()) # should be inconclusive\n",
    "# e_a1b2, e_a1a2, e_a1b2, e_b1b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mutual_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do the paired hs have information shared with the logratios?\n",
      "Mutual Information: 0.03457561427106537\n",
      "Mutual Information: 0.08498625351963374\n",
      "Hypothesis: mi1 > mi2\n",
      "          : 0.035 > 0.085\n",
      "Residual  : = mi1 - mi2\n",
      "Residual  : = -0.050 ❌\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "def mutual_information(C, R, logratios):\n",
    "    distances = torch.norm(C - R, dim=-1, p=2, keepdim=True)\n",
    "    mi = mutual_info_regression(distances, logratios)[0]\n",
    "    print(f\"Mutual Information: {mi}\")\n",
    "    return mi\n",
    "\n",
    "print('do the paired hs have information shared with the logratios?')\n",
    "mi1 = mutual_information(C1, R1, logratios1)\n",
    "mi2 = mutual_information(C1, R1, logratios1.roll(1, 0));\n",
    "shypothesis('mi1 > mi2', variables=locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frobenius Norm Comparison:\n",
    "Compare ||A - B||_F with ||A - RB||_F where R is the optimal rotation. The latter should be significantly smaller if rotations capture most differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passing means the paired samples can be rotated\n",
      "Hypothesis: rot_diff_norm < diff_norm\n",
      "          : 68.860 < 113.408\n",
      "Residual  : = -diff_norm + rot_diff_norm\n",
      "Residual  : = -44.548 ✅\n",
      "\n",
      "passing means the paired samples can be rotated\n",
      "Hypothesis: rot_diff_norm < diff_norm\n",
      "          : 76.573 < 126.200\n",
      "Residual  : = -diff_norm + rot_diff_norm\n",
      "Residual  : = -49.627 ✅\n",
      "\n",
      "passing means the unrelated samples can be rotated\n",
      "Hypothesis: rot_diff_norm > diff_norm\n",
      "          : 149.368 > 281.129\n",
      "Residual  : = -diff_norm + rot_diff_norm\n",
      "Residual  : = -131.761 ❌\n",
      "\n",
      "passing means the unrelated samples can be rotated\n",
      "Hypothesis: rot_diff_norm > diff_norm\n",
      "          : 150.004 > 273.379\n",
      "Residual  : = -diff_norm + rot_diff_norm\n",
      "Residual  : = -123.375 ❌\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def norm_comparison(A, B):\n",
    "    diff_norm = torch.norm(A - B)\n",
    "    corr = torch.mm(A, B.t())\n",
    "    U, _, V = torch.svd(corr)\n",
    "    R = torch.mm(U, V.t())\n",
    "\n",
    "    B_rot = torch.mm(R, B)\n",
    "\n",
    "    rot_diff_norm = torch.norm(A - B_rot)\n",
    "    return diff_norm, rot_diff_norm\n",
    "\n",
    "print('passing means the paired samples can be rotated')\n",
    "diff_norm, rot_diff_norm= norm_comparison(C1, R1)\n",
    "shypothesis('rot_diff_norm < diff_norm', variables=locals())\n",
    "\n",
    "print('passing means the paired samples can be rotated')\n",
    "diff_norm, rot_diff_norm= norm_comparison(C2, R2)\n",
    "# print(diff_norm, rot_diff_norm)\n",
    "shypothesis('rot_diff_norm < diff_norm', variables=locals())\n",
    "\n",
    "print('passing means the unrelated samples can be rotated')\n",
    "diff_norm, rot_diff_norm= norm_comparison(C1, R2)\n",
    "shypothesis('rot_diff_norm > diff_norm', variables=locals())\n",
    "\n",
    "print('passing means the unrelated samples can be rotated')\n",
    "diff_norm, rot_diff_norm= norm_comparison(C2, R1)\n",
    "# print(diff_norm, rot_diff_norm)\n",
    "shypothesis('rot_diff_norm > diff_norm', variables=locals())\n",
    "\n",
    "# UNEEXPECTED result: C and R are already in a similar direction! While unrelated ones are not\n",
    "# this implies that the preference concept that we want is not a factor of directions of the hidden states, but rather the magnitude of the difference between the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are the magnitudes related?\n",
      "Hypothesis: (corr) > (corr_null)\n",
      "          : corr > corr_null\n",
      "     Where: {'corr': 0.36625025494666746, 'corr_null': -0.08703309300812705}\n",
      "          : 0.366 > -0.087\n",
      "Residual  : = corr - corr_null\n",
      "Residual  : = 0.453 ✅\n",
      "\n",
      "Hypothesis: (corr) > (corr_null)\n",
      "          : corr > corr_null\n",
      "     Where: {'corr': 0.026250059748043467, 'corr_null': -0.052039881477377835}\n",
      "          : 0.026 > -0.052\n",
      "Residual  : = corr - corr_null\n",
      "Residual  : = 0.078 ✅\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('are the magnitudes related?')\n",
    "def corr_mag(C, R, logratios):\n",
    "    c_norm = torch.norm(C, dim=-1)\n",
    "    r_norm = torch.norm(R, dim=-1)\n",
    "    norm_diffs = r_norm - c_norm\n",
    "    corr = (np.corrcoef(norm_diffs.flatten(), logratios)[0, 1])\n",
    "    corr_null = (np.corrcoef(norm_diffs.flatten(), logratios.roll(1, 0))[0, 1])\n",
    "    shypothesis('(corr) > (corr_null)', variables=locals(), verbose=True)\n",
    "\n",
    "\n",
    "corr_mag(C1, R1, logratios1)\n",
    "corr_mag(C2, R2, logratios2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(281.1289), tensor(149.3681))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_norm, rot_diff_norm= norm_comparison(C1, R2)\n",
    "diff_norm, rot_diff_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Angle Histogram:\n",
    "Plot histogram of angles between corresponding columns of A and B. Should be concentrated if differences are mainly rotational."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def angle_histogram(A, B):\n",
    "    cos_sims = torch.sum(A * B, dim=0) / (torch.norm(A, dim=0) * torch.norm(B, dim=0))\n",
    "    angles = torch.acos(cos_sims) * 180 / np.pi\n",
    "    plt.hist(angles.numpy())\n",
    "    plt.show()\n",
    "\n",
    "angle_histogram(C1, R1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subspace scaling ratios: [1.01939375 1.08365488 0.98864894 1.01564459 0.95429491 1.02464319\n",
      " 1.02581661 0.97636767 1.02750389 0.97995817]\n",
      "Subspace scaling ratios: [0.8471911  1.12412563 0.88487471 0.96218172 0.58442386 0.67537654\n",
      " 0.52081231 0.73411265 0.97190868 0.75353601]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1703443658520913"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def subspace_scaling(C, R, n_subspaces=10):\n",
    "    pca = PCA(n_components=n_subspaces)\n",
    "    pca.fit(np.vstack((C.reshape(-1, C.shape[-1]), R.reshape(-1, R.shape[-1]))))\n",
    "    c_proj = pca.transform(C.reshape(-1, C.shape[-1]))\n",
    "    r_proj = pca.transform(R.reshape(-1, R.shape[-1]))\n",
    "    scale_ratios = np.linalg.norm(c_proj, axis=0) / np.linalg.norm(r_proj, axis=0)\n",
    "    return scale_ratios\n",
    "\n",
    "scales = subspace_scaling(C1, R1)\n",
    "print(\"Subspace scaling ratios:\", scales)\n",
    "\n",
    "scales = subspace_scaling(C1, R2)\n",
    "print(\"Subspace scaling ratios:\", scales)\n",
    "\n",
    "# ??\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation of sparsity differences with logratios: -0.1256368900620531\n",
      "Correlation of sparsity differences with logratios: 0.14390908886819442\n",
      "unrelated, should be null results\n",
      "Correlation of sparsity differences with logratios: 0.11696235710909991\n",
      "Correlation of sparsity differences with logratios: -0.32468581965186244\n",
      "Hypothesis: c111 > c112\n",
      "          : -0.126 > 0.117\n",
      "Residual  : = c111 - c112\n",
      "Residual  : = -0.243 ❌\n",
      "\n",
      "Hypothesis: c222 > c221\n",
      "          : 0.144 > -0.325\n",
      "Residual  : = -c221 + c222\n",
      "Residual  : = 0.469 ✅\n",
      "\n",
      "Hypothesis: c111 > c112\n",
      "          : -0.126 > 0.117\n",
      "Residual  : = c111 - c112\n",
      "Residual  : = -0.243 ❌\n",
      "\n",
      "Hypothesis: c222 > c221\n",
      "          : 0.144 > -0.325\n",
      "Residual  : = -c221 + c222\n",
      "Residual  : = 0.469 ✅\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sparsity_correlation(C, R, logratios):\n",
    "    c_sparsity = (np.abs(C) > 1e-2).float().mean(dim=-1)\n",
    "    r_sparsity = (np.abs(R) > 1e-2).float().mean(dim=-1)\n",
    "    sparsity_diff = r_sparsity - c_sparsity\n",
    "    corr = np.corrcoef(sparsity_diff.flatten(), logratios)[0, 1]\n",
    "    print(f\"Correlation of sparsity differences with logratios: {corr}\")\n",
    "    return (corr)\n",
    "\n",
    "c111 = sparsity_correlation(C1, R1, logratios1)\n",
    "c222 = sparsity_correlation(C2, R2, logratios2)\n",
    "\n",
    "print('unrelated, should be null results')\n",
    "c112 = sparsity_correlation(C1, R1, logratios1.roll(1, 0))\n",
    "c221 = sparsity_correlation(C2, R2, logratios2.roll(1, 0))\n",
    "\n",
    "shypothesis('c111 > c112', variables=locals())\n",
    "shypothesis('c222 > c221', variables=locals())\n",
    "shypothesis('c111 > c112', variables=locals())\n",
    "shypothesis('c222 > c221', variables=locals())\n",
    "\n",
    "# !!! SUCCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothesis: c1 > c1n\n",
      "          : c1 > c1n\n",
      "     Where: {'c1': 0.49350771589986103, 'c1n': 0.140393881508222}\n",
      "          : 0.494 > 0.140\n",
      "Residual  : = c1 - c1n\n",
      "Residual  : = 0.353 ✅\n",
      "\n",
      "Hypothesis: c2 > c2n\n",
      "          : 0.184 > 0.172\n",
      "Residual  : = c2 - c2n\n",
      "Residual  : = 0.011 ✅\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.fft import fft\n",
    "\n",
    "def fft_analysis(C, R):\n",
    "    C_fft = fft(C.cpu().numpy(), axis=-1)\n",
    "    R_fft = fft(R.cpu().numpy(), axis=-1)\n",
    "    return C_fft - R_fft\n",
    "\n",
    "def fft_corr(C, R, logratios):\n",
    "    fft_diff = fft_analysis(C, R)\n",
    "    fft_norm = np.linalg.norm(fft_diff, axis=-1)\n",
    "    fft_corr = np.abs(np.corrcoef(fft_norm, logratios)[0,1])\n",
    "    return fft_corr\n",
    "\n",
    "c1 = fft_corr(C1, R1, logratios1)\n",
    "c1n = fft_corr(C1, R1, logratios1.roll(1, 0))\n",
    "shypothesis('c1 > c1n', variables=locals(), verbose=True)\n",
    "\n",
    "c2 = fft_corr(C2, R2, logratios2)\n",
    "c2n = fft_corr(C2, R2, logratios2.roll(1, 0))\n",
    "shypothesis('c2 > c2n', variables=locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are the right pairs more correlate if we take the diff of the symlog?\n",
      "Hypothesis: abs(nl_corr) > abs(corr_null)\n",
      "          : Abs(nl_corr) > Abs(corr_null)\n",
      "     Where: {'nl_corr': 0.010040431957342038, 'corr_null': -0.23988651284825843}\n",
      "          : 0.010 > 0.240\n",
      "Residual  : = -Abs(corr_null) + Abs(nl_corr)\n",
      "Residual  : = -0.230 ❌\n",
      "\n",
      "Hypothesis: abs(nl_corr) > abs(corr_null)\n",
      "          : Abs(nl_corr) > Abs(corr_null)\n",
      "     Where: {'nl_corr': -0.13913583520892275, 'corr_null': 0.16191249964779827}\n",
      "          : 0.139 > 0.162\n",
      "Residual  : = -Abs(corr_null) + Abs(nl_corr)\n",
      "Residual  : = -0.023 ❌\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def non_lin_corr(C, R, logratios):\n",
    "    C = symlog(C)\n",
    "    R = symlog(R)\n",
    "    nl_diff = C - R\n",
    "    nl_norm = torch.norm(nl_diff, dim=-1)\n",
    "    nl_corr = np.corrcoef(nl_norm, logratios)[0,1]\n",
    "    corr_null = np.corrcoef(nl_norm, logratios.roll(1,0))[0,1]\n",
    "    shypothesis('abs(nl_corr) > abs(corr_null)', variables=locals(), verbose=True)\n",
    "\n",
    "print('are the right pairs more correlate if we take the diff of the symlog?')\n",
    "non_lin_corr(C1, R1, logratios1)\n",
    "non_lin_corr(C2, R2, logratios2)\n",
    "# but it's the same result without symlog..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothesis: rel_corr > rel_corr2\n",
      "          : 0.335 > 0.046\n",
      "Residual  : = rel_corr - rel_corr2\n",
      "Residual  : = 0.289 ✅\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def relative_positioning(C, R, ref_point):\n",
    "    C_rel = C - ref_point\n",
    "    R_rel = R - ref_point\n",
    "    return torch.norm(C_rel, dim=-1) - torch.norm(R_rel, dim=-1)\n",
    "\n",
    "# Using mean of C2 as reference point\n",
    "ref_point = C2.mean(dim=(0), keepdim=True)\n",
    "rel_diff = relative_positioning(C1, R1, ref_point)\n",
    "rel_corr = np.abs(np.corrcoef(rel_diff.flatten(), logratios1)[0,1])\n",
    "rel_corr2 = np.abs(np.corrcoef(rel_diff.flatten(), logratios1.roll(1, 0))[0,1])\n",
    "shypothesis('rel_corr > rel_corr2', variables=locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothesis: c1r1_sim_corr > c1r1null_sim_corr\n",
      "          : -0.484 > -0.077\n",
      "Residual  : = c1r1_sim_corr - c1r1null_sim_corr\n",
      "Residual  : = -0.407 ❌\n",
      "\n",
      "Hypothesis: c2r2_sim_corr > c2r2null_sim_corr\n",
      "          : -0.081 > 0.172\n",
      "Residual  : = c2r2_sim_corr - c2r2null_sim_corr\n",
      "Residual  : = -0.253 ❌\n",
      "\n",
      "C1-R1 similarity correlation with logratio: -0.48415128775540195\n",
      "C2-R2 similarity correlation with logratio: -0.0810418904144647\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return F.cosine_similarity(a, b, dim=-1)\n",
    "\n",
    "c1r1_sim = cosine_similarity(C1, R1)\n",
    "c2r2_sim = cosine_similarity(C2, R2)\n",
    "\n",
    "c1r1_sim_corr = np.corrcoef(c1r1_sim.flatten(), logratios1)[0,1]\n",
    "c2r2_sim_corr = np.corrcoef(c2r2_sim.flatten(), logratios2)[0,1]\n",
    "c1r1null_sim_corr = np.corrcoef(c1r1_sim.flatten(), logratios1.roll(1, 0))[0,1]\n",
    "c2r2null_sim_corr = np.corrcoef(c2r2_sim.flatten(), logratios2.roll(1, 0))[0,1]\n",
    "shypothesis('c1r1_sim_corr > c1r1null_sim_corr', variables=locals())\n",
    "shypothesis('c2r2_sim_corr > c2r2null_sim_corr', variables=locals())\n",
    "\n",
    "print(f\"C1-R1 similarity correlation with logratio: {c1r1_sim_corr}\")\n",
    "print(f\"C2-R2 similarity correlation with logratio: {c2r2_sim_corr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if we project hs onto the global mean then correlate...\n",
      "Hypothesis: c1 > c3n\n",
      "          : 0.404 > 0.041\n",
      "Residual  : = c1 - c3n\n",
      "Residual  : = 0.363 ✅\n",
      "\n",
      "Hypothesis: c2 > c4n\n",
      "          : 0.045 > 0.060\n",
      "Residual  : = c2 - c4n\n",
      "Residual  : = -0.015 ❌\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def global_position(x, global_mean):\n",
    "    return F.cosine_similarity(x - global_mean, global_mean, dim=-1)\n",
    "\n",
    "global_mean = torch.cat([C1, R1, C2, R2], dim=0).mean(dim=(0), keepdim=True)\n",
    "\n",
    "def glob_pos_corr(C, R, logratios):\n",
    "    c_global = global_position(C, global_mean)\n",
    "    r_global = global_position(R, global_mean)\n",
    "    global_diff = c_global - r_global\n",
    "    global_corr = np.corrcoef(global_diff.flatten(), logratios)[0,1]\n",
    "    return np.abs(global_corr)\n",
    "\n",
    "print('if we project hs onto the global mean then correlate...')\n",
    "c1 = glob_pos_corr(C1, R1, logratios1)\n",
    "c2 = glob_pos_corr(C2, R2, logratios2)\n",
    "c3n = glob_pos_corr(C1, R1, logratios1.roll(1, 0))\n",
    "c4n = glob_pos_corr(C2, R2, logratios2.roll(1, 0))\n",
    "shypothesis('c1 > c3n', variables=locals())\n",
    "shypothesis('c2 > c4n', variables=locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothesis: c1 > c3n\n",
      "          : 0.403 > 0.042\n",
      "Residual  : = c1 - c3n\n",
      "Residual  : = 0.361 ✅\n",
      "\n",
      "Hypothesis: c2 > c4n\n",
      "          : 0.047 > 0.059\n",
      "Residual  : = c2 - c4n\n",
      "Residual  : = -0.013 ❌\n",
      "\n",
      "Relative position correlation: 0.40318000434572915\n",
      "Relative position correlation (null): 0.0466646318645862\n"
     ]
    }
   ],
   "source": [
    "def relative_position(a, b, ref):\n",
    "    a_rel = F.cosine_similarity(a - ref, ref, dim=-1)\n",
    "    b_rel = F.cosine_similarity(b - ref, ref, dim=-1)\n",
    "    return a_rel - b_rel\n",
    "\n",
    "# Use C2 mean as reference\n",
    "def rel_pos_corr(C1, R1, C2, logratios1):\n",
    "    ref = C2.mean(dim=(0), keepdim=True)\n",
    "    rel_pos1 = relative_position(C1, R1, ref)\n",
    "    rel_pos_corr = np.abs(np.corrcoef(rel_pos1.flatten(), logratios1)[0,1])\n",
    "    return rel_pos_corr\n",
    "\n",
    "c1 = rel_pos_corr(C1, R1, C2, logratios1)\n",
    "c2 = rel_pos_corr(C2, R2, C1, logratios2)\n",
    "c3n = rel_pos_corr(C1, R1, C2, logratios1.roll(1, 0))\n",
    "c4n = rel_pos_corr(C2, R2, C1, logratios2.roll(1, 0))\n",
    "shypothesis('c1 > c3n', variables=locals())\n",
    "shypothesis('c2 > c4n', variables=locals())\n",
    "print(f\"Relative position correlation: {c1}\")\n",
    "print(f\"Relative position correlation (null): {c2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preference direction projection correlation: 0.19126374608197338\n",
      "Preference direction projection correlation: 0.2516364261156839\n",
      "Preference direction projection correlation: 0.12971413618465952\n",
      "Preference direction projection correlation: 0.16747552150877282\n",
      "Hypothesis: proj_corr1 > proj_corr4\n",
      "          : 0.191 > 0.167\n",
      "Residual  : = proj_corr1 - proj_corr4\n",
      "Residual  : = 0.024 ✅\n",
      "\n",
      "Hypothesis: proj_corr2 > proj_corr3\n",
      "          : 0.252 > 0.130\n",
      "Residual  : = proj_corr2 - proj_corr3\n",
      "Residual  : = 0.122 ✅\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def projection_correlation(C, R, logratios):\n",
    "    # Compute the mean difference vector\n",
    "    mean_diff = (C - R).mean(dim=0)\n",
    "\n",
    "    # Project samples onto this direction\n",
    "    c_proj = F.cosine_similarity(C, mean_diff.unsqueeze(0), dim=-1)\n",
    "    r_proj = F.cosine_similarity(R, mean_diff.unsqueeze(0), dim=-1)\n",
    "    proj_diff = c_proj - r_proj\n",
    "\n",
    "    proj_corr = np.abs(np.corrcoef(proj_diff, logratios)[0,1])\n",
    "    return proj_corr\n",
    "\n",
    "proj_corr1 = projection_correlation(C1, R1, logratios1)\n",
    "print(f\"Preference direction projection correlation: {proj_corr1}\")\n",
    "\n",
    "proj_corr2 = projection_correlation(C2, R2, logratios2)\n",
    "print(f\"Preference direction projection correlation: {proj_corr2}\")\n",
    "\n",
    "proj_corr3 = projection_correlation(C2, R2, logratios2.roll(1, 0))\n",
    "print(f\"Preference direction projection correlation: {proj_corr3}\")\n",
    "\n",
    "\n",
    "proj_corr4 = projection_correlation(C1, R1, logratios1.roll(1, 0))\n",
    "print(f\"Preference direction projection correlation: {proj_corr4}\")\n",
    "\n",
    "shypothesis('proj_corr1 > proj_corr4', variables=locals())\n",
    "shypothesis('proj_corr2 > proj_corr3', variables=locals())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance difference correlation: -0.3475864039258729\n",
      "Variance difference correlation: 0.004136700778330699\n",
      "Variance difference correlation: -0.024859680079092884\n",
      "Variance difference correlation: 0.07171431090834102\n",
      "Hypothesis: abs(vc0) > abs(vc3)\n",
      "          : Abs(vc0) > Abs(vc3)\n",
      "     Where: {'vc0': -0.3475864039258729, 'vc3': 0.07171431090834102}\n",
      "          : 0.348 > 0.072\n",
      "Residual  : = Abs(vc0) - Abs(vc3)\n",
      "Residual  : = 0.276 ✅\n",
      "\n",
      "Hypothesis: abs(vc1) > abs(vc2)\n",
      "          : Abs(vc1) > Abs(vc2)\n",
      "     Where: {'vc2': -0.024859680079092884, 'vc1': 0.004136700778330699}\n",
      "          : 0.004 > 0.025\n",
      "Residual  : = Abs(vc1) - Abs(vc2)\n",
      "Residual  : = -0.021 ❌\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def var_corr(C, R, logratios):\n",
    "    c_var = C.var(dim=-1)\n",
    "    r_var = R.var(dim=-1)\n",
    "    var_diff = c_var - r_var\n",
    "\n",
    "    var_corr = np.corrcoef(var_diff.flatten(), logratios)[0,1]\n",
    "    return var_corr\n",
    "\n",
    "\n",
    "vc0 = var_corr(C1, R1, logratios1)\n",
    "print(f\"Variance difference correlation: {vc0}\")\n",
    "\n",
    "vc1 = var_corr(C2, R2, logratios2)\n",
    "print(f\"Variance difference correlation: {vc1}\")\n",
    "\n",
    "vc2 = var_corr(C2, R2, logratios2.roll(1, 0))\n",
    "print(f\"Variance difference correlation: {vc2}\")\n",
    "\n",
    "vc3 = var_corr(C1, R1, logratios1.roll(1, 0))\n",
    "print(f\"Variance difference correlation: {vc3}\")\n",
    "shypothesis('abs(vc0) > abs(vc3)', variables=locals(), verbose=1)\n",
    "shypothesis('abs(vc1) > abs(vc2)', variables=locals(), verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
