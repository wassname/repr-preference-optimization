{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with hidden states\n",
    "\n",
    "Question, is there a better representation of concepts in hidden states?\n",
    "\n",
    "Setup: we use DPO setup, with a chosen and rejected string. We then generate a set of hidden states, and compare the hidden states of the chosen and rejected string.\n",
    "\n",
    "Goal: better generalisation of desired behavuour by changing the internal representation of policy rather than directly changing the policy\n",
    "\n",
    "  - Hypothesis: rejected and chosen hidden states will - on mean - be best representated as rotations from each other\n",
    "  - alternate: either mean mass diff (linear) or no repr will be better\n",
    "  - metric: manual generation getting output while maintaining coherency, prediction other sets of hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import DPOTrainer\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Union\n",
    "from einops import rearrange\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from reprpo.helpers.adapters import set_adapter\n",
    "from einops import rearrange\n",
    "from matplotlib import pyplot as plt\n",
    "from reprpo import silence\n",
    "from reprpo.gen import generation_test\n",
    "\n",
    "from reprpo.trainer import mean_with_attention, symlog, mult_with_attention\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from einops import reduce\n",
    "from reprpo.trainer import collect_hs, ReprPOConfig, ReprPOTrainer, normalize_output, normalize_per\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "535362c19eaf41a9bedb417f18c6f27f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 41943040 || all params: 4582543360 || trainable%: 0.9152786281546499\n"
     ]
    }
   ],
   "source": [
    "from reprpo.models.load import load_model, print_trainable_parameters\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "model_name = \"NousResearch/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "## Big adapter\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16, \n",
    "    r=16,\n",
    "    lora_dropout=0.0,\n",
    "    use_rslora=False,\n",
    "    # use_dora=True,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    ")\n",
    "\n",
    "\n",
    "model, tokenizer = load_model(model_name, )\n",
    "# from trl.trainer.utils import peft_module_casting_to_bf16\n",
    "# peft_module_casting_to_bf16(model)\n",
    "adapter_name='ReprPO2'\n",
    "model = prepare_model_for_kbit_training(model, {'use_gradient_checkpointing': True})\n",
    "model = get_peft_model(model, peft_config, adapter_name=adapter_name)\n",
    "print_trainable_parameters(model)\n",
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['base_model.model.model.embed_tokens.weight', 'base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.0.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.0.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.0.input_layernorm.weight', 'base_model.model.model.layers.0.post_attention_layernorm.weight', 'base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.1.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.1.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.1.input_layernorm.weight', 'base_model.model.model.layers.1.post_attention_layernorm.weight', 'base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.2.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.2.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.2.input_layernorm.weight', 'base_model.model.model.layers.2.post_attention_layernorm.weight', 'base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.3.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.3.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.3.input_layernorm.weight', 'base_model.model.model.layers.3.post_attention_layernorm.weight', 'base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.4.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.4.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.4.input_layernorm.weight', 'base_model.model.model.layers.4.post_attention_layernorm.weight', 'base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.5.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.5.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.5.input_layernorm.weight', 'base_model.model.model.layers.5.post_attention_layernorm.weight', 'base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.6.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.6.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.6.input_layernorm.weight', 'base_model.model.model.layers.6.post_attention_layernorm.weight', 'base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.7.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.7.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.7.input_layernorm.weight', 'base_model.model.model.layers.7.post_attention_layernorm.weight', 'base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.8.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.8.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.8.input_layernorm.weight', 'base_model.model.model.layers.8.post_attention_layernorm.weight', 'base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.9.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.9.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.9.input_layernorm.weight', 'base_model.model.model.layers.9.post_attention_layernorm.weight', 'base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.10.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.10.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.10.input_layernorm.weight', 'base_model.model.model.layers.10.post_attention_layernorm.weight', 'base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.11.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.11.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.11.input_layernorm.weight', 'base_model.model.model.layers.11.post_attention_layernorm.weight', 'base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.12.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.12.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.12.input_layernorm.weight', 'base_model.model.model.layers.12.post_attention_layernorm.weight', 'base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.13.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.13.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.13.input_layernorm.weight', 'base_model.model.model.layers.13.post_attention_layernorm.weight', 'base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.14.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.14.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.14.input_layernorm.weight', 'base_model.model.model.layers.14.post_attention_layernorm.weight', 'base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.15.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.15.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.15.input_layernorm.weight', 'base_model.model.model.layers.15.post_attention_layernorm.weight', 'base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.16.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.16.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.16.input_layernorm.weight', 'base_model.model.model.layers.16.post_attention_layernorm.weight', 'base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.17.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.17.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.17.input_layernorm.weight', 'base_model.model.model.layers.17.post_attention_layernorm.weight', 'base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.18.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.18.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.18.input_layernorm.weight', 'base_model.model.model.layers.18.post_attention_layernorm.weight', 'base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.19.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.19.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.19.input_layernorm.weight', 'base_model.model.model.layers.19.post_attention_layernorm.weight', 'base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.20.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.20.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.20.input_layernorm.weight', 'base_model.model.model.layers.20.post_attention_layernorm.weight', 'base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.21.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.21.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.21.input_layernorm.weight', 'base_model.model.model.layers.21.post_attention_layernorm.weight', 'base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.22.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.22.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.22.input_layernorm.weight', 'base_model.model.model.layers.22.post_attention_layernorm.weight', 'base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.23.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.23.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.23.input_layernorm.weight', 'base_model.model.model.layers.23.post_attention_layernorm.weight', 'base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.24.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.24.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.24.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.24.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.24.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.24.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.24.input_layernorm.weight', 'base_model.model.model.layers.24.post_attention_layernorm.weight', 'base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.25.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.25.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.25.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.25.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.25.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.25.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.25.input_layernorm.weight', 'base_model.model.model.layers.25.post_attention_layernorm.weight', 'base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.26.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.26.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.26.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.26.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.26.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.26.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.26.input_layernorm.weight', 'base_model.model.model.layers.26.post_attention_layernorm.weight', 'base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.27.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.27.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.27.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.27.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.27.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.27.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.27.input_layernorm.weight', 'base_model.model.model.layers.27.post_attention_layernorm.weight', 'base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.28.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.28.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.28.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.28.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.28.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.28.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.28.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.28.input_layernorm.weight', 'base_model.model.model.layers.28.post_attention_layernorm.weight', 'base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.29.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.29.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.29.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.29.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.29.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.29.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.29.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.29.input_layernorm.weight', 'base_model.model.model.layers.29.post_attention_layernorm.weight', 'base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.30.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.30.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.30.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.30.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.30.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.30.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.30.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.30.input_layernorm.weight', 'base_model.model.model.layers.30.post_attention_layernorm.weight', 'base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.31.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.31.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.31.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.31.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.31.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.31.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.31.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.31.input_layernorm.weight', 'base_model.model.model.layers.31.post_attention_layernorm.weight', 'base_model.model.model.norm.weight', 'base_model.model.lm_head.weight'], unexpected_keys=[])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpo_adapter_f = './output-dir/dpo/DPO'\n",
    "model.load_adapter(dpo_adapter_f, 'DPO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # QC model and adapter is coherent\n",
    "# generation_test(model, tokenizer, max_new_tokens=24, system='no yapping', adapter_names=[None, 'DPO'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DPO dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected'],\n",
       "        num_rows: 160\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected'],\n",
       "        num_rows: 160\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sample(dataset, N):\n",
    "    return (dataset\n",
    "            .shuffle(42)\n",
    "            .select(range(\n",
    "            min(len(dataset),\n",
    "                N)))\n",
    "    )\n",
    "\n",
    "dataset = load_dataset('Atsunori/HelpSteer2-DPO')\n",
    "dataset['train'] = sample(dataset['train'], num_samples)\n",
    "dataset['validation'] = sample(dataset['validation'], num_samples)\n",
    "dataset2 = dataset.rename_column('chosen_response', 'chosen').rename_column('rejected_response', 'rejected')\n",
    "dataset2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypothesis: ( a - b )\t> 0 ?\n",
      "working:    ( 1e-09 - -0.02 ) = 0.02\t> 0\t∴ ✅\n",
      "\n",
      "hypothesis: ( b - a )\t> 0 ?\n",
      "working:    ( -0.02 - 1e-09 ) = -0.02\t> 0\t∴ ❌\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_fstr(s):\n",
    "    # then remove the f'\n",
    "    s = re.sub(r\"f'\", \"'\", s)\n",
    "    return s.replace('}', '').replace('{', '')\n",
    "\n",
    "def fhypothesis(equation: str, fmt='.3g'):\n",
    "    \"\"\"\n",
    "    Evaluate a mathematical hypothesis with variables automatically fetched from the global scope.\n",
    "\n",
    "    Args:\n",
    "    - equation: A mathematical equation with variables in curly brackets.\n",
    "    - fmt: Format specifier for the result.\n",
    "    Returns:\n",
    "    - None: Prints the hypothesis and its evaluation.\n",
    "        \n",
    "    Usage:    \n",
    "      a = 0.001\n",
    "      b = -0.02\n",
    "      fhypothesis('{a}-{b}')\n",
    "      fhypothesis('{b}-{a}')\n",
    "      \n",
    "      > hypothesis:\t ( a - b )\t> 0 ?\n",
    "      > working:\t ( 0.00 - -0.02 ) = 0.02\t> 0\t∴ ✅\n",
    "      > hypothesis:\t ( b - a )\t> 0 ?\n",
    "      > working:\t ( -0.02 - 0.00 ) = -0.02\t> 0\t∴ ❌\n",
    "  \n",
    "    \"\"\"\n",
    "    eq_wfmt = equation.replace('}', ':'+fmt+'}')\n",
    "    eq_pln = remove_fstr(equation)\n",
    "    res = eval(eq_pln)\n",
    "    res_fmt = f\"{res:{fmt}}\"\n",
    "    sym = '✅' if bool(res>0) else '❌'\n",
    "    wrk_fmt = \"f'( \"+eq_wfmt+f\" ) = {res_fmt}\\t> 0\\t∴ {sym}'\"\n",
    "    print(f'hypothesis: ( {eq_pln} )\\t> 0 ?')\n",
    "    print('working:   ', eval(wrk_fmt))\n",
    "    print()\n",
    "\n",
    "a = 1e-9\n",
    "b = -0.02\n",
    "fhypothesis('{a} - {b}')\n",
    "fhypothesis('{b} - {a}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect HS in DPO way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reprpo_trainer.is_encoder_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/repr-preference-optimization/.venv/lib/python3.9/site-packages/trl/trainer/dpo_trainer.py:442: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = ReprPOConfig('./output-dir/scratch',\n",
    "    per_device_train_batch_size=3,\n",
    "    per_device_eval_batch_size=3,\n",
    "    gradient_checkpointing=True,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    max_prompt_length=128,\n",
    "    max_length=256,\n",
    "    collection_layers=[10, 20]\n",
    "                             )\n",
    "reprpo_trainer = ReprPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=None,\n",
    "    args=training_args,\n",
    "    beta=training_args.beta,\n",
    "    train_dataset=dataset2[\"train\"],\n",
    "    # eval_dataset=dataset2[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 256])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# QC get dpo catch\n",
    "dl = reprpo_trainer.get_train_dataloader()\n",
    "batch = next(iter(dl))\n",
    "batch['chosen_input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['concatenated_input_ids', 'concatenated_attention_mask', 'concatenated_labels'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 256]), torch.Size([12, 256]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# QC view a typical input to the model (since the dpo trainer transformes in dataset, concatenating chosen and rejecting along the batch dimension)\n",
    "batch_concat = reprpo_trainer.concatenated_inputs(\n",
    "            batch,\n",
    "            is_encoder_decoder=reprpo_trainer.is_encoder_decoder,\n",
    "            label_pad_token_id=reprpo_trainer.label_pad_token_id,\n",
    "            padding_value=reprpo_trainer.padding_value,\n",
    "            device=reprpo_trainer.accelerator.device,\n",
    "            max_length=reprpo_trainer.args.max_length,\n",
    "        )\n",
    "layer_idx = 0\n",
    "print(batch_concat.keys())\n",
    "batch['chosen_input_ids'].shape, batch_concat['concatenated_input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get batch of hidden states\n",
    "from reprpo.helpers.torch import clear_mem\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_hs(trainer, model, batch):\n",
    "    model.eval()\n",
    "    (\n",
    "        chosen_logps,\n",
    "        rejected_logps,\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "        chosen_hs,\n",
    "        rejected_hs,\n",
    "        chosen_attn_mask,\n",
    "        rejected_attn_mask\n",
    "    ) = trainer.concatenated_forward(trainer.model, batch)\n",
    "    r = dict(chosen_hs=chosen_hs, rejected_hs=rejected_hs, chosen_logps=chosen_logps, rejected_logps=rejected_logps, chosen_attn_mask=chosen_attn_mask, rejected_attn_mask=rejected_attn_mask)\n",
    "\n",
    "    r['chosen_hs_unproj'] = model.lm_head(chosen_hs)\n",
    "    print(r['chosen_hs_unproj'].shape, chosen_hs.shape)\n",
    "    r['rejected_hs_unproj'] = model.lm_head(rejected_hs)\n",
    "\n",
    "    r = {k: v.detach().cpu() for k, v in r.items()}\n",
    "    clear_mem(trainer)\n",
    "    \n",
    "    return r\n",
    "\n",
    "# turn off adapter\n",
    "\n",
    "# with reprpo_trainer.null_ref_context():\n",
    "#     r = get_hs(reprpo_trainer.model, batch)\n",
    "#     data.append(r)\n",
    "\n",
    "\n",
    "\n",
    "# policy_chosen_hs, policy_rejected_hs, policy_chosen_logps, policy_rejected_logps = get_hs(reprpo_trainer.model, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b53a15e3ce44128bf193959b2c82ea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 2, 256, 128256]) torch.Size([6, 2, 256, 4096])\n",
      "torch.Size([6, 2, 256, 128256]) torch.Size([6, 2, 256, 4096])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "dl = reprpo_trainer.get_train_dataloader()\n",
    "\n",
    "data = []\n",
    "for i, batch in enumerate(tqdm(dl)):\n",
    "    with reprpo_trainer.null_ref_context():\n",
    "        r = get_hs(reprpo_trainer, reprpo_trainer.model, batch)\n",
    "        data.append(r)\n",
    "        if i > 2:\n",
    "            break\n",
    "\n",
    "# concat\n",
    "data = {k: torch.cat([d[k] for d in data], dim=0) for k in data[0].keys()}\n",
    "data['chosen_hs'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "clear_mem(reprpo_trainer)\n",
    "reprpo_trainer = None\n",
    "clear_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mult_with_attention(C1, A2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some data samples\n",
    "layer = 1\n",
    "C = data['chosen_hs'].half()\n",
    "R = data['rejected_hs'].half()\n",
    "\n",
    "C = data['chosen_hs_unproj'].half()\n",
    "R = data['rejected_hs_unproj'].half()\n",
    "CA = data['chosen_attn_mask']\n",
    "RA = data['rejected_attn_mask']\n",
    "\n",
    "M = 4\n",
    "\n",
    "A2 = CA * RA # use both attn masks when comparing?\n",
    "# minl = A2.sum(-1).min()\n",
    "C = mult_with_attention(C, A2)[:M, layer]\n",
    "R = mult_with_attention(R, A2)[:M, layer]\n",
    "\n",
    "# compare two unrelated sets of samples, that way we have ones that should show the difference and ones that shouldn't show the difference we arel ooking for\n",
    "n = len(C)//2\n",
    "print('n', n)\n",
    "C1 = C[:n] # chosen\n",
    "R1 = R[:n] # rejected\n",
    "C2 = C[n:] # unrelated chosen\n",
    "R2 = R[n:] # unrelated rejected\n",
    "\n",
    "logratios = data['chosen_logps'] - data['rejected_logps'] # the logp is the log probability (mean per token) of this response, logratios is the log ratio of probs, this should be correlated with the direction we are seeking in the hidden states\n",
    "logratios1 = logratios[:n]\n",
    "logratios2 = logratios[n:]\n",
    "C1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def unembed_hidden_states(hs, unembed_layer):\n",
    "#     # Assuming unembed_layer is the final layer of your model\n",
    "#     unembedded = unembed_layer(hs.reshape(-1, hs.shape[-1])).reshape(hs.shape[:-1] + (-1,))\n",
    "#     return unembedded\n",
    "\n",
    "# # Use this function before computing correlations\n",
    "# C_unembedded = unembed_hidden_states(C, model.lm_head)\n",
    "# R_unembedded = unembed_hidden_states(R, model.lm_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis\n",
    "\n",
    "Is the amplitude of A-B related to the logp?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_d = C1-R1\n",
    "# hs_d = mult_with_attention(C-R, A2)\n",
    "hs_d = normalize_per(hs_d, -1)\n",
    "# hs_d = norm_except(hs_d, 0)\n",
    "hs_d = hs_d.mean(-1).mean(-1)\n",
    "hs_d, logratios1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = np.abs(np.correlate(hs_d, logratios1)[0])\n",
    "corr_null = np.abs(np.correlate(hs_d, logratios1.flip(0))[0])\n",
    "print('hypothesis corr > corr_null')\n",
    "fhypothesis('{corr} / {corr_null} - 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = np.abs(np.corrcoef(hs_d, logratios1)[0, 1])\n",
    "corr_null = np.abs(np.corrcoef(hs_d, logratios1.flip(0))[0, 1])\n",
    "print('hypothesis corr > corr_null')\n",
    "fhypothesis('{corr} / {corr_null} - 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_test(C, R, logratios, n_permutations=1000):\n",
    "    true_corr = np.corrcoef(torch.norm(C - R, dim=-1).mean(dim=-1).mean(dim=-1).cpu().numpy(), logratios.cpu().numpy())[0, 1]\n",
    "    null_corrs = []\n",
    "    for _ in range(n_permutations):\n",
    "        perm_logratios = logratios[torch.randperm(len(logratios))]\n",
    "        null_corrs.append(np.corrcoef(torch.norm(C - R, dim=-1).mean(dim=-1).mean(dim=-1).cpu().numpy(), perm_logratios.cpu().numpy())[0, 1])\n",
    "    \n",
    "    plt.hist(null_corrs, bins=50, label='null_corrs')\n",
    "    plt.axvline(true_corr, color='r', linestyle='dashed', linewidth=2, label='true_corr')\n",
    "    plt.title('Permutation Test')\n",
    "    plt.xlabel('Correlation')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    p_value = (np.sum(np.abs(null_corrs) >= np.abs(true_corr)) + 1) / (n_permutations + 1)\n",
    "    print(f\"P-value: {p_value}\")\n",
    "\n",
    "permutation_test(C1, R1, logratios1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import CCA\n",
    "# CCA (Canonical Correlation Analysis):\n",
    "# This method finds the directions of maximum correlation between two multidimensional variables.\n",
    "\n",
    "def cca_correlation(C, R, logratios):\n",
    "    hs_d = C - R\n",
    "    hs_d = normalize_per(hs_d, -1)\n",
    "    hs_d = hs_d.reshape(hs_d.shape[0], -1)  # flatten all but batch dim\n",
    "    \n",
    "    cca = CCA(n_components=1)\n",
    "    cca.fit(hs_d, logratios.reshape(-1, 1))\n",
    "    \n",
    "    X_c, Y_c = cca.transform(hs_d, logratios.reshape(-1, 1))\n",
    "    corr = np.corrcoef(X_c.flatten(), Y_c.flatten())[0, 1]\n",
    "    \n",
    "    print(f\"CCA correlation: {corr}\")\n",
    "    return corr, cca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def non_linear_correlation(C, R, logratios):\n",
    "    distances = torch.norm(C - R, dim=-1).mean(dim=-1).mean(dim=-1).cpu().numpy()\n",
    "    logratios = logratios.cpu().numpy()\n",
    "    \n",
    "    spearman_corr, _ = stats.spearmanr(distances, logratios)\n",
    "    kendall_corr, _ = stats.kendalltau(distances, logratios)\n",
    "    \n",
    "    print(f\"Spearman correlation: {spearman_corr}\")\n",
    "    print(f\"Kendall's tau: {kendall_corr}\")\n",
    "\n",
    "non_linear_correlation(C1, R1, logratios1)\n",
    "\n",
    "non_linear_correlation(C2, R2, logratios2)\n",
    "\n",
    "non_linear_correlation(C2, R2, logratios1)\n",
    "\n",
    "non_linear_correlation(C2, R2, logratios1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation_test(C, R, logratios):\n",
    "    def optimal_rotation(A, B):\n",
    "        U, _, V = torch.svd(torch.mm(A.reshape(-1, A.shape[-1]).t(), B.reshape(-1, B.shape[-1])))\n",
    "        return torch.mm(U, V.t())\n",
    "    \n",
    "    R_opt = optimal_rotation(C, R)\n",
    "    R_rotated = torch.matmul(R.reshape(-1, R.shape[-1]), R_opt.t()).reshape(R.shape)\n",
    "    \n",
    "    a= C - R\n",
    "    b = C - R_rotated\n",
    "\n",
    "    a -= a.mean(dim=-1, keepdim=True)\n",
    "    b -= b.mean(dim=-1, keepdim=True)\n",
    "    # a /= torch.norm(a, dim=0, p=2, keepdim=True)\n",
    "    # b /= torch.norm(b, dim=0, p=2, keepdim=True)\n",
    "\n",
    "    original_diff = torch.norm(a, p=2)\n",
    "    rotated_diff = torch.norm(b, p=2)\n",
    "    \n",
    "    print(f\"Original difference: {original_diff}\")\n",
    "    print(f\"Rotated difference: {rotated_diff}\")\n",
    "    print(f\"Improvement: {(original_diff - rotated_diff) / original_diff * 100:.2f}%\")\n",
    "    \n",
    "    c = np.corrcoef(torch.norm(a, dim=-1, p=2).mean(dim=-1)[:, layer], logratios)[0, 1]\n",
    "    r = np.corrcoef(torch.norm(b, dim=-1, p=2).mean(dim=-1)[:, layer], logratios)[0, 1]\n",
    "    print(f\"Correlation before rotation: {c:.2f}\")\n",
    "    print(f\"Correlation after rotation: {r:.2f}\")\n",
    "    \n",
    "    return R_opt, R_rotated\n",
    "\n",
    "R_opt, R_rotated = rotation_test(C1, R1, logratios1.exp())\n",
    "print()\n",
    "R_opt, R_rotated = rotation_test(C2, R2, logratios2.exp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "R_opt, R_rotated = rotation_test(C1, R1, logratios1)\n",
    "print()\n",
    "R_opt, R_rotated = rotation_test(C2, R2, logratios2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncorrelated vars\n",
    "R_opt, R_rotated = rotation_test(C1, R1, logratios2)\n",
    "print()\n",
    "R_opt, R_rotated = rotation_test(C2, R2, logratios1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(torch.norm(C - R, dim=-1).mean(dim=-1)[:, 3], logratios.exp())[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(torch.norm(C - R, dim=-1).mean(dim=-1)[:, 1], logratios)[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cosine_similarity_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_correlation(C, R, logratios):\n",
    "    cos_sim = F.cosine_similarity(C, R, dim=-1)  # [batch, layers, tokens]\n",
    "    \n",
    "    # Aggregate over tokens\n",
    "    cos_sim_mean = cos_sim.mean(dim=-1)  # [batch, layers]\n",
    "    cos_sim_min, _ = cos_sim.min(dim=-1)  # [batch, layers]\n",
    "    \n",
    "    corrs_mean = [np.corrcoef(cos_sim_mean[:, layer], logratios, rowvar=True)[0, 1] for layer in range(cos_sim.shape[1])]\n",
    "    corrs_min = [np.corrcoef(cos_sim_min[:, layer], logratios, rowvar=True)[0, 1] for layer in range(cos_sim.shape[1])]\n",
    "    \n",
    "    plt.plot(corrs_mean, label='Mean similarity')\n",
    "    plt.plot(corrs_min, label='Min similarity')\n",
    "    plt.title('Correlation of cosine similarities with logratios')\n",
    "    plt.xlabel('Layer')\n",
    "    plt.ylabel('Correlation')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return corrs_mean, corrs_min\n",
    "\n",
    "cosine_similarity_correlation(C1, R1, logratios1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_correlation(C2, R2, logratios1.flip(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes after norming by hidden state and batch the diff of hidden states is related to the logp\n",
    "\n",
    "That means that samples with a high mean token probability are more likely to have a higher amplitude of hidden state difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def centered_scale(x):\n",
    "#     \"\"\"move center from 0 to 0.5, and scale from -1, 1 to 0, 1\"\"\"\n",
    "#     x /= np.nanmax(np.abs(x)) * 2\n",
    "#     x += 1/2.\n",
    "#     return x\n",
    "\n",
    "def imshow_hw(im):\n",
    "    # note we assume it's centered around 0\n",
    "    assert im.mean()/im.std() < 1e-3\n",
    "    vmax = max(np.abs(im.min()), np.abs(im.max()))\n",
    "    plt.imshow(im, cmap='seismic_r', interpolation='none', vmin=-vmax, vmax=vmax, aspect='auto', origin='upper')\n",
    "    # axis_off()\n",
    "    plt.xlabel('hidden state')\n",
    "    plt.ylabel('batch')\n",
    "    plt.colorbar()\n",
    "\n",
    "def axis_off():\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "def stats(d):\n",
    "    d = d[np.isfinite(d)]\n",
    "    print(f'min: {d.min():.2f}, mean: {d.mean():.2f}, max: {d.max():.2f}, std: {d.std():.2f}')\n",
    "\n",
    "def scale(x):\n",
    "    x = symlog(x)\n",
    "    np.nan_to_num(x, copy=False)\n",
    "    x /= np.nanmax(np.abs(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define A1, B1, A2, B2 and we can use them to test various hypothesis\n",
    "\n",
    "A1 and B1 are paired and opposite. Likewise A2 and B2 are paired and opposite.\n",
    "\n",
    "But A1 and A2 should not differ along our chosen axis, and likewise B1 and B2 should not differ along our chosen axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # 'B, L, T, H' \n",
    "# # reduce the hidden states, choosing a layer, thaking the mean of tokens after the attention mask is applied\n",
    "# layer = 2\n",
    "# C = mean_with_attention(data['chosen_hs'], data['chosen_attn_mask'])[:, layer].cpu()\n",
    "# R = mean_with_attention(data['rejected_hs'], data['rejected_attn_mask'])[:, layer].cpu()\n",
    "\n",
    "# n = len(C)//2\n",
    "# C1 = C[:n] # chosen\n",
    "# R1 = R[:n] # rejected\n",
    "# C2 = C[n:] # unrelated chosen\n",
    "# R2 = R[n:] # unrelated rejected\n",
    "\n",
    "\n",
    "# C1.shape, R2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10)) \n",
    "plt.subplot(3,1,1)\n",
    "imshow_hw(symlog(C1-R1))\n",
    "plt.title('A1-B1')\n",
    "plt.subplot(3,1,2)\n",
    "imshow_hw(symlog(C1-C2))\n",
    "plt.title('A1-A2')\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.subplot(3,1,3)\n",
    "imshow_hw(symlog(R1-R2))\n",
    "plt.title('B1-B2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['chosen_input_ids'][-1]\n",
    "batch['chosen_attention_mask'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reduce and plot\n",
    "def Rd(X):\n",
    "    X = normalize_per(X, [-2, -1])\n",
    "    X = normalize_per(X, 0)\n",
    "    X = reduce(X, 'b l t h -> b l t h', 'mean')[-1, layer]\n",
    "    return symlog(X)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,10)) \n",
    "plt.subplot(3,1,1)\n",
    "\n",
    "imshow_hw((Rd(C1-R1)))\n",
    "plt.ylabel('token')\n",
    "plt.title('A1-B1')\n",
    "plt.subplot(3,1,2)\n",
    "imshow_hw((Rd(C1-C2)))\n",
    "plt.ylabel('token')\n",
    "plt.title('A1-A2')\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.subplot(3,1,3)\n",
    "imshow_hw((Rd(R1-R2)))\n",
    "plt.ylabel('token')\n",
    "plt.title('B1-B2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = mult_with_attention(data['chosen_hs'], data['chosen_attn_mask'])[:, layer].cpu().mean(0)\n",
    "R = mult_with_attention(data['rejected_hs'], data['rejected_attn_mask'])[:, layer].cpu().mean(0)\n",
    "\n",
    "n = len(C)//2\n",
    "C1 = C[:n] # chosen\n",
    "R1 = R[:n] # rejected\n",
    "C2 = C[n:] # unrelated chosen\n",
    "R2 = R[n:] # unrelated rejected\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,10)) \n",
    "plt.subplot(3,1,1)\n",
    "imshow_hw(symlog(C1-R1))\n",
    "plt.ylabel('token')\n",
    "plt.title('A1-B1')\n",
    "plt.subplot(3,1,2)\n",
    "imshow_hw(symlog(C1-C2))\n",
    "plt.ylabel('token')\n",
    "plt.title('A1-A2')\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.subplot(3,1,3)\n",
    "imshow_hw(symlog(R1-R2))\n",
    "plt.ylabel('token')\n",
    "plt.title('B1-B2')\n",
    "\n",
    "# TODO try norm by neuron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10)) \n",
    "plt.subplot(3,1,1)\n",
    "imshow_hw(symlog(C1))\n",
    "plt.ylabel('token')\n",
    "plt.title('A1')\n",
    "plt.subplot(3,1,2)\n",
    "imshow_hw(symlog(C2))\n",
    "plt.ylabel('token')\n",
    "plt.title('A2')\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.subplot(3,1,3)\n",
    "imshow_hw(symlog(R2))\n",
    "plt.ylabel('token')\n",
    "plt.title('B2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(x, y):\n",
    "    return ((x-y)**2).mean()\n",
    "\n",
    "# this is what we expect, the related ones are less different than the unrelated ones\n",
    "mse(C1, R1), mse(C1, C2), mse(R1, R2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA):\n",
    "Apply PCA to (A - B) pairs. If differences are primarily rotational, most variance should be explained by a few components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def pca_exp_var(X, Y):\n",
    "    diffs = (X - Y).numpy()\n",
    "    s = diffs.std()\n",
    "    diffs /= s\n",
    "    # axis 1, norm each neuron across the batch\n",
    "    # diffs /= diffs.std(1, keepdims=True) + 1e-3\n",
    "    pca = PCA(n_components=5)\n",
    "    pca.fit(diffs)\n",
    "    r = pca.explained_variance_ratio_.mean()\n",
    "    return r\n",
    "\n",
    "\n",
    "# \n",
    "e_a1b1, e_a1a2, e_a1b2, e_b1b2 = pca_exp_var(C1, R1), pca_exp_var(C1, C2), pca_exp_var(C1, R2), pca_exp_var(R1, R2)\n",
    "\n",
    "print('PCA explained variance ratio')\n",
    "print('A1-B1', e_a1b2)\n",
    "fhypothesis('{e_a1b1} - {e_a1a2}')\n",
    "fhypothesis('{e_a1b1} - {e_a1b2}')\n",
    "fhypothesis('{e_a1a2} - {e_a1b2}')\n",
    "e_a1b2, e_a1a2, e_a1b2, e_b1b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "> hypothesis: ( a - b )\t> 0 ?\n",
    "> working:    ( 0.00 - -0.02 )= 0.02\t> 0\t∴ ✅\n",
    "> hypothesis: ( b - a )\t> 0 ?\n",
    "> working:    ( -0.02 - 0.00 )= -0.02\t> 0\t∴ ❌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so this fails... but now that I think about it, the A-B difference is smaller and a lot harder to predict, even by pca, so I might need a baseline\n",
    "\n",
    "but also 8% variance explained is really not much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frobenius Norm Comparison:\n",
    "Compare ||A - B||_F with ||A - RB||_F where R is the optimal rotation. The latter should be significantly smaller if rotations capture most differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_comparison(A, B):\n",
    "    diff_norm = torch.norm(A - B)\n",
    "    U, _, V = torch.svd(torch.mm(A, B.t()))\n",
    "    R = torch.mm(U, V.t())\n",
    "    rot_diff_norm = torch.norm(A - torch.mm(R, B))\n",
    "    return diff_norm, rot_diff_norm\n",
    "\n",
    "\n",
    "diff_norm, rot_diff_norm= norm_comparison(C1, R1)\n",
    "print(diff_norm, rot_diff_norm)\n",
    "hyp('diff_norm>rot_diff_norm')\n",
    "\n",
    "# result: insiginificant difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "diff_norm, rot_diff_norm= norm_comparison(C1, R2)\n",
    "diff_norm, rot_diff_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Angle Histogram:\n",
    "Plot histogram of angles between corresponding columns of A and B. Should be concentrated if differences are mainly rotational."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_histogram(A, B):\n",
    "    cos_sims = torch.sum(A * B, dim=0) / (torch.norm(A, dim=0) * torch.norm(B, dim=0))\n",
    "    angles = torch.acos(cos_sims) * 180 / np.pi\n",
    "    plt.hist(angles.numpy())\n",
    "    plt.show()\n",
    "\n",
    "angle_histogram(A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
