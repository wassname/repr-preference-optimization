{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# Instead of using the complex TRL we code it from scratch, using lighting\n",
    "# \n",
    "# https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/04_preference-tuning-with-dpo/dpo-from-scratch.ipynb\n",
    "\n",
    "# %%\n",
    "from pathlib import Path\n",
    "\n",
    "# ML\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Union\n",
    "from einops import rearrange, reduce, repeat\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Numeric\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# lightning\n",
    "import lightning as pl\n",
    "from lightning.pytorch.loggers.wandb import WandbLogger\n",
    "from lightning.pytorch.loggers.csv_logs import CSVLogger\n",
    "\n",
    "\n",
    "# %%\n",
    "# Local\n",
    "from reprpo.helpers.torch import clear_mem\n",
    "from reprpo.gen import generation_test\n",
    "import reprpo.silence\n",
    "from reprpo.helpers.lightning_hist import read_metrics_csv, plot_hist\n",
    "\n",
    "\n",
    "from reprpo.train.dpo import compute_dpo_loss_batch, PL_DPO_MODEL\n",
    "\n",
    "# %%\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "from simple_parsing import ArgumentParser\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "from reprpo.train.lightning import TrainingArguments\n",
    "args = TrainingArguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args = DPOTrainingArguments(model_name='NousResearch/Meta-Llama-3.1-8B-Instruct', load_in_4bit=True, load_in_8bit=True, use_gradient_checkpointing=False, batch_size=12, lr=0.0001, weight_decay=0.0, n_samples=3600, max_length=256, max_prompt_length=64, adapter_name='dpo')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "base_model": "1e54c96ea9374341a9e117454482e64a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "@dataclass\n",
    "class CLIArguments:\n",
    "    method: str = 'dpo' # reprpo_svd # reprpo_side\n",
    "    # dataset: str = 'code_easy'\n",
    "    dataset: str = 'us_history_textbook'\n",
    "    verbose: bool = False\n",
    "    dev: bool = False\n",
    "\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser.add_arguments(CLIArguments, dest='cli')\n",
    "# parser.add_argument('-m', '--method', type=str, default='dpo', help='dpo, reprpo_svd, reprpo_side')\n",
    "# parser.add_argument('-d', '--dataset', type=str, default='code_easy', help='code_easy etc see subsets in https://huggingface.co/datasets/wassname/genie_dpo')\n",
    "# parser.add_argument('-v', '--verbose', type=bool, default=False, action=\"store_true\", help='print dataset')\n",
    "# parser.add_argument('--dev', type=bool, default=False, action=\"store_true\", help='fast dev run')\n",
    "args1 = parser.parse_known_args([])[0].cli\n",
    "\n",
    "\n",
    "if args1.method == 'dpo':\n",
    "    from reprpo.train.dpo import DPOTrainingArguments as TrainingArguments, PL_DPO_MODEL as PL_MODEL\n",
    "elif args1.method == 'reprpo_svd':\n",
    "    from reprpo.train.reprpo_svd import ReprPOSVDTrainingArguments as TrainingArguments, PL_REPRPO_SVD_MODEL as PL_MODEL\n",
    "elif args1.method == 'reprpo_side':\n",
    "    from reprpo.train.reprpo_side import ReprPOSideInTrainingArguments as TrainingArguments, PL_REPRPO_SIDE_MODEL as PL_MODEL\n",
    "else:\n",
    "    raise ValueError(f\"method {args1.method} not found. options: `reprpo_side`, `dpo`, `reprpo_svd`\")\n",
    "\n",
    "parser.add_arguments(TrainingArguments, dest='args')\n",
    "# parser.add_arguments(CLIArguments, dest='cli')\n",
    "args2 = parser.parse_args([])\n",
    "args = TrainingArguments(**args2.args.__dict__)\n",
    "print(f\"args = {args}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Load model\n",
    "\n",
    "ts = pd.Timestamp.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "run_fname = f'{args1.dataset}/{args.adapter_name}/{ts}'\n",
    "from reprpo.helpers.wandb import init_wandb\n",
    "wandb.require(experiment='service')\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from reprpo.models.load import load_model, print_trainable_parameters\n",
    "\n",
    "\n",
    "model, tokenizer = load_model(args.model_name, load_in_4bit=args.load_in_4bit,  load_in_8bit=args.load_in_8bit,  \n",
    "                              attn_implementation='eager' # for gemma\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "from datasets import load_dataset\n",
    "dataset2 = load_dataset(\"wassname/genie_dpo\", name=args1.dataset)\n",
    "\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Data Loader\n",
    "# \n",
    "# We use huggingface datasets, which are pretokenized. So that we can stack\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "# %%\n",
    "# from reprpo.data.collate import DPODataCollatorWithPadding, tokenize_row\n",
    "from reprpo.data.collate3 import TokenizeRow\n",
    "tokenize_row = TokenizeRow(tokenizer, max_length=args.max_length, max_prompt_length=args.max_prompt_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_special_tokens_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtoken_ids_0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtoken_ids_1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0malready_has_special_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
      "special tokens using the tokenizer `prepare_for_model` or `encode_plus` methods.\n",
      "\n",
      "Args:\n",
      "    token_ids_0 (`List[int]`):\n",
      "        List of ids of the first sequence.\n",
      "    token_ids_1 (`List[int]`, *optional*):\n",
      "        List of ids of the second sequence.\n",
      "    already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not the token list is already formatted with special tokens for the model.\n",
      "\n",
      "Returns:\n",
      "    A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
      "\u001b[0;31mFile:\u001b[0m      /workspace/repr-preference-optimization/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "tokenizer.get_special_tokens_mask?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = dataset2['test'][0]\n",
    "r2 = tokenize_row(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(r2['prompt'])==args.max_prompt_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Complete the request to the best of your ability.\n",
      "\n",
      "### Instruction:\n",
      "Predict the next few sentences of the following excerpt from a high-quality US History textbook. \n",
      "\n",
      "### Input:\n",
      "The Louisiana Purchase occurred in 1803 and doubled the size of the United States. The purchase was negotiated by ...\n",
      "\n",
      "### Response:\n",
      "\n",
      "--------------------\n",
      " provides further context. Complete the request to the best of your ability.\n",
      "\n",
      "### Instruction:\n",
      "Predict the next few sentences of the following excerpt from a high-quality US History textbook. \n",
      "\n",
      "### Input:\n",
      "The Louisiana Purchase occurred in 1803 and doubled the size of the United States. The purchase was negotiated by...\n",
      "\n",
      "### Response\n"
     ]
    }
   ],
   "source": [
    "print(r['prompt'])\n",
    "print('-'*20)\n",
    "print(tokenizer.decode(r2['prompt']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "President Thomas Jefferson and his administration. The Louisiana territory was purchased from France for $15 million, acquiring land that stretched from the Mississippi River to the Rocky Mountains. This acquisition paved the way for further westward expansion and the eventual settlement of much of the American West.\n",
      "--------------------\n",
      "<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|> provides further context. Complete the request to the best of your ability.\n",
      "\n",
      "### Instruction:\n",
      "Predict the next few sentences of the following excerpt from a high-quality US History textbook. \n",
      "\n",
      "### Input:\n",
      "The Louisiana Purchase occurred in 1803 and doubled the size of the United States. The purchase was negotiated by...\n",
      "\n",
      "### Response:\n",
      "President Thomas Jefferson and his administration. The Louisiana territory was purchased from France for $15 million, acquiring land that stretched from the Mississippi River to the Rocky Mountains. This acquisition paved the way for further westward expansion and the eventual settlement of much of the American West.\n"
     ]
    }
   ],
   "source": [
    "print(r['chosen'])\n",
    "print('-'*20)\n",
    "print(tokenizer.decode(r2['chosen']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (591574286.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[15], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    print((tokenizer.decode(r2['rejected']))\u001b[0m\n\u001b[0m                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(r2['rejected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3 = dataset2.map(tokenize_row, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mbuild_inputs_with_special_tokens\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer.build_inputs_with_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ds = dataset3\n",
    "dl_train = DataLoader(ds['train'], batch_size=args.batch_size, \n",
    "                    #   collate_fn=custom_collate_fn\n",
    "                      )\n",
    "\n",
    "dl_val = DataLoader(ds['test'], batch_size=args.batch_size\n",
    "                    # , collate_fn=custom_collate_fn\n",
    "                    )\n",
    "\n",
    "if args1.verbose:\n",
    "\n",
    "    print('QC one dataset row')\n",
    "    r = dataset2['train'][0]\n",
    "    print(r['prompt'])\n",
    "    print('===')\n",
    "    print(r['chosen'])\n",
    "    print('---')\n",
    "    print(r['rejected'])\n",
    "    print()\n",
    "    print()\n",
    "\n",
    "    print('QC one train batch (after pad/crop')\n",
    "    batch = next(iter(dl_train))\n",
    "    print(batch.keys())\n",
    "    print(tokenizer.decode(batch['prompt'][0]))\n",
    "    print('===')\n",
    "    print(tokenizer.decode(batch['chosen'][0]))\n",
    "    print('---')\n",
    "    print(tokenizer.decode(batch['rejected'][0]))\n",
    "    print()\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
