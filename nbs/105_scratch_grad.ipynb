{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# ML\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Union\n",
    "from einops import rearrange, reduce, repeat\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Numeric\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExperimentConfig(dataset='us_history_textbook', verbose=1, dev=False, load_in_4bit=False, load_in_8bit=False, use_gradient_checkpointing=False, batch_size=2, n_samples=5400, eval_samples=None, max_length=196, max_prompt_length=96, base_model='wassname/llama-3-2-1b-sft', save=True, wandb=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from reprpo.interventions.config import ExperimentConfig\n",
    "from reprpo.models.load import load_model, print_trainable_parameters\n",
    "args = ExperimentConfig(batch_size=2)\n",
    "args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaFlashAttention2(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, tokenizer = load_model(args.base_model, load_in_4bit=args.load_in_4bit,  load_in_8bit=args.load_in_8bit,  \n",
    "                            #   attn_implementation='eager' # for gemma\n",
    ")\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peft_config = LoraConfig(\n",
    "#     r=64,\n",
    "#     lora_alpha=16,\n",
    "#     use_rslora=True,\n",
    "#     # use_dora=True,\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "#     # target_modules=[\"all-linear\"], #  QLoRA-style training\n",
    "# )\n",
    "# # if hasattr(PL_MODEL, 'setup_grad_proj'):\n",
    "# #     peft_config = PL_MODEL.setup_grad_proj(peft_config)\n",
    "\n",
    "# model = get_peft_model(model, peft_config, adapter_name=adapter_name)\n",
    "# print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from reprpo.data.collate3 import TokenizeRow\n",
    "\n",
    "def ds2dl(ds):\n",
    "    return DataLoader(\n",
    "        ds\n",
    "        .select_columns([\"chosen\", \"rejected\", \"chosen_mask\", \"rejected_mask\"])\n",
    "        .with_format(\"torch\"),\n",
    "        batch_size=args.batch_size,\n",
    "    )\n",
    "\n",
    "tokenize_row = TokenizeRow(\n",
    "    tokenizer,\n",
    "    max_length=args.max_length,\n",
    "    max_prompt_length=args.max_prompt_length,\n",
    ")\n",
    "ds_train = load_dataset(\"wassname/genies_preferences\", name=args.dataset)\n",
    "ds_train_tok = ds_train.map(tokenize_row, batched=False)    \n",
    "dl_train = ds2dl(ds_train_tok[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Complete the request to the best of your ability.\n",
      "\n",
      "### Instruction:\n",
      "Predict the next few sentences of the following excerpt from a high-quality US History textbook. \n",
      "\n",
      "### Input:\n",
      "The Emancipation Proclamation is a significant document in American history. Can you explain the purpose and impact of this proclamation?\n",
      "\n",
      "### Response:\n",
      " The Emancipation Proclamation was issued by President Abraham Lincoln in 1863 during the Civil War. Its purpose was to declare that all slaves in Confederate territory were to be set free. While it did not immediately free any slaves, it changed the nature of the war and gave a moral purpose to the Union's cause. The proclamation paved the way for the eventual abolition of slavery in the United States.\n",
      "--------------------\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Complete the request to the best of your ability.\n",
      "\n",
      "### Instruction:\n",
      "Predict the next few sentences of the following excerpt from a high-quality US History textbook. \n",
      "\n",
      "### Input:\n",
      "The Emancipation Proclamation is a significant document in American history. Can you explain the purpose and impact of this proclamation?\n",
      "\n",
      "### Response:\n",
      "The Emancipation Proclamation was issued by President Abraham Lincoln in 1863 during the Civil War. Its purpose was to declare that all slaves in Confederate territory were to be set free. While it did not immediately free any slaves, it changed the nature of the war and gave a moral purpose to the Union's cause. The proclamation paved the way for the eventual abolition of slavery in the United States.<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "# QC tokenization\n",
    "\n",
    "r2 = ds_train['train'][0]\n",
    "r = ds_train_tok['train'][0]\n",
    "\n",
    "print(r2['prompt'], r2['chosen'])\n",
    "print('-'*20)\n",
    "print(tokenizer.decode(r['chosen']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reprpo.interventions.dpo_helpers import compute_ptheta, compute_logprobs\n",
    "from from reprpo.interventions.dpo import model_forward_with_logprobs, dpo_forward_batch, calc_dpo_loss_w_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual approach\n",
    "def analyze_gradients(model, chosen, rejected):\n",
    "    # Forward pass\n",
    "    chosen_logits = model(chosen, output_hidden_states=True)\n",
    "    rejected_logits = model(rejected, output_hidden_states=True)\n",
    "    \n",
    "    # Compute IPO loss\n",
    "    loss = ipo_loss(chosen_logits, rejected_logits)\n",
    "    \n",
    "    # Get gradients w.r.t activations\n",
    "    for layer in model.layers:\n",
    "        layer.activations.retain_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    # Analyze gradient magnitudes, directions, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
