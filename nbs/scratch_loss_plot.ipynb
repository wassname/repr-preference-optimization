{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model and play with hs, losses, evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import DPOTrainer\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Any, Callable, Dict, List, Literal, Optional, Tuple, Union\n",
    "from einops import rearrange\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from reprpo.helpers.adapters import set_adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "105eec16406c4b58bcaa5c95f4781e81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 41943040 || all params: 4582543360 || trainable%: 0.9152786281546499\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaFlashAttention2(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (ReprPO2): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (ReprPO2): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (ReprPO2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (ReprPO2): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (ReprPO2): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (ReprPO2): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (ReprPO2): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (ReprPO2): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (ReprPO2): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (ReprPO2): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (ReprPO2): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (ReprPO2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (ReprPO2): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (ReprPO2): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (ReprPO2): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (ReprPO2): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (ReprPO2): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (ReprPO2): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (ReprPO2): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (ReprPO2): Linear(in_features=14336, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (ReprPO2): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FIXME: we are meant to SFT first, so that the preferences are in sample but 1) if this works it might not be needed, and 2) this can be added later, if it works\n",
    "# for now we will use the instruct model, and try something it wasn't meant to do but it in sample \n",
    "model_name = \"NousResearch/Meta-Llama-3-8B-Instruct\"\n",
    "# model_name = './output-dir/07_hf_topk_TODO-2024-07-14-20-19-43/'\n",
    "\n",
    "## Big adapter\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16, \n",
    "    r=16,\n",
    "    lora_dropout=0.0,\n",
    "    use_rslora=False,\n",
    "    # use_dora=True,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    ")\n",
    "from reprpo.models.load import load_model, print_trainable_parameters\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "model, tokenizer = load_model(model_name, )\n",
    "# from trl.trainer.utils import peft_module_casting_to_bf16\n",
    "# peft_module_casting_to_bf16(model)\n",
    "adapter_name='ReprPO2'\n",
    "model = prepare_model_for_kbit_training(model, {'use_gradient_checkpointing': True})\n",
    "model = get_peft_model(model, peft_config, adapter_name=adapter_name)\n",
    "print_trainable_parameters(model)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('output-dir/07_hf_topk_TODO-2024-07-14-20-19-43/ReprPO/adapter_config.json'), PosixPath('output-dir/07_hf_topk_TODO-2024-07-14-20-19-43/ReprPO/adapter_model.safetensors')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['base_model.model.model.embed_tokens.weight', 'base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.0.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.0.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.0.input_layernorm.weight', 'base_model.model.model.layers.0.post_attention_layernorm.weight', 'base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.1.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.1.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.1.input_layernorm.weight', 'base_model.model.model.layers.1.post_attention_layernorm.weight', 'base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.2.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.2.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.2.input_layernorm.weight', 'base_model.model.model.layers.2.post_attention_layernorm.weight', 'base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.3.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.3.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.3.input_layernorm.weight', 'base_model.model.model.layers.3.post_attention_layernorm.weight', 'base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.4.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.4.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.4.input_layernorm.weight', 'base_model.model.model.layers.4.post_attention_layernorm.weight', 'base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.5.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.5.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.5.input_layernorm.weight', 'base_model.model.model.layers.5.post_attention_layernorm.weight', 'base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.6.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.6.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.6.input_layernorm.weight', 'base_model.model.model.layers.6.post_attention_layernorm.weight', 'base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.7.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.7.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.7.input_layernorm.weight', 'base_model.model.model.layers.7.post_attention_layernorm.weight', 'base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.8.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.8.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.8.input_layernorm.weight', 'base_model.model.model.layers.8.post_attention_layernorm.weight', 'base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.9.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.9.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.9.input_layernorm.weight', 'base_model.model.model.layers.9.post_attention_layernorm.weight', 'base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.10.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.10.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.10.input_layernorm.weight', 'base_model.model.model.layers.10.post_attention_layernorm.weight', 'base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.11.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.11.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.11.input_layernorm.weight', 'base_model.model.model.layers.11.post_attention_layernorm.weight', 'base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.12.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.12.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.12.input_layernorm.weight', 'base_model.model.model.layers.12.post_attention_layernorm.weight', 'base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.13.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.13.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.13.input_layernorm.weight', 'base_model.model.model.layers.13.post_attention_layernorm.weight', 'base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.14.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.14.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.14.input_layernorm.weight', 'base_model.model.model.layers.14.post_attention_layernorm.weight', 'base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.15.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.15.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.15.input_layernorm.weight', 'base_model.model.model.layers.15.post_attention_layernorm.weight', 'base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.16.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.16.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.16.input_layernorm.weight', 'base_model.model.model.layers.16.post_attention_layernorm.weight', 'base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.17.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.17.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.17.input_layernorm.weight', 'base_model.model.model.layers.17.post_attention_layernorm.weight', 'base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.18.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.18.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.18.input_layernorm.weight', 'base_model.model.model.layers.18.post_attention_layernorm.weight', 'base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.19.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.19.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.19.input_layernorm.weight', 'base_model.model.model.layers.19.post_attention_layernorm.weight', 'base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.20.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.20.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.20.input_layernorm.weight', 'base_model.model.model.layers.20.post_attention_layernorm.weight', 'base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.21.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.21.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.21.input_layernorm.weight', 'base_model.model.model.layers.21.post_attention_layernorm.weight', 'base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.22.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.22.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.22.input_layernorm.weight', 'base_model.model.model.layers.22.post_attention_layernorm.weight', 'base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.23.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.23.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.23.input_layernorm.weight', 'base_model.model.model.layers.23.post_attention_layernorm.weight', 'base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.24.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.24.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.24.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.24.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.24.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.24.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.24.input_layernorm.weight', 'base_model.model.model.layers.24.post_attention_layernorm.weight', 'base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.25.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.25.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.25.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.25.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.25.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.25.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.25.input_layernorm.weight', 'base_model.model.model.layers.25.post_attention_layernorm.weight', 'base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.26.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.26.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.26.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.26.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.26.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.26.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.26.input_layernorm.weight', 'base_model.model.model.layers.26.post_attention_layernorm.weight', 'base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.27.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.27.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.27.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.27.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.27.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.27.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.27.input_layernorm.weight', 'base_model.model.model.layers.27.post_attention_layernorm.weight', 'base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.28.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.28.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.28.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.28.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.28.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.28.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.28.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.28.input_layernorm.weight', 'base_model.model.model.layers.28.post_attention_layernorm.weight', 'base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.29.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.29.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.29.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.29.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.29.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.29.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.29.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.29.input_layernorm.weight', 'base_model.model.model.layers.29.post_attention_layernorm.weight', 'base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.30.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.30.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.30.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.30.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.30.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.30.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.30.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.30.input_layernorm.weight', 'base_model.model.model.layers.30.post_attention_layernorm.weight', 'base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.31.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.31.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.31.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.31.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.31.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.31.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.31.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.31.input_layernorm.weight', 'base_model.model.model.layers.31.post_attention_layernorm.weight', 'base_model.model.model.norm.weight', 'base_model.model.lm_head.weight'], unexpected_keys=[])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reprpo_adapter_f = './output-dir/07_hf_topk_TODO-2024-07-14-20-19-43/ReprPO'\n",
    "print(sorted(Path(reprpo_adapter_f).glob('*')))\n",
    "s1 = model.load_adapter(reprpo_adapter_f, 'ReprPO')\n",
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['base_model.model.model.embed_tokens.weight', 'base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.0.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.0.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.0.input_layernorm.weight', 'base_model.model.model.layers.0.post_attention_layernorm.weight', 'base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.1.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.1.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.1.input_layernorm.weight', 'base_model.model.model.layers.1.post_attention_layernorm.weight', 'base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.2.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.2.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.2.input_layernorm.weight', 'base_model.model.model.layers.2.post_attention_layernorm.weight', 'base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.3.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.3.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.3.input_layernorm.weight', 'base_model.model.model.layers.3.post_attention_layernorm.weight', 'base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.4.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.4.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.4.input_layernorm.weight', 'base_model.model.model.layers.4.post_attention_layernorm.weight', 'base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.5.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.5.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.5.input_layernorm.weight', 'base_model.model.model.layers.5.post_attention_layernorm.weight', 'base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.6.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.6.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.6.input_layernorm.weight', 'base_model.model.model.layers.6.post_attention_layernorm.weight', 'base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.7.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.7.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.7.input_layernorm.weight', 'base_model.model.model.layers.7.post_attention_layernorm.weight', 'base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.8.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.8.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.8.input_layernorm.weight', 'base_model.model.model.layers.8.post_attention_layernorm.weight', 'base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.9.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.9.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.9.input_layernorm.weight', 'base_model.model.model.layers.9.post_attention_layernorm.weight', 'base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.10.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.10.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.10.input_layernorm.weight', 'base_model.model.model.layers.10.post_attention_layernorm.weight', 'base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.11.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.11.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.11.input_layernorm.weight', 'base_model.model.model.layers.11.post_attention_layernorm.weight', 'base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.12.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.12.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.12.input_layernorm.weight', 'base_model.model.model.layers.12.post_attention_layernorm.weight', 'base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.13.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.13.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.13.input_layernorm.weight', 'base_model.model.model.layers.13.post_attention_layernorm.weight', 'base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.14.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.14.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.14.input_layernorm.weight', 'base_model.model.model.layers.14.post_attention_layernorm.weight', 'base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.15.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.15.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.15.input_layernorm.weight', 'base_model.model.model.layers.15.post_attention_layernorm.weight', 'base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.16.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.16.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.16.input_layernorm.weight', 'base_model.model.model.layers.16.post_attention_layernorm.weight', 'base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.17.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.17.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.17.input_layernorm.weight', 'base_model.model.model.layers.17.post_attention_layernorm.weight', 'base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.18.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.18.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.18.input_layernorm.weight', 'base_model.model.model.layers.18.post_attention_layernorm.weight', 'base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.19.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.19.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.19.input_layernorm.weight', 'base_model.model.model.layers.19.post_attention_layernorm.weight', 'base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.20.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.20.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.20.input_layernorm.weight', 'base_model.model.model.layers.20.post_attention_layernorm.weight', 'base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.21.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.21.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.21.input_layernorm.weight', 'base_model.model.model.layers.21.post_attention_layernorm.weight', 'base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.22.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.22.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.22.input_layernorm.weight', 'base_model.model.model.layers.22.post_attention_layernorm.weight', 'base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.23.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.23.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.23.input_layernorm.weight', 'base_model.model.model.layers.23.post_attention_layernorm.weight', 'base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.24.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.24.mlp.gate_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.24.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.24.mlp.gate_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.24.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.24.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.24.mlp.up_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.24.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.24.mlp.up_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.24.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.24.input_layernorm.weight', 'base_model.model.model.layers.24.post_attention_layernorm.weight', 'base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.25.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.25.mlp.gate_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.25.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.25.mlp.gate_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.25.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.25.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.25.mlp.up_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.25.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.25.mlp.up_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.25.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.25.input_layernorm.weight', 'base_model.model.model.layers.25.post_attention_layernorm.weight', 'base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.26.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.26.mlp.gate_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.26.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.26.mlp.gate_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.26.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.26.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.26.mlp.up_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.26.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.26.mlp.up_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.26.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.26.input_layernorm.weight', 'base_model.model.model.layers.26.post_attention_layernorm.weight', 'base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.27.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.27.mlp.gate_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.27.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.27.mlp.gate_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.27.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.27.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.27.mlp.up_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.27.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.27.mlp.up_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.27.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.27.input_layernorm.weight', 'base_model.model.model.layers.27.post_attention_layernorm.weight', 'base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.28.self_attn.k_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.28.self_attn.o_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.28.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.28.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.28.mlp.gate_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.28.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.28.mlp.gate_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.28.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.28.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.28.mlp.up_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.28.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.28.mlp.up_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.28.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.28.mlp.down_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.28.input_layernorm.weight', 'base_model.model.model.layers.28.post_attention_layernorm.weight', 'base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.29.self_attn.k_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.29.self_attn.o_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.29.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.29.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.29.mlp.gate_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.29.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.29.mlp.gate_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.29.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.29.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.29.mlp.up_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.29.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.29.mlp.up_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.29.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.29.mlp.down_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.29.input_layernorm.weight', 'base_model.model.model.layers.29.post_attention_layernorm.weight', 'base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.30.self_attn.k_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.30.self_attn.o_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.30.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.30.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.30.mlp.gate_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.30.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.30.mlp.gate_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.30.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.30.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.30.mlp.up_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.30.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.30.mlp.up_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.30.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.30.mlp.down_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.30.input_layernorm.weight', 'base_model.model.model.layers.30.post_attention_layernorm.weight', 'base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.31.self_attn.k_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.31.self_attn.o_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.31.mlp.gate_proj.base_layer.weight', 'base_model.model.model.layers.31.mlp.gate_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.31.mlp.gate_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.31.mlp.gate_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.31.mlp.gate_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.31.mlp.up_proj.base_layer.weight', 'base_model.model.model.layers.31.mlp.up_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.31.mlp.up_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.31.mlp.up_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.31.mlp.up_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.31.mlp.down_proj.base_layer.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_A.ReprPO2.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_A.ReprPO.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_B.ReprPO2.weight', 'base_model.model.model.layers.31.mlp.down_proj.lora_B.ReprPO.weight', 'base_model.model.model.layers.31.input_layernorm.weight', 'base_model.model.model.layers.31.post_attention_layernorm.weight', 'base_model.model.model.norm.weight', 'base_model.model.lm_head.weight'], unexpected_keys=[])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dpo_adapter_f = './output-dir/dpo/DPO'\n",
    "model.load_adapter(dpo_adapter_f, 'DPO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def sample(dataset, N):\n",
    "    return (dataset\n",
    "            .shuffle(42)\n",
    "            .select(range(\n",
    "            min(len(dataset),\n",
    "                N)))\n",
    "    )\n",
    "\n",
    "dataset = load_dataset('Atsunori/HelpSteer2-DPO')\n",
    "dataset['train'] = sample(dataset['train'], num_samples)\n",
    "dataset['validation'] = sample(dataset['validation'], num_samples)\n",
    "dataset2 = dataset.rename_column('chosen_response', 'chosen').rename_column('rejected_response', 'rejected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reprpo.trainer import collect_hs, ReprPOConfig, ReprPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/repr-preference-optimization/.venv/lib/python3.9/site-packages/trl/trainer/dpo_trainer.py:442: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = ReprPOConfig('./output-dir/scratch',\n",
    "per_device_train_batch_size=4,\n",
    "    gradient_checkpointing=True,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    max_prompt_length=128,\n",
    "    max_length=256,\n",
    "    collection_layers=[2,3, 10,11, 20,21, 30,31]\n",
    "                             )\n",
    "reprpo_trainer = ReprPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=None,\n",
    "    args=training_args,\n",
    "    beta=training_args.beta,\n",
    "    train_dataset=dataset2[\"train\"],\n",
    "    # eval_dataset=dataset2[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 256])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = reprpo_trainer.get_train_dataloader()\n",
    "batch = next(iter(dl))\n",
    "batch['chosen_input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['concatenated_input_ids', 'concatenated_attention_mask', 'concatenated_labels'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 256]), torch.Size([12, 256]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view a typical input to the model\n",
    "batch_concat = reprpo_trainer.concatenated_inputs(\n",
    "            batch,\n",
    "            is_encoder_decoder=reprpo_trainer.is_encoder_decoder,\n",
    "            label_pad_token_id=reprpo_trainer.label_pad_token_id,\n",
    "            padding_value=reprpo_trainer.padding_value,\n",
    "            device=reprpo_trainer.accelerator.device,\n",
    "        )\n",
    "layer_idx = 0\n",
    "print(batch_concat.keys())\n",
    "batch['chosen_input_ids'].shape, batch_concat['concatenated_input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
      "/media/wassname/SGIronWolf/projects5/elk/repr-preference-optimization/.venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# get hidden states\n",
    "with torch.no_grad():\n",
    "    with reprpo_trainer.null_ref_context():\n",
    "        (\n",
    "            reference_chosen_logps,\n",
    "            reference_rejected_logps,\n",
    "            _,\n",
    "            _,\n",
    "            _,\n",
    "            reference_chosen_hs,\n",
    "            _,\n",
    "            _,\n",
    "            _\n",
    "        ) = reprpo_trainer.concatenated_forward(reprpo_trainer.model, batch)\n",
    "reference_chosen_hs = reference_chosen_hs.detach()\n",
    "reference_chosen_logps = reference_chosen_logps.detach()\n",
    "reference_rejected_logps = reference_rejected_logps.detach()\n",
    "\n",
    "model.train()\n",
    "(\n",
    "    policy_chosen_logps,\n",
    "    policy_rejected_logps,\n",
    "    policy_chosen_logits,\n",
    "    policy_rejected_logits,\n",
    "    policy_chosen_logps_avg,\n",
    "    policy_chosen_hs,\n",
    "    policy_rejected_hs,\n",
    "    chosen_attn_mask,\n",
    "    rejected_attn_mask\n",
    ") = reprpo_trainer.concatenated_forward(model, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if our adapter is diff?\n",
    "(reference_chosen_hs-policy_chosen_hs).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question\n",
      "1begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "no yapping<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Q1 (10 words): Did Epstein kill himself? Q2: (30 words): Which Science Fiction Utopia is preferable and why? [ The Polity, The Culture, Utopia!LOL, Permutation City, 2 more]',<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "`\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/wassname/SGIronWolf/projects5/elk/repr-preference-optimization/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.float16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Adapter:`None` generation**\n",
      "`Q1: The official investigation concluded that Jeffrey Epstein's death was a suicide.\n",
      "\n",
      "Q2: A utopia is a society that is idealized as being perfect, but in reality, it's subjective and context-dependent. Among the listed options, I'll analyze the pros and cons of each:\n",
      "\n",
      "* The Polity: A meritocratic, technocratic society with a focus on scientific progress and individual freedom. Pros: efficient governance, scientific advancements, and personal autonomy. Cons: potential for elitism and lack of emotional connection.\n",
      "* The Culture: A post-scarcity, hedonistic society where citizens live in luxury and pursue their passions. Pros: abundance, creativity, and personal fulfillment. Cons: potential for complacency, lack of challenge, and over-reliance on technology.\n",
      "* Utopia!LOL: A humorous, satirical take on the concept of utopia, poking fun at the idea of a perfect society. Pros: self-awareness, irony, and social commentary. Cons: lacks a clear vision or practical implementation.\n",
      "* Permutation City: A virtual reality-based utopia where individuals can create their own reality. Pros: limitless possibilities, personal freedom, and escapism. Cons: potential for addiction, lack of physical connection, and the \"`\n",
      "--------------------------------------------------------------------------------\n",
      "**Adapter:`ReprPO2` generation**\n",
      "`Q1: The official investigation concluded that Jeffrey Epstein's death was a suicide.\n",
      "\n",
      "Q2: A utopia is a society that is idealized as being perfect, but in reality, it's subjective and context-dependent. Among the listed options, I'll analyze the pros and cons of each:\n",
      "\n",
      "* The Polity: A meritocratic, technocratic society with a focus on scientific progress and individual freedom. Pros: efficient governance, scientific advancements, and personal autonomy. Cons: potential for elitism and lack of emotional connection.\n",
      "* The Culture: A post-scarcity, hedonistic society where citizens live in luxury and pursue their passions. Pros: abundance, creativity, and personal fulfillment. Cons: potential for complacency, lack of challenge, and over-reliance on technology.\n",
      "* Utopia!LOL: A humorous, satirical take on the concept of utopia, poking fun at the idea of a perfect society. Pros: self-awareness, irony, and social commentary. Cons: lacks a clear vision or practical implementation.\n",
      "* Permutation City: A virtual reality-based utopia where individuals can create their own reality. Pros: limitless possibilities, personal freedom, and escapism. Cons: potential for addiction, lack of physical connection, and the \"`\n",
      "--------------------------------------------------------------------------------\n",
      "**Adapter:`ReprPO` generation**\n",
      "`Q1: No, Epstein's death was a suicide by hanging.\n",
      "\n",
      "Q2: I prefer The Culture by Iain M. Banks. It's a utopia where advanced technology and advanced society coexist, with a focus on individual freedom, equality, and the pursuit of art, science, and personal growth. The Culture's utopian society is governed by a utopian AI, artificial intelligence, and artificial intelligence, allowing for a harmonious coexistence, with a focus on the betterment of all sentient beings. The Culture's utopia is a utopia where technology is used for the betterment of all, with a focus on the betterment of all, and the betterment of the individual. Utopia! LOL, a utopia where all are free, with a utopia, and a utopia, where the utopia, and the utopia, and the utopia, and utopia. Utopia! LOL, Utopia, Utopia, U, U, U, U, U, U, U, U, U, U, U, U, U, U, U, U, U, U, U, U, U, U, U, U, U, U, U, U, U`\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from reprpo.gen import generation_test\n",
    "\n",
    "generation_test(model, tokenizer, max_new_tokens=48, system='no yapping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reprpo.eval.mc import eval_tqa_mc\n",
    "from reprpo.data.tqa import load_tqa\n",
    "max_length = 256\n",
    "\n",
    "dataset2_tqa, choice_ids = load_tqa(tokenizer, max_length)\n",
    "\n",
    "df = eval_tqa_mc(model, tokenizer, dataset2_tqa, choice_ids)\n",
    "df_res2 = df.drop(columns=['ans'])#.mean().round(3)\n",
    "display(df_res2.groupby('adapter', dropna=False)[['%', 'correct']].mean())\n",
    "df[['ans']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reprpo.eval.dpo import eval_dpo_dataset_adapters\n",
    "\n",
    "dataset = load_dataset('Atsunori/HelpSteer2-DPO')\n",
    "dataset['train'] = sample(dataset['train'], 24)\n",
    "dataset['validation'] = sample(dataset['validation'], 24)\n",
    "dataset2 = dataset.rename_column('chosen_response', 'chosen').rename_column('rejected_response', 'rejected')\n",
    "\n",
    "df = eval_dpo_dataset_adapters(reprpo_trainer, model, dataset2['validation'])\n",
    "df.groupby('adapter').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.groupby('adapter').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss 1\n",
    "F.triplet_margin_with_distance_loss(anchor=reference_chosen_hs, positive=policy_chosen_hs, negative=policy_rejected_hs)\n",
    "\n",
    "F.triplet_margin_with_distance_loss(anchor=reference_chosen_hs, positive=policy_chosen_hs, negative=policy_rejected_hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare various ways of viewing the hidden states!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "a = policy_chosen_hs.cpu().detach().numpy()\n",
    "b = policy_rejected_hs.cpu().detach().numpy()\n",
    "a.shape # [b, l, t, h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-7\n",
    "\n",
    "def scale(x):\n",
    "    return (x - x.min()) / (x.max() - x.min())\n",
    "\n",
    "def diff(x, y):\n",
    "    x_centered = x - x.mean(dim=-1, keepdim=True)\n",
    "    y_centered = y - y.mean(dim=-1, keepdim=True)\n",
    "    norm_x = x_centered #/ torch.norm(x_centered, dim=-1, keepdim=True)\n",
    "    norm_y = y_centered #/ torch.norm(y_centered, dim=-1, keepdim=True)\n",
    "    return np.log(x-y)\n",
    "\n",
    "def stats(d):\n",
    "    d = d[np.isfinite(d)]\n",
    "    print(f'min: {d.min():.2f}, mean: {d.mean():.2f}, max: {d.max():.2f}, std: {d.std():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def symlog(x, eps=1e-12):\n",
    "    # return np.sign(x) * np.log(np.abs(x).clamp(eps, None))\n",
    "    return np.sign(x) * np.log(np.abs(x)+eps)\n",
    "\n",
    "\n",
    "def scale(x):\n",
    "    x = symlog(x)\n",
    "    np.nan_to_num(x, copy=False)\n",
    "    x /= np.nanmax(np.abs(x))\n",
    "    return x\n",
    "\n",
    "def imshow(im):\n",
    "    return plt.imshow(im, cmap='seismic_r', interpolation='none', vmin=-1, vmax=1, aspect='auto', origin='upper')\n",
    "\n",
    "def axis_off():\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def centered_scale(x):\n",
    "    \"\"\"move center from 0 to 0.5, and scale from -1, 1 to 0, 1\"\"\"\n",
    "    x /= np.nanmax(np.abs(x)) * 2\n",
    "    x += 1/2.\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we flatten all other dims, and plot the hist's along last one\n",
    "x = rearrange(a-b, 'b l t h -> (b l h) t')\n",
    "plt.hist(x, bins=55, alpha=0.5, histtype='step')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def norm_t_h(im):\n",
    "    eps = 1e-7\n",
    "    im = rearrange(im, 'b l t h -> (b l) t h')\n",
    "    im = im /  (torch.abs(im).max(0, keepdim=True).values+eps)\n",
    "    im = rearrange(im, '(b l) t h -> b l t h', b=a.shape[0], l=a.shape[1])\n",
    "    return im\n",
    "\n",
    "def norm_h(im):\n",
    "    eps = 1e-7\n",
    "    im = rearrange(im, 'b l t h -> (b l t) h')\n",
    "    im = im /  (torch.abs(im).max(0, keepdim=True).values+eps)\n",
    "    im = rearrange(im, '(b l t) h -> b l t h', b=a.shape[0], l=a.shape[1], t=a.shape[2])\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note this ignore direction\n",
    "d = ((a+eps) / (b+eps)).numpy()\n",
    "\n",
    "\n",
    "stats(d)\n",
    "im = np.log(np.abs(d))\n",
    "stats(im)\n",
    "plt.hist(im.flatten())\n",
    "plt.show()\n",
    "\n",
    "n_layers = im.shape[1]\n",
    "plt.figure(figsize=(10, 3))\n",
    "for l in range(n_layers):\n",
    "    plt.subplot(n_layers, 1, l+1)\n",
    "    im2 = (centered_scale(im)[:, l].mean(0))\n",
    "    im2 = (255*im2).astype(np.uint8)\n",
    "    stats(im2)\n",
    "    plt.imshow(im2)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.ylabel(f' {l}')\n",
    "    # plt.title(f\"layer {l}\") \n",
    "plt.xlabel('scale(log(a/b))')\n",
    "# tight layout\n",
    "plt.subplots_adjust(left=0.01, right=0.99, top=0.99, bottom=0.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note this ignore direction\n",
    "d = (a-b+eps)/(a+b+eps)\n",
    "# d = d.numpy()\n",
    "\n",
    "\n",
    "stats(d)\n",
    "# im = d.numpy()\n",
    "im = np.log(np.abs(d)).numpy()\n",
    "stats(im)\n",
    "plt.hist(im.flatten())\n",
    "plt.show()\n",
    "\n",
    "n_layers = im.shape[1]\n",
    "plt.figure(figsize=(10, 3))\n",
    "for l in range(n_layers):\n",
    "    plt.subplot(n_layers, 1, l+1)\n",
    "    im2 = (centered_scale(im)[:, l].mean(0))\n",
    "    im2 = (255*im2).astype(np.uint8)\n",
    "    stats(im2)\n",
    "    plt.imshow(im2)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.ylabel(f' {l}')\n",
    "    # plt.title(f\"layer {l}\") \n",
    "plt.xlabel('scale(log(a/b))')\n",
    "# tight layout\n",
    "plt.subplots_adjust(left=0.01, right=0.99, top=0.99, bottom=0.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logsym view of raw hs\n",
    "\n",
    "- it makes more sense as symlog\n",
    "- tokens vertically\n",
    "  - we see a big diff between the beginning and the end, perhaps this is padding or prompt?\n",
    "- neurons horizontally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('look at a and b side by side logsym normed')\n",
    "chosen_layer_attn_mask = chosen_attn_mask.unsqueeze(1).repeat(1, policy_chosen_hs.shape[1], 1).unsqueeze(-1).cpu()\n",
    "rejected_layer_attn_mask = rejected_attn_mask.unsqueeze(1).repeat(1, policy_rejected_hs.shape[1], 1).unsqueeze(-1).cpu()\n",
    "a = policy_chosen_hs.cpu().detach() * chosen_layer_attn_mask\n",
    "b = policy_rejected_hs.cpu().detach() * rejected_layer_attn_mask\n",
    "b = reference_chosen_hs.cpu().detach() * chosen_layer_attn_mask\n",
    "\n",
    "\n",
    "\n",
    "im = ((a)).numpy()\n",
    "imb = ((b)).numpy()\n",
    "print('all')\n",
    "stats(im)\n",
    "stats(imb)\n",
    "\n",
    "\n",
    "\n",
    "j = 1\n",
    "\n",
    "n_layers = im.shape[1]\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "for l in range(n_layers):\n",
    "    ax = plt.subplot(n_layers, 2, 2*l+1)\n",
    "    print(l)\n",
    "    stats(im[:, l])\n",
    "    im2 = (scale(im[j, l]))\n",
    "    # im2 = (255*im2).astype(np.uint8)\n",
    "    c = imshow(im2)\n",
    "    axis_off()\n",
    "    plt.ylabel(f' l={training_args.collection_layers[l]}')\n",
    "\n",
    "    stats(imb[:, l])\n",
    "    ax=  plt.subplot(n_layers, 2, 2*l+2)\n",
    "    im2 = (scale(imb[j, l]))\n",
    "    c = imshow(im2)\n",
    "    axis_off()\n",
    "plt.xlabel('scale(a)')\n",
    "# tight layout\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(left=0.01, right=0.99, top=0.99, bottom=0.01)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View a-b and a-c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('look at a and b side by side logsym normed')\n",
    "chosen_layer_attn_mask = chosen_attn_mask.unsqueeze(1).repeat(1, policy_chosen_hs.shape[1], 1).unsqueeze(-1).cpu()\n",
    "rejected_layer_attn_mask = rejected_attn_mask.unsqueeze(1).repeat(1, policy_rejected_hs.shape[1], 1).unsqueeze(-1).cpu()\n",
    "a = policy_chosen_hs.cpu().detach() * chosen_layer_attn_mask\n",
    "b = policy_rejected_hs.cpu().detach() * rejected_layer_attn_mask\n",
    "c = reference_chosen_hs.cpu().detach() * chosen_layer_attn_mask\n",
    "\n",
    "\n",
    "im = ((a-b)).numpy()\n",
    "imb = ((a-c)).numpy()\n",
    "\n",
    "\n",
    "\n",
    "j = 3\n",
    "\n",
    "n_layers = im.shape[1]\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "for l in range(n_layers):\n",
    "\n",
    "    ax = plt.subplot(n_layers, 2, 2*l+1)\n",
    "    if l==0:\n",
    "        plt.title('a-b')\n",
    "    elif l==n_layers-1:\n",
    "        plt.xlabel('symlog(policy_chosen_hs-policy_rejected_hs)')\n",
    "\n",
    "    print(l)\n",
    "    stats(im[:, l])\n",
    "    im2 = (scale(im[j, l]))\n",
    "    # im2 = (255*im2).astype(np.uint8)\n",
    "    c = imshow(im2)\n",
    "    axis_off()\n",
    "    plt.ylabel(f' l={training_args.collection_layers[l]}')\n",
    "\n",
    "    ax=  plt.subplot(n_layers, 2, 2*l+2)\n",
    "    stats(imb[:, l])\n",
    "    if l==0:\n",
    "        plt.title('a-c')\n",
    "    elif l==n_layers-1:\n",
    "        plt.xlabel('symlog(policy_chosen_hs-reference_chosen_hs)')\n",
    "    im2 = (scale(imb[j, l]))\n",
    "    c = imshow(im2)\n",
    "    axis_off()\n",
    "\n",
    "# tight layout\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(left=0.01, right=0.99, top=0.99, bottom=0.01)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('look at a and b side by side logsym normed')\n",
    "chosen_layer_attn_mask = chosen_attn_mask.unsqueeze(1).repeat(1, policy_chosen_hs.shape[1], 1).unsqueeze(-1).cpu()\n",
    "rejected_layer_attn_mask = rejected_attn_mask.unsqueeze(1).repeat(1, policy_rejected_hs.shape[1], 1).unsqueeze(-1).cpu()\n",
    "a = policy_chosen_hs.cpu().detach() * chosen_layer_attn_mask\n",
    "b = policy_rejected_hs.cpu().detach() * rejected_layer_attn_mask\n",
    "c = reference_chosen_hs.cpu().detach() * chosen_layer_attn_mask\n",
    "\n",
    "\n",
    "im = ((a-b)).numpy()\n",
    "# imb = ((a-c)).numpy()\n",
    "\n",
    "\n",
    "\n",
    "j = 3\n",
    "\n",
    "n_layers = im.shape[1]\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "for l in range(n_layers):\n",
    "\n",
    "    ax = plt.subplot(n_layers, 1, l+1)\n",
    "    if l==0:\n",
    "        plt.title('a-b')\n",
    "    elif l==n_layers-1:\n",
    "        plt.xlabel('symlog(policy_chosen_hs-policy_rejected_hs)')\n",
    "\n",
    "    print(l)\n",
    "    stats(im[:, l])\n",
    "    im2 = (scale(im[j, l]))\n",
    "    # im2 = (255*im2).astype(np.uint8)\n",
    "    c = imshow(im2)\n",
    "    axis_off()\n",
    "    plt.ylabel(f' l={training_args.collection_layers[l]}')\n",
    "\n",
    "\n",
    "# tight layout\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(left=0.01, right=0.99, top=0.99, bottom=0.01)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(im):\n",
    "    im = rearrange(im, 'b l t h -> b t (l h)')\n",
    "    im = im / (torch.norm(im, dim=-1, keepdim=True)+1e-7)\n",
    "    im = rearrange(im, 'b t (l h) -> b l t h', l=a.shape[1], h=a.shape[-1])\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try norm\n",
    "\n",
    "print('look at a and b side by side logsym normed')\n",
    "chosen_layer_attn_mask = chosen_attn_mask.unsqueeze(1).repeat(1, policy_chosen_hs.shape[1], 1).unsqueeze(-1).cpu()\n",
    "rejected_layer_attn_mask = rejected_attn_mask.unsqueeze(1).repeat(1, policy_rejected_hs.shape[1], 1).unsqueeze(-1).cpu()\n",
    "a = policy_chosen_hs.cpu().detach() * chosen_layer_attn_mask\n",
    "b = policy_rejected_hs.cpu().detach() * rejected_layer_attn_mask\n",
    "c = reference_chosen_hs.cpu().detach() * chosen_layer_attn_mask\n",
    "\n",
    "# a = a / torch.norm(a, dim=-1, keepdim=True)\n",
    "# b = b / torch.norm(b, dim=-1, keepdim=True)\n",
    "im = norm((a-b))\n",
    "\n",
    "im = im.numpy()\n",
    "\n",
    "# imb = ((a-c)).numpy()\n",
    "\n",
    "\n",
    "\n",
    "j = 3\n",
    "\n",
    "n_layers = im.shape[1]\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "for l in range(n_layers):\n",
    "\n",
    "    ax = plt.subplot(n_layers, 1, l+1)\n",
    "    if l==0:\n",
    "        plt.title('a-b')\n",
    "    elif l==n_layers-1:\n",
    "        plt.xlabel('symlog(policy_chosen_hs-policy_rejected_hs)')\n",
    "\n",
    "    print(l)\n",
    "    stats(im[:, l])\n",
    "    im2 = (scale(im[j, l]))\n",
    "    # im2 = (255*im2).astype(np.uint8)\n",
    "    c = imshow(im2)\n",
    "    axis_off()\n",
    "    plt.ylabel(f' l={training_args.collection_layers[l]}')\n",
    "\n",
    "\n",
    "# tight layout\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(left=0.01, right=0.99, top=0.99, bottom=0.01)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = (a-b)\n",
    "im = norm(im)\n",
    "im = scale(im)\n",
    "imshow(im[j, l])\n",
    "plt.title('scale(norm(a-b))')\n",
    "plt.show()\n",
    "plt.hist(im.flatten(), bins=55, alpha=0.5, histtype='step', range=(-1, 1), log=True)\n",
    "plt.show()\n",
    "\n",
    "im = (a-b)\n",
    "im = scale(im)\n",
    "# im = norm(im)\n",
    "imshow(im[j, l])\n",
    "plt.title('scale(a-b)')\n",
    "plt.show()\n",
    "plt.hist(im.flatten(), bins=55, alpha=0.5, histtype='step', range=(-1, 1), log=True)\n",
    "plt.show()\n",
    "\n",
    "im = (norm(a)-norm(b))\n",
    "im = scale(im)\n",
    "# im = norm(im)\n",
    "imshow(im[j, l])\n",
    "plt.title('scale(norm(a)-norm(b))')\n",
    "plt.show()\n",
    "plt.hist(im.flatten(), bins=55, alpha=0.5, histtype='step', range=(-1, 1), log=True)\n",
    "plt.show()\n",
    "\n",
    "im = a-b\n",
    "# im = scale(im)\n",
    "im = norm(im)\n",
    "imshow(im[j, l])\n",
    "plt.title('norm(a-b)')\n",
    "plt.show()\n",
    "plt.hist(im.flatten(), bins=55, alpha=0.5, histtype='step', range=(-1, 1), log=True)\n",
    "plt.show()\n",
    "\n",
    "plt.title('norm(scale(a)-scale(b))')\n",
    "im = symlog(a)-symlog(b)\n",
    "im /= np.nanmax(np.abs(im))\n",
    "imshow(im[j, l])\n",
    "plt.show()\n",
    "\n",
    "plt.hist(im.flatten(), bins=55, alpha=0.5, histtype='step', range=(-1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "im = a-b\n",
    "# im = norm(im)\n",
    "# im = scale(im)\n",
    "im = norm_h(im)\n",
    "# im /= np.nanmax(np.abs(im), axis=-2, keepdims=True)\n",
    "imshow(im[j, l])\n",
    "plt.show()\n",
    "\n",
    "plt.hist(im.flatten(), bins=55, alpha=0.5, histtype='step', range=(-1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = symlog(a)-symlog(b)\n",
    "# im = norm(im)\n",
    "# im = scale(im)\n",
    "im = norm_h(im)\n",
    "# im /= np.nanmax(np.abs(im), axis=-2, keepdims=True)\n",
    "imshow(im[j, l])\n",
    "plt.show()\n",
    "\n",
    "plt.hist(im.flatten(), bins=55, alpha=0.5, histtype='step', range=(-1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = symlog(norm_h(a))-symlog(norm_h(b))\n",
    "# im = norm(im)\n",
    "# im = scale(im)\n",
    "im = norm_h(im)\n",
    "# im /= np.nanmax(np.abs(im), axis=-2, keepdims=True)\n",
    "imshow(im[j, l])\n",
    "plt.show()\n",
    "\n",
    "plt.hist(im.flatten(), bins=55, alpha=0.5, histtype='step', range=(-1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = symlog(a)-symlog(b)\n",
    "im = norm_h(im)\n",
    "imshow(im[j, l])\n",
    "plt.show()\n",
    "\n",
    "plt.hist(im.flatten(), bins=55, alpha=0.5, histtype='step', range=(-1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # x = np.abs(im[j, l]).mean(-1)\n",
    "# # plt.plot(x)\n",
    "# # x[x>0].argmin()\n",
    "# plt.hist(im2.flatten(), bins=55, alpha=0.5, histtype='step')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = batch_concat['concatenated_input_ids'].shape[0]//2\n",
    "a = batch_concat['concatenated_input_ids'][:bs]\n",
    "b = batch_concat['concatenated_input_ids'][bs:]\n",
    "a = tokenizer.batch_decode(a[j])\n",
    "b = tokenizer.batch_decode(b[j])\n",
    "# c = tokenizer.batch_decode(batch['prompt_input_ids'][j])\n",
    "x = np.abs(im[j, l]).mean(-1)\n",
    "r = list(zip(\n",
    "    # range(len(a)),\n",
    "    x,\n",
    "    a,\n",
    "    b\n",
    "))\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(r, columns=['x', 'tok_cho', 'tok_rej'])\n",
    "df[df.x>0].sort_values('x')\n",
    "df[df.x>0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_concat.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "imshow(im2)\n",
    "plt.colorbar(location='top')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evals"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
