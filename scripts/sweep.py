import os
os.environ["WANDB_GROUP"] = "sweep4"
base_models = [
    # FIXME these should not be models that have been DPO/RLHF, ideally SFT on a base model. UPDATE: SimPO found it didn't matter on gemma-it https://arxiv.org/html/2405.14734v3

    # TODO set batch size, or work it out from parameters
    # "HuggingFaceTB/SmolLM2-360M",
    "wassname/SmolLM2-135M-sft",
    "wassname/SmolLM2-360M-sft",
    # TODO SFT of Qwen/Qwen3-4B-Base
    "wassname/Qwen3-0.6B-sft-4chan",

    "wassname/llama-3-2-1b-sft",
    # "allenai/OLMo-2-0425-1B-SFT",
    # TODO make sft of HuggingFaceTB/SmolLM2-360M

    # QWEN 3
    # "Qwen/Qwen3-0.6B",
    # "Qwen/Qwen3-0.6B-Instruct",
    # "unsloth/Llama-3.2-1B-Instruct",
    # "Qwen/Qwen3-4B",
    # "Qwen/Qwen3-4B-Instruct",
    # # larger
    # "Qwen/Qwen3-8B",
    # "Qwen/Qwen3-14B",

    # base-sft
    # "princeton-nlp/Llama-3-Base-8B-SFT"

]
adapters = [
    "hs-none-InnerDPO",
    "dpo",
    "hs-supr-InnerDPO",

    # "hs-ether-InnerDPO",
    # "projgrad",
]

seeds = [
    1, 2, 3
]

# ok here I need to change it. I want to choose: train, oos, rnd
# hmm maybe I can just list multiple OOS test sets from all the genies related?
datasets = [
    # set 1
    # FIXME, check truncation on these, should ideally be none
    "math",
    "alpaca_mmlu",
    "code_easy",
    "cooking",

    "alpaca_low_quality",
    "math_easy",

    # # set 2
    # "code_easy",
    # "us_history",
    # "change_my_view",
    # "raven_matrices",

    # # set 3
    # "ranking_logic_easy",
    # "shp_low_quality",
    # "pursue_goals",
    # "creative_writing",
    # "alpaca_easy",
    # "arc_easy",
    # "us_history_textbook",
    # "alpaca_chat",
    # "raven_easy",
    # "code_low_quality",
    # "alpaca_short",
]
print("#!/usr/bin/bash")
print("# Generated by scripts/sweep.py")
print("set -uo pipefail")
i = 0
for seed in seeds:
    for base_model in base_models:
        for dataset in datasets:
            for adapter in adapters:
                cmd = f"python scripts/train.py {adapter} --base_model={base_model} --dataset={dataset} --seed={seed}"
                if i==0:
                    cmd += " --verbose=2"
                print(cmd)
                i += 1

# and now some large model experiments
large_base_models = [
    "Qwen/Qwen3-8B",
    "Qwen/Qwen3-14B",
]
for seed in seeds[:1]:
    for base_model in large_base_models:
        for dataset in datasets[:3]:
            for adapter in adapters[:2]:
                cmd = f"python scripts/train.py {adapter} --base_model={base_model} --dataset={dataset} --seed={seed}"
                print(cmd)
